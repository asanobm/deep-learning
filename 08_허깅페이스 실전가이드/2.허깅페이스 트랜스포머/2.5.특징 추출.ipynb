{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5. 특징 추출\n",
    "\n",
    "특징 추출은 이미지나 오디오와 같은 원시 데이터에서 딥러닝 모델의 입력으로 사용될 수 있는 특징(feature)을 추출하는 역할을 한다.\n",
    "**특징이란 데이터에서 유용한 정보를 나타내는 부분이나 속성**을 말한다. \n",
    "\n",
    "* 이미지에서는 가장자리(Edge), 질감(Texture), 색상(Color) 등이 특징이 될 수 있다.\n",
    "* 오디오에서는 주파수(Frequency), 진폭(Magnitude), 음량(Amplitude) 등이 특징이 될 수 있다.\n",
    "\n",
    "이미지 분야에서는 이미지를 픽셀 값을 표현하는 대신 CNN 등의 딥러닝 모델을 활용해 고수준의 의미 있는 특징을 추출할 수 있다. ResNet과 같은 사전 학습된 모델의 중간 계층 출력이나 마지막 계층의 출력을 활용하면 이미지의 시각적 특징을 효과적으로 추출할 수 있다.\n",
    "\n",
    "오디오 분야에서는 원시 오디오 파형(Waveform)에서 직접 특징을 추출하기 어려우므로 멜스펙트로그램(Mel Spectrogram)과 같은 중간 표현(Intermediate Representation)을 활용해 특징을 추출한다. 이러한 특징 추출 방법을 통해 오디오의 주파수, 진폭, 음량 등의 특징을 추출할 수 있다.\n",
    "\n",
    "**특징 추출 주요 기능**\n",
    "\n",
    "* **이미지/오디오 전처리**: 입력 데이터에 대한 전처리 기능을 제공한다. 이미지의 경우 크기 조절, 정규화 등의 작업을 오디오의 경우 리샘플링, 패딩 등의 작업을 수행한다.\n",
    "* **특징 추출**: 전처리된 데이터에서 CNN, 오디오 모델 등을 활용해 특징을 추출한다. 이미지의 경우 CNN출력, 오디오의 경우 멜 스펙트로그램 등이 특징 벡터로 사용된다.\n",
    "* **특징 후처리**: 추출된 특징에 대한 후처리 기능을 제공한다. 이를 통해 모델 입력에 적합한 형태로 특징 벡터를 변환할 수 있다.\n",
    "* **일관된 인터페이스**: 이미지, 오디오 등 다양한 모달리터에 대해 일관된 API를 제공해 코드 재사용성을 높인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1. ImageFeatureExtractor 클래스\n",
    "ImageFeatureExtractor는 허깅페이스에서 이미지 데이터를 전처리하고 시각적 특징을 추출하는 데 사용되는 클래스다.\n",
    "\n",
    "이미지 크기 조절, 정규화, 데이터 증강 등의 전처리 작업을 수행한다. 합성곱 신경망, 비전 트랜스포머 등의 모델을 활용해 이미지의 시각적 특징을 추출한다. 추출된 특징 벡터에 대한 후처리 작업을 수행해 모델 입력에 적합한 형태로 변환한다. \n",
    "\n",
    "CLIPFeatureExtractor.from_pretrained()를 통해 \"openai/clip-vit-base-patch32\" 모델의 특징 추출기를 불러온다. feature_extractor()에 이미지를 전달하면서 전처리 옵션을 지정한다. 여기서는 이미지를 512*512크기로 변경하고 중심 자르기를 적용한다. \n",
    "\n",
    "출력된 inputs는 딕셔너리 형태이며, pixel_values에는 전처리된 이미지 텐서가 포함된다. 이렇게 추출된 특징 벡터(pixel_values)를 지정해 파이토치 텐서 형태로 출력한다.\n",
    "\n",
    "출력된 inputs는 딕셔너리 형태이며, ㅔ는ㅇㄹㄴㅇㄹㅁㅇㄹ 전처리된 이미지 텐서가 포함된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512, 512])\n",
      "{'pixel_values': tensor([[[[ 0.5143,  0.5727,  0.6603,  ..., -0.0259, -0.1280, -0.0696],\n",
      "          [ 0.5435,  0.6165,  0.6603,  ...,  0.0325,  0.0179, -0.0113],\n",
      "          [ 0.5435,  0.5873,  0.6019,  ...,  0.0179,  0.0179,  0.1347],\n",
      "          ...,\n",
      "          [ 1.8135,  1.8573,  1.9157,  ...,  1.4778,  1.4486,  1.5654],\n",
      "          [ 1.9157,  1.8573,  1.8865,  ...,  1.2588,  1.1712,  1.6238],\n",
      "          [ 1.8719,  1.8573,  1.9011,  ...,  1.1712,  1.4486,  1.5654]],\n",
      "\n",
      "         [[-1.4069, -1.3469, -1.2568,  ..., -1.4970, -1.5870, -1.4970],\n",
      "          [-1.3769, -1.2718, -1.2268,  ..., -1.4519, -1.4369, -1.4519],\n",
      "          [-1.3469, -1.2718, -1.2418,  ..., -1.5120, -1.4669, -1.3769],\n",
      "          ...,\n",
      "          [ 0.0789,  0.0939,  0.1389,  ..., -0.6565, -0.6565, -0.5665],\n",
      "          [ 0.1689,  0.1089,  0.0789,  ..., -0.8816, -0.9117, -0.4164],\n",
      "          [ 0.0939,  0.0789,  0.1239,  ..., -0.9117, -0.5965, -0.4464]],\n",
      "\n",
      "         [[-0.6839, -0.5275, -0.3426,  ..., -0.8545, -0.8830, -0.8972],\n",
      "          [-0.4279, -0.4137, -0.4137,  ..., -0.8972, -0.8688, -0.7977],\n",
      "          [-0.4279, -0.4422, -0.4564,  ..., -0.8972, -0.8403, -0.6981],\n",
      "          ...,\n",
      "          [ 1.6340,  1.5771,  1.6482,  ...,  0.8803,  0.6528,  0.7523],\n",
      "          [ 1.6340,  1.5913,  1.7193,  ...,  0.9656,  0.5248,  0.7808],\n",
      "          [ 1.7193,  1.6340,  1.6055,  ...,  0.8661,  0.8661,  0.9941]]]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asanobm/miniforge3/envs/ai/lib/python3.11/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import CLIPFeatureExtractor\n",
    "\n",
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "image = dataset[\"test\"]['image'][0]\n",
    "\n",
    "feature_extractor = CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "inputs = feature_extractor(\n",
    "  images=image,\n",
    "  do_resize=True,\n",
    "  size=512,\n",
    "  do_center_crop=True,\n",
    "  crop_size=512,\n",
    "  return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "print(inputs[\"pixel_values\"].shape)\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2. AudioFeatureExtractor 클래스\n",
    "\n",
    "AudioFeatureExtractor는 허깅페이스에서 오디오 데이터를 전처리하고 오디오의 주파수, 진폭, 음량 등의 특징을 추출하는 데 사용되는 클래스다. 이 클래스는 오디오 파형을 딥러닝 모델의 입력으로 사용하기 위해 필요한 전처리 작업과 특징 추출 작업을 수행한다.\n",
    "\n",
    "오디오 파형에 대한 리샘플링, 패딩, 정규화등의 전처리 작업을 수행하며, 오디오 모델을 활용해 오디오 파형으로부터 특징 벡터를 추출한다.\n",
    "\n",
    "load_dataset을 사용해 \"PolyAI/minds14\"데이터세트의 한국어 부분을 불러오고, 첫 두개의 오디오 파일을 선택한다. 이후 Wav2Vec2FeatureExtractor.from_pretrained()를 통해 \"facebook/wav2vec2-base-960h\"모델의 특징 추출기를 불러온다.\n",
    "\n",
    "feature_extractor()에 오디오 파일을 전달하면서 전처리 옵션을 지정한다. 여기서는 패딩과 어텐션 마스크 생성 옵션을 활성화한다. 출력된 inputs는 딕셔너리 형태이며, input_values에는 전처리된 오디오 텐서가 포함된다. 이렇게 추출된 특징 벡터(input_values)를 지정해 파이토치 텐서 형태로 출력한다.\n",
    "\n",
    "**오디오 특징 벡턱 활용 예시**\n",
    "\n",
    "* **음성 인식**: 오디오 특징 벡터를 활용해 음성 인식 모델을 학습한다.\n",
    "* **오디오 분류**: 특징 벡터를 분류 모델의 입력으로 사용해 오디오를 분류한다.\n",
    "* **스피커 인식**: 특징 벡터를 활용해 스피커의 음성을 인식하고 구별한다.\n",
    "* **오디오 이벤트 검출**: 특징 벡터를 분석해 오디오 이벤트를 검출하고 분류한다.\n",
    "\n",
    "오디오 데이터는 이미지 데이터와 몇가지 차이점이 있다. 오디오 데이터는 일련의 시퀀스 데이터로 구성돼 있다. 이미지는 일반적으로 고정된 크기의 2차원 데이터이다. 오디오 데이터는 다양한 샘플링 레이트(Sampling rage)를 가질 수 있다. 이로 인해 전처리 과정에서 리샘플링이 필요할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([70315])\n",
      "torch.Size([70315])\n",
      "{'input_values': tensor([[2.3359e-03, 2.8042e-05, 2.8042e-05,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [3.4663e-03, 1.6907e-04, 1.6907e-04,  ..., 1.3358e-02, 1.3358e-02,\n",
      "         1.0061e-02]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]], dtype=torch.int32)}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "\n",
    "dataset = load_dataset(\"PolyAI/minds14\", \"ko-KR\", split=\"train\", trust_remote_code=True)\n",
    "audios = [audio[\"array\"] for audio in dataset[\"audio\"][:2]]\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "inputs = feature_extractor(\n",
    "  raw_speech=audios,\n",
    "  padding=True,\n",
    "  return_attention_mask=True,\n",
    "  return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "print(inputs[\"input_values\"][0].shape)\n",
    "print(inputs[\"input_values\"][1].shape)\n",
    "print(inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
