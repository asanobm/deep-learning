{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. 질의응답 RoBERTa\n",
    "\n",
    "**질의 응답(Question Answering)**은 주어진 지식이나 맥락을 바탕으로 질문에 대한 답변을 찾는 과제다. 답변 생성 방식에 따라 **추출 질의 응답(Extractive Question Answering)**과 **생성 질의 응답(Generative Question Answering)**으로 나뉜다. 추출 질의 응답은 주어진 문맥에서 정답이 위치한 토큰의 위치를 찾아내는 방식이고, 생성 질의 응답은 주어진 문맥과 질문을 바탕으로 정답을 생성하는 방식이다.\n",
    "\n",
    "추출 질의 응답과 생성 질의 응답은 데이터세트, 모델 구조, 성능 평가 방식 등에서 차이가 있다. 추출 질의 응답은 일반적으로 **질문-지문-지문 내 답변**으로 구성된 데이터세트를 사용하고 (예: SQuAD), 생성 질의 응답은 **질문-지문-답변**으로 구성된 데이터세트를 사용한다 (예: NarrativeQA) \n",
    "\n",
    "**추출 질의 응답**\n",
    "\n",
    "* **질문**: 파이토치를 사용하여 BERT 기반 질의 응답 모델을 학습시키는 방법은 무엇인가요?\n",
    "* **지문**: 파이토치를 사용하여 BERT 기반 질의 응답 모델을 학습시키는 방법은 다음과 같다. 먼저 파이토치로 BERT 모델을 불러온다. 그 다음, SQuAD 데이터셋을 불러와서 모델을 학습시킨다. 학습이 완료되면, 테스트 데이터셋으로 성능을 평가한다.\n",
    "* **정답**: 먼저 파이토치로 BERT 모델을 불러온다.\n",
    "\n",
    "**생성 질의 응답**\n",
    "* **질문**: 파이토치를 사용하여 BERT 기반 질의 응답 모델을 학습시키는 방법은 무엇인가요?\n",
    "* **지문**: 파이토치를 사용하여 BERT 기반 질의 응답 모델을 학습시키는 방법은 다음과 같다. 먼저 파이토치로 BERT 모델을 불러온다. 그 다음, SQuAD 데이터셋을 불러와서 모델을 학습시킨다. 학습이 완료되면, 테스트 데이터셋으로 성능을 평가한다.\n",
    "* **정답**: 파이토치로 BERT 모델을 불러온다. 그 다음, SQuAD 데이터셋을 불러와서 모델을 학습시킨다. 학습이 완료되면, 테스트 데이터셋으로 성능을 평가한다.\n",
    "\n",
    "추출 질의 응답은 지문 내에 정답이 존재하므로 비교적 간단하고 정확한 답변을 제공할 수 있다. 하지만 지문에 명시적으로 드러나지 않는 정보에 대해서는 대답하기 어렵다. 반면에 생성 질의 응답은 지문 외의 외부 지식을 활용해 답변할 수 있지만 생성된 답변의 정확성과 일관성을 보장하기 어렵다.(지맘대로 만든다) 또한 생성 질의 응답은 외부 지식과 추론 능력을 필요로 하기 때문에 겁나 어렵다. ...\n",
    "\n",
    "질의 응답은 맥락 이해, 추론 등 고차원적인 언어 이해 능력이 필요하며, 같은 질문이라도 주어진 지문에 따라 다른 답변이 요구될 수 있다. 이러한 특징들로 인해 질의 응답은 어렵다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. RoBERTa\n",
    "\n",
    "RoBERTa(A Robustly Optimized BERT Approach)는 BERT 모델의 성능을 개선하기 위해 메타의 FAIR에서 제안한 변형 모델이다. BERT는 혁신적인 사전 학습 방법론을 제시했지만, 일부 학습 전략의 한계로 인해 성능 향상에 제약이 있었다. RoBERTa는 이러한 한계를 극복하기 위해서 다른 학습 전략을 적용하고 있다.\n",
    "\n",
    "**RoBERTa 학습전략**\n",
    "\n",
    "* BERT가 13GB의 비교적 작은 데이터세트로 학습한 것과 달리, RoBERTa는 160GB의 웹 크롤링 데이터와 같은 거대한 규모의 데이트세트로 사전 학습이 되어 있다. 데이터 규모가 크면 클수록 모델이 더 많은 지식을 습득할 수 있기 때문에 성능이 향상된다.\n",
    "* BERT는 입력 시퀀스 길이가 512 토큰으로 제한되어 있지만, RoBERTa는 2,048 토큰까지 처리할 수 있다.\n",
    "* BERT는 사전 학습 시 고정된 마스킹 패턴을 사용하지만, RoBERTa는 동적 마스킹(Dynamic Masking)을 사용한다. 각 에폭마다 다른 마스킹 패턴을 생성함으로써 모델이 더 다양한 정보를 학습할 수 있도록 했다.\n",
    "* BERT는 워드피스 토큰화를 사용한 반면, RoBERTa는 BPE(Byte Pair Encoding) 토큰화를 사용한다. BPE 토큰화는 단어를 서브워드로 분리하는 방식으로, 언어의 특성을 더 잘 반영할 수 있다.\n",
    "* RoBERTa는 BERT의 다음 문장 예측(NSP)작업을 제거하고 마스크 언어 모델(MLM)만 사용한다. NSP 작업을 제거함으로써 모델이 문맥을 더 잘 이해하도록 했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. 추출 질의 응답 모델 학습\n",
    "\n",
    "RoBERTa의 모델 구조는 BERT와 동일하게 임베딩 계층과 12개의 트랜스포머 인코더로 이루어져 있으며, 마지막 인코더 계층의 출력값은 질의 응답 헤드를 통과해 답변 텍스트의 시작 위치와 끝 위치를 예측하는데 사용된다. 그러므로 토크나이저에 질문과 답변에 참고할 지식 정보를 입력한다.\n",
    "\n",
    "질의 응답 모델의 학습을 위해서는 질문과 관련 지식정보를 토크나이저에 입력해야한다. 토크나이저는 [SEP] 토큰을 사용해 두 텍스트를 연결하고 인코딩한다. 인코딩된 입력 텍스트는 RoBERTa 모델을 통과하면서 시작 위치 로짓과 끝 위치 로짓으로 변환된다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기계 톡해 데이터세트 토큰화\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 전처리 함수\n",
    "def preprocess_data(example, tokenizer):\n",
    "    \"\"\"\n",
    "    이 함수는 질문과 문맥을 토큰화하고, 답변의 시작과 끝 위치를 설정합니다.\n",
    "    \n",
    "    1. 질문과 문맥을 토큰화합니다.\n",
    "    2. 답변의 시작 인덱스와 텍스트를 가져옵니다.\n",
    "    3. 답변 텍스트를 토큰화하고 길이를 계산합니다.\n",
    "    4. 문맥의 시작 토큰 인덱스를 찾고, offset 매핑을 가져옵니다.\n",
    "    5. 문맥 offset 매핑을 반복하면서 답변의 시작 위치와 끝 위치를 설정합니다.\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        example['question'], # 질문\n",
    "        example['context'], # 문맥\n",
    "        truncation='only_second', # 문맥만 자르기\n",
    "        return_offsets_mapping=True, # offset 정보 반환\n",
    "    )\n",
    "    start_index = example['answers']['answer_start'][0] # 답변 시작 인덱스\n",
    "    answer_text = example['answers']['text'][0] # 답변 텍스트\n",
    "    answer_tokens = tokenizer.encode(answer_text, add_special_tokens=False) # 답변 토큰화\n",
    "    answer_tokens_length = len(answer_tokens) # 답변 토큰 길이\n",
    "\n",
    "    start_context_tokens_index = tokenized[\"input_ids\"].index(tokenizer.sep_token_id) # 문맥 시작 토큰 인덱스\n",
    "    context_offset_mapping = tokenized[\"offset_mapping\"][start_context_tokens_index:] # 문맥 offset 매핑\n",
    "    tokenized[\"start_positions\"] = len(tokenized[\"input_ids\"]) # 시작 위치 초기화\n",
    "    tokenized[\"end_positions\"] = len(tokenized[\"input_ids\"]) # 끝 위치 초기화\n",
    "\n",
    "    for i, (start_offset, end_offset) in enumerate(context_offset_mapping): # 문맥 offset 매핑 반복\n",
    "        if start_offset >= start_index: # 시작 offset이 답변 시작 인덱스보다 크거나 같으면\n",
    "            tokenized[\"start_positions\"] = start_context_tokens_index + i # 시작 위치 설정\n",
    "            tokenized[\"end_positions\"] = tokenized[\"start_positions\"] + answer_tokens_length # 끝 위치 설정\n",
    "            break\n",
    "    \n",
    "    return tokenized # 토큰화된 결과 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizerFast'.\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers'],\n",
      "        num_rows: 17554\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers'],\n",
      "        num_rows: 5841\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers', 'input_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions'],\n",
      "        num_rows: 11083\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers', 'input_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions'],\n",
      "        num_rows: 3696\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizerFast, RobertaForQuestionAnswering\n",
    "\n",
    "model_name = \"klue/roberta-base\"\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_name) # 토크나이저 불러오기\n",
    "model = RobertaForQuestionAnswering.from_pretrained(model_name) # 모델 불러오기\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # GPU 사용 여부 확인\n",
    "\n",
    "# 데이터세트 불러오기\n",
    "dataset = load_dataset(\"klue\", \"mrc\") # 기계 독해 데이터세트 불러오기\n",
    "processed_dataset = dataset.filter(lambda x: not x[\"is_impossible\"]) # 불가능한 데이터 제거\n",
    "processed_dataset = processed_dataset.map(lambda example: preprocess_data(example, tokenizer), batched=False,) # 전처리 함수 적용\n",
    "processed_dataset = processed_dataset.filter(lambda x: x[\"start_positions\"] < tokenizer.model_max_length) # 최대 길이보다 긴 데이터 제거\n",
    "processed_dataset = processed_dataset.filter(lambda x: x[\"end_positions\"] < tokenizer.model_max_length) # 최대 길이보다 긴 데이터 제거\n",
    "\n",
    "print(dataset)\n",
    "print(processed_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KLUE의 데이터세트는 뉴스 기사와 관련된 질문에 대한 답변을 제공하는 데이터로 구성되어 있다. 이 데이터세트는 뉴스 제목(title), 뉴스 본문(context), 카테고리(category), 언론사 출처(source), 고유 번호(guid), 답변 가능 여부(is_impossible), 질문 유형(question_type), 질문(question), 답변(answer)으로 구성되어 있다.\n",
    "\n",
    "이 데이터세트에는 답변이 불가능한 질문도 포함되어 있다. (필터링 해버림)\n",
    "\n",
    "RoBERTa 모델을 이용한 추출형 질의 응답 모델 학습을 위해서는 뉴스 본문에서 답변의 위치를 찾아야 한다. 이때는 원본 텍스트가 아니라 토큰화된 입력에서 답변 위치를 찾아야 한다. 전처리 과정에서 답변의 위치를 반환하도록 한다.\n",
    "\n",
    "`preprocess_function` 함수에서 토크나이저의 질문과 뉴스 본문을 입력해, 두 텍스트를 하나의 입력 시퀀스로 연결한다. 이렇게 연결된 입력 시퀀스의 전체 길이가 토크나이저의 최대 허용 길이(max_length)를 초과하는 경우가 발생한다. 이때 `truncation`에서 `only_second`로 설정하면 질문 부분은 그대로 유지하고 뉴스 본문 부분에서만 잘라내기를 수행한다. \n",
    "\n",
    "`return_offsets_mapping` 인자를 `True`로 설정하면 각 토큰의 원본 텍스트에서 시작 위치와 끝 위치가 `offset_mapping`키로 반환된다. 예를 들어 \"안녕하세요\"를 [\"안녕\", \"하\", \"세요\"]로 토큰화한 경우, `offset_mapping`은 [(0, 2), (2, 3), (3, 5)]로 반환된다. 이때 (0, 2)는 \"안녕\" 토큰이 원본 텍스트에서 시작 위치가 0이고 끝 위치가 2라는 의미다.\n",
    "\n",
    "MRC 데이터세트의 answers 필드는 기사 본문에서 추출한 정답 정보가 있다. answer_start에는 정답 텍스트의 위치 정보가 리스트 형태로 저장되어 있다. 이는 본문 내에서 여러 개의 정답으로 사용 가능한 텍스트가 존재할 수 있어서다. 그러나 모델 학습 시에는 이중 첫 번째 위치 정보만 활용한다. 또한 answers 필드의 text에 있는 정답 텍스트 역시 여러 개 일 수 있지만 첫 번째 텍스트만 사용된다. 이는 대부분 질문에 대한 답변이 하나뿐이기 때문이다. 또한 모델 학습에 단순화 하기 위한 전략이기도 하다?\n",
    "\n",
    "토큰화된 입력ID(tokenized[\"input_ids\"])는 [질문 입력 ID, [SEP], 뉴스 본문 입력 ID, [SEP]] 형태로 구성되어 있다. 이때 질문 입력 ID와 뉴스 본문 입력 ID는 각각 `question_input_ids`와 `context_input_ids`로 분리된다. 또한 `offset_mapping`은 [질문 offset_mapping, 뉴스 본문 offset_mapping] 형태로 구성된다. 이때 질문 offset_mapping과 뉴스 본문 offset_mapping은 각각 `question_offset_mapping`과 `context_offset_mapping`으로 분리된다.\n",
    "\n",
    "preprocess_data 함수는 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "# 추출 질의 응답 모델 학습\n",
    "\n",
    "from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_name) # 토크나이저 불러오기\n",
    "collator = DataCollatorWithPadding(tokenizer, padding=\"longest\") # 패딩 설정\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./runs/question-answering\", # 출력 경로\n",
    "  per_device_train_batch_size=8, # 배치 크기\n",
    "  per_device_eval_batch_size=16, # 평가 배치 크기\n",
    "  learning_rate=5e-5, # 학습률\n",
    "  num_train_epochs=4, # 에폭 수\n",
    "  eval_steps=100, # 평가 스텝\n",
    "  logging_steps=100, # 로깅 스텝\n",
    "  seed=42, # 시드\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  data_collator=collator,\n",
    "  train_dataset=processed_dataset[\"train\"].select(range(10000)), # 학습 데이터\n",
    "  eval_dataset=processed_dataset[\"validation\"].select(range(100)), # 검증 데이터\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 06:44, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.174600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.309800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.941500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.592400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.562000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.267400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.255900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.192800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.125700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.102200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asanobm/miniforge3/envs/ai/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/asanobm/miniforge3/envs/ai/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/asanobm/miniforge3/envs/ai/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/asanobm/miniforge3/envs/ai/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=0.5524352737426758, metrics={'train_runtime': 404.3465, 'train_samples_per_second': 98.925, 'train_steps_per_second': 6.183, 'total_flos': 1.1104066974253056e+16, 'train_loss': 0.5524352737426758, 'epoch': 4.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() # 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataCollatorWithPadding` 클래스의 `padding` 인자를 `longest`로 설정하면 컬레이터가 배치 내 가장 긴 문장에 맞춰 패딩한다. 학습 데이터 10,000개 검증 데이터 100개를 사용한다. \n",
    "`epoch=4`, `learning_rate=5e-5`, 로 설정했다.\n",
    "\n",
    "질문과 문맥을 토크나이저로 전처리해 모델에 입력할 수 있는 형태로 변환한다. 모델에 입력을 전달하고 출력을 생성한다. 출력값에는 정답 시작점과 끝점의 로짓이 포함되어 있다. 이 로짓에서 가장 큰 값의 인덱스를 추출하면 예측된 정답 텍스트를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서울\n"
     ]
    }
   ],
   "source": [
    "# 추출 질의 응답 수행\n",
    "\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # GPU 사용 여부 확인\n",
    "\n",
    "model.eval() # 평가 모드로 변경\n",
    "\n",
    "question = \"한국의 수도는 어디인가요?\" # 질문\n",
    "context = \"한국의 수도는 서울이다.\" # 문맥\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\").to(device) # 입력 토큰 생성\n",
    "with torch.no_grad(): # 그래디언트 계산 비활성\n",
    "    outputs = model(**inputs) # 모델 수행\n",
    "\n",
    "start_index = outputs[\"start_logits\"].argmax(dim=-1).item() # 시작 위치 예측\n",
    "end_index = outputs[\"end_logits\"].argmax(dim=-1).item() # 끝 위치 예측\n",
    "\n",
    "predicted_ids = inputs[\"input_ids\"][0][start_index:end_index] # 예측된 토큰 ID\n",
    "predicted_text = tokenizer.decode(predicted_ids) # 예측된 텍스트\n",
    "print(predicted_text) # 예측된 텍스트 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**추출 질의 응답 모델 평가**\n",
    "\n",
    "허깅페이스의 평가 라이브러리는 과제별로 평가지표를 계산하는 `evaluator` 함수를 제공한다. 이 함수는 과제 이름을 입력해 평가 지표를 불러온다. \n",
    "평가지표는 `compute` 메서드를 사용해 계산한다. \n",
    "\n",
    "**질의 응답 compute 메서드 매개변수**\n",
    "\n",
    "* **model**: 평가할 모델\n",
    "* **tokenizer**: 평가할 토크나이저\n",
    "* **data**: 모델 평가에 사용할 데이터세트\n",
    "* **id_column**: 각 데이터 인스턴스의 고유 식별자 열 이름\n",
    "* **question_column**: 질문 열 이름\n",
    "* **context_column**: 문맥 열 이름\n",
    "* **label_column**: 정답 열 이름\n",
    "\n",
    "질의 응답 평가지표로 완전 일치(Exact Match)와 F1 점수의 두가지가 사용된다.\n",
    "완전 일치는 모델의 예측과 정확히 일치하는 비율을 나타낸다. 엄격한 평가 방식으로 100% 정확한 답변만을 인정한다. \n",
    "F1 점수는 예측과 정답 간의 단어 중복을 기반으로 계산한다. 부분적으로 정확한 답변도 인정 받을 수 있다.\n",
    "\n",
    "`evaluator` 함수는 모델 예측 속도도 평가한다. `total_time_in_seconds` 는 전체 평가에 소요된 시간(초), `samples_per_second` 는 초당 처리된 샘플 수, `latency_in_seconds`는 샘플당 평가 시간(초)를 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae8722e7c4c4461b4d413b85a336016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`squad_v2_format` parameter not provided to QuestionAnsweringEvaluator.compute(). Automatically inferred `squad_v2_format` as False.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exact_match</th>\n",
       "      <th>f1</th>\n",
       "      <th>total_time_in_seconds</th>\n",
       "      <th>samples_per_second</th>\n",
       "      <th>latency_in_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>25.98</td>\n",
       "      <td>0.98</td>\n",
       "      <td>101.75</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   exact_match     f1  total_time_in_seconds  samples_per_second  \\\n",
       "0          3.0  25.98                   0.98              101.75   \n",
       "\n",
       "   latency_in_seconds  \n",
       "0                0.01  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evaluate import evaluator\n",
    "\n",
    "metric = evaluator(\"question-answering\") # 메트릭 생성\n",
    "results = metric.compute(\n",
    "  model,\n",
    "  tokenizer=tokenizer,\n",
    "  data=processed_dataset[\"validation\"].select(range(100)), # 검증 데이터\n",
    "  id_column=\"guid\",\n",
    "  question_column=\"question\",\n",
    "  context_column=\"context\",\n",
    "  label_column=\"answers\",\n",
    ")\n",
    "\n",
    "pd.DataFrame([results]).round(2) # 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
