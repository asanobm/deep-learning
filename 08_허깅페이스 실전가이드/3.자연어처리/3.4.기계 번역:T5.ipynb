{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. 기계 번역: T5\n",
    "\n",
    "기계 번역(Machine Translation)은 컴퓨터 프로그램을 활용해 한 언어의 텍스트를 다른 언어의 텍스트로 자동 변환하는 기술이다. 빠르고 저렴하지만 신조어, 관용구, 은유등 쫌 힘든 부분이 있다.\n",
    "\n",
    "기계 번역 기술은 통계적 기계 번역(Statistical Machine Translation), 신경망 기계 번역(Neural Machine Translation) 등이 있다.\n",
    "통계적 기계 번역은 원문과 번역문 쌍을 기반으로 단어 순서와 언어 패턴을 인식해 학습한다. 반면, 신경망 기계 번역은 심층 신경망 모델을 사용해 번역문과 단어 시퀀스 간의 관계를 학습한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1. T5\n",
    "\n",
    "T5(Text-to-Text Transfer Transformer)는 구글에서 개발한 언어 모델로 인코더와 디코더로 이루어진 시퀀스-투-시퀀스(Seq2Seq) 모델이다. 이 모델은 모든 자연어 처리 과제를 텍스트-투-텍스트 형태의 데이터로 변환하고, 이를 시퀀스-투-시퀀스 문제로 인식해 해결한다. 입력과 출력이 모두 텍스트이다.\n",
    "\n",
    "**기계 번역 과제**\n",
    "\n",
    "* **입력 텍스트**: \"오늘 날씨는 어때요?\"\n",
    "* **입력 시퀀스**: [\"오늘, 날씨는, 어때요?\"]\n",
    "* **출력 시퀀스**: [\"How's, the, weather, today?\"]\n",
    "* **출력 텍스트**: \"How's the weather today?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2. 기계 번역 모델 학습\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type longt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at KETI-AIR/long-ke-t5-small and are newly initialized: ['encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [20004, 20525, 20048, 20298, 20480, 20025, 20263, 20027, 20187, 20050, 43305, 20009, 21015, 20047, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [20004, 23477, 20048, 92, 14, 4256, 11, 1363, 71, 1133, 2951, 20371, 33, 16, 75, 242, 10, 513, 20047, 1]}\n",
      "변환된 출발 언어: en: What makes you think I want an intro to anyone?</s>\n",
      "변환된 도착 언어: ko: 내가 너를 누구에게 소개하고 싶어한다고 생각하니?</s>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "\n",
    "def preprocess_data(example, tokenizer):\n",
    "    translation = example['translation']\n",
    "    translation_source = [\"en: \" + instance[\"en\"] for instance in translation]\n",
    "    translation_target = [\"ko: \" + instance[\"ko\"] for instance in translation]\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        translation_source,\n",
    "        text_target=translation_target,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "model_name = \"KETI-AIR/long-ke-t5-small\"\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ko\")\n",
    "\n",
    "processed_dataset = dataset.map(\n",
    "    lambda example: preprocess_data(example, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "sample = processed_dataset[\"test\"][0]\n",
    "print(sample)\n",
    "print(\"변환된 출발 언어:\", tokenizer.decode(sample[\"input_ids\"]))\n",
    "print(\"변환된 도착 언어:\", tokenizer.decode(sample[\"labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPUS-100 데이터세트는 다양한 언어 간 기계 번역 작업을 위해 구축된 대규모 병렬 말붕치 (Parallel corpus)다. 이중 en-ko는 영어와 한국어 간의 번역 쌍을 나타닌다. 이 데이터세트의 영어-한국어 부분은 100만개의 학습데이터와 각각 2,000개의 검증 및 테스트 데이터로 구성되어 있다.\n",
    "\n",
    "데이터세트는`translation` 열로만 이루어져 있다. 이 열에는 영어 문장과 한국어 문장이 en과 ko 키를 갖는 딕셔너리 형태로 저장 되어 있다. \n",
    "\n",
    "`T5ForConditionalGeneration`을 사용할 때는 수행할 하위 작업을 프롬프트 형식으로 정의해야 한다. `en: 프롬프트`,를 이용해 모델에 전달한다. 모델은 이 프롬프트를 입력받아 처리한후 \"ko: 프롬프트\"를 출력한다.\n",
    "\n",
    "`T5ForConditionalGeneration` 은 프롬프트 기반 접근법을 사용해 다양한 자연어 처리 작업을 수행할 수 있다. 번역 작업뿐만 아니라 요약, 질의응답 등의 장업도 프롬프트 형식을 적절히 정의하면 동일한 방식으로 수행할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type longt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at KETI-AIR/long-ke-t5-small and are newly initialized: ['encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # 첫 번째 GPU를 사용하도록 설정\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, T5TokenizerFast, T5ForConditionalGeneration\n",
    "\n",
    "def preprocess_data(example, tokenizer):\n",
    "  translation = example['translation']  # 번역된 문장 쌍을 가져옴\n",
    "  translation_source = [\"en: \" + instance[\"en\"] for instance in translation]  # 영어 문장에 \"en: \" 접두사 추가\n",
    "  translation_target = [\"ko: \" + instance[\"ko\"] for instance in translation]  # 한국어 문장에 \"ko: \" 접두사 추가\n",
    "\n",
    "  tokenized = tokenizer(\n",
    "    translation_source,\n",
    "    text_target=translation_target,\n",
    "    truncation=True,  # 길이가 너무 길면 잘라냄\n",
    "  )\n",
    "\n",
    "  return tokenized  # 토큰화된 결과 반환\n",
    "\n",
    "model_name = \"KETI-AIR/long-ke-t5-small\"  # 사용할 모델 이름\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)  # 모델에 맞는 토크나이저 로드\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)  # 모델 로드\n",
    "\n",
    "dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ko\")  # 데이터셋 로드\n",
    "\n",
    "processed_dataset = dataset.map(\n",
    "  lambda example: preprocess_data(example, tokenizer),  # 데이터셋 전처리\n",
    "  batched=True,  # 배치 단위로 처리\n",
    "  remove_columns=dataset[\"train\"].column_names,  # 원래 컬럼 제거\n",
    ")\n",
    "\n",
    "\n",
    "seq2seq2_collator = DataCollatorForSeq2Seq(\n",
    "  tokenizer=tokenizer,\n",
    "  padding=\"longest\",  # 가장 긴 시퀀스에 맞춰 패딩\n",
    "  return_tensors=\"pt\",  # PyTorch 텐서로 반환\n",
    ")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "  output_dir=\"./runs/t5-translation\",  # 출력 디렉토리\n",
    "  logging_dir='./runs/t5-translation/logs',  # 로그 디렉토리\n",
    "  per_device_train_batch_size=8,  # 훈련 배치 크기\n",
    "  per_device_eval_batch_size=16,  # 평가 배치 크기\n",
    "  learning_rate=2e-5,  # 학습률\n",
    "  num_train_epochs=100,  # 학습 에포크 수\n",
    "  eval_steps=200,  # 평가 스텝\n",
    "  logging_steps=50,  # 로그 스텝\n",
    "  save_steps=200,  # 저장 스텝\n",
    "  seed=42,  # 랜덤 시드\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "  model=model,  # 모델\n",
    "  data_collator=seq2seq2_collator,  # 데이터 콜레이터\n",
    "  args=training_args,  # 훈련 인자\n",
    "  train_dataset=processed_dataset[\"train\"].select(range(100000)),  # 훈련 데이터셋\n",
    "  eval_dataset=processed_dataset[\"test\"].select(range(1000)),  # 평가 데이터셋\n",
    ")\n",
    "\n",
    "# trainer.train()  # 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5ForConditionalGeneration 을 활용한 번역 모델 학습에서는 DataCollectorForSeq2Seq 클래스를 이용해 배치 단위로 입력 시퀀스와 출력 시퀀스에 대한 패딩 처리를 수행한다. T5와 BART등의 시퀀스-투-시퀀스 모델에서는 입력 데이터와 출력 데이터가 모두 텍스트 형태의 시퀀스로 구성되어 있다.\n",
    "따라서 입력 ID와 레이블을 동일한 길이로 맞추기 위해 DataCollatorForSeq2Seq 클래스로 패딩 작업을 수행한다.\n",
    "\n",
    "기계 번역은 출발어와 목적어 모두를 깊이 이해해야 하는 복잡한 자연어 처리 과제다. 그러므로 기존 엄청나게 오래 걸린다. tmux를 사용해야 한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ko: 새로운 지식을 얻을 때마다\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"./runs/t5-translation/best-model\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "data = \"en: It's always great to acquire new knowledge.\"\n",
    "\n",
    "inputs = tokenizer(data, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=512,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.06456637647777932, 'precisions': [0.41389728096676737, 0.21530249110320285, 0.04112554112554113, 0.011049723756906077], 'brevity_penalty': 0.8093851751674768, 'length_ratio': 0.8254364089775561, 'translation_length': 662, 'reference_length': 802}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>번역 대상 문장</th>\n",
       "      <th>참조 번역</th>\n",
       "      <th>생성된 번역</th>\n",
       "      <th>BLEU 점수</th>\n",
       "      <th>단문 패널티</th>\n",
       "      <th>길이 비율</th>\n",
       "      <th>번역 문장 길이</th>\n",
       "      <th>참조 문장 길이</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en: What makes you think I want an intro to an...</td>\n",
       "      <td>ko: 내가 너를 누구에게 소개하고 싶어한다고 생각하니?</td>\n",
       "      <td>ko: 내가 뭐라고 생각하는 거야?</td>\n",
       "      <td>0.064566</td>\n",
       "      <td>0.809385</td>\n",
       "      <td>0.825436</td>\n",
       "      <td>662</td>\n",
       "      <td>802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en: ( tires squealing )</td>\n",
       "      <td>ko: ( tires squealing )</td>\n",
       "      <td>ko: ( tires queing)</td>\n",
       "      <td>0.064566</td>\n",
       "      <td>0.809385</td>\n",
       "      <td>0.825436</td>\n",
       "      <td>662</td>\n",
       "      <td>802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en: That would have put you in jail for a long...</td>\n",
       "      <td>ko: 그리고 당신의 마약 스캔들을 근절하려고 했고요 그건 당신을 오랫동안 감옥에 ...</td>\n",
       "      <td>ko: 그가 감옥에 갔을 때 당신은 그를 죽였을거야</td>\n",
       "      <td>0.064566</td>\n",
       "      <td>0.809385</td>\n",
       "      <td>0.825436</td>\n",
       "      <td>662</td>\n",
       "      <td>802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en: -He seems happy. -Happy is for ordinary pe...</td>\n",
       "      <td>ko: 성형외과에나 기웃거리게 만들고 말이지</td>\n",
       "      <td>ko: - 행복한 사람들이야</td>\n",
       "      <td>0.064566</td>\n",
       "      <td>0.809385</td>\n",
       "      <td>0.825436</td>\n",
       "      <td>662</td>\n",
       "      <td>802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en: I'll find out where she's from and get the...</td>\n",
       "      <td>ko: 그녀가 어디소속이지 찾아서 물러나도록 할거야</td>\n",
       "      <td>ko: 그들을 찾아낼 수 있는 곳이 어디야?</td>\n",
       "      <td>0.064566</td>\n",
       "      <td>0.809385</td>\n",
       "      <td>0.825436</td>\n",
       "      <td>662</td>\n",
       "      <td>802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>en: I do two weeks on, two weeks off.</td>\n",
       "      <td>ko: 2주 일하고 2주 쉬죠</td>\n",
       "      <td>ko: 2주 정도 기다려</td>\n",
       "      <td>0.064566</td>\n",
       "      <td>0.809385</td>\n",
       "      <td>0.825436</td>\n",
       "      <td>662</td>\n",
       "      <td>802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>en: The way you left me.</td>\n",
       "      <td>ko: 그래서 떠난거군</td>\n",
       "      <td>ko: 당신은 나를 떠났어요</td>\n",
       "      <td>0.064566</td>\n",
       "      <td>0.809385</td>\n",
       "      <td>0.825436</td>\n",
       "      <td>662</td>\n",
       "      <td>802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>en: We had to restrain him. I requested a psyc...</td>\n",
       "      <td>ko: 저희가 묶어뒀습니다 정신과 진찰도 요청했습니다</td>\n",
       "      <td>ko: 우리는 그를 보호하기로 했어요</td>\n",
       "      <td>0.064566</td>\n",
       "      <td>0.809385</td>\n",
       "      <td>0.825436</td>\n",
       "      <td>662</td>\n",
       "      <td>802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>en: And yours, I'm afraid. Idea of the month g...</td>\n",
       "      <td>ko: 당신도 물론이고</td>\n",
       "      <td>ko: 그리고 내가 당신의 계획을 두려워하는 거야</td>\n",
       "      <td>0.064566</td>\n",
       "      <td>0.809385</td>\n",
       "      <td>0.825436</td>\n",
       "      <td>662</td>\n",
       "      <td>802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>en: Commentators on all English Channels explo...</td>\n",
       "      <td>ko: 오바마 대통령은 파키스탄이 평화적이며 안정적인 나라가 되는 것이 이웃국가인 ...</td>\n",
       "      <td>ko: 영어로 연설하는 동안에는 모든 언어가 파열되었습니다.</td>\n",
       "      <td>0.064566</td>\n",
       "      <td>0.809385</td>\n",
       "      <td>0.825436</td>\n",
       "      <td>662</td>\n",
       "      <td>802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             번역 대상 문장  \\\n",
       "0   en: What makes you think I want an intro to an...   \n",
       "1                             en: ( tires squealing )   \n",
       "2   en: That would have put you in jail for a long...   \n",
       "3   en: -He seems happy. -Happy is for ordinary pe...   \n",
       "4   en: I'll find out where she's from and get the...   \n",
       "..                                                ...   \n",
       "95              en: I do two weeks on, two weeks off.   \n",
       "96                           en: The way you left me.   \n",
       "97  en: We had to restrain him. I requested a psyc...   \n",
       "98  en: And yours, I'm afraid. Idea of the month g...   \n",
       "99  en: Commentators on all English Channels explo...   \n",
       "\n",
       "                                                참조 번역  \\\n",
       "0                     ko: 내가 너를 누구에게 소개하고 싶어한다고 생각하니?   \n",
       "1                             ko: ( tires squealing )   \n",
       "2   ko: 그리고 당신의 마약 스캔들을 근절하려고 했고요 그건 당신을 오랫동안 감옥에 ...   \n",
       "3                            ko: 성형외과에나 기웃거리게 만들고 말이지   \n",
       "4                        ko: 그녀가 어디소속이지 찾아서 물러나도록 할거야   \n",
       "..                                                ...   \n",
       "95                                   ko: 2주 일하고 2주 쉬죠   \n",
       "96                                       ko: 그래서 떠난거군   \n",
       "97                      ko: 저희가 묶어뒀습니다 정신과 진찰도 요청했습니다   \n",
       "98                                       ko: 당신도 물론이고   \n",
       "99  ko: 오바마 대통령은 파키스탄이 평화적이며 안정적인 나라가 되는 것이 이웃국가인 ...   \n",
       "\n",
       "                               생성된 번역   BLEU 점수    단문 패널티     길이 비율  번역 문장 길이  \\\n",
       "0                 ko: 내가 뭐라고 생각하는 거야?  0.064566  0.809385  0.825436       662   \n",
       "1                 ko: ( tires queing)  0.064566  0.809385  0.825436       662   \n",
       "2        ko: 그가 감옥에 갔을 때 당신은 그를 죽였을거야  0.064566  0.809385  0.825436       662   \n",
       "3                     ko: - 행복한 사람들이야  0.064566  0.809385  0.825436       662   \n",
       "4            ko: 그들을 찾아낼 수 있는 곳이 어디야?  0.064566  0.809385  0.825436       662   \n",
       "..                                ...       ...       ...       ...       ...   \n",
       "95                      ko: 2주 정도 기다려  0.064566  0.809385  0.825436       662   \n",
       "96                    ko: 당신은 나를 떠났어요  0.064566  0.809385  0.825436       662   \n",
       "97               ko: 우리는 그를 보호하기로 했어요  0.064566  0.809385  0.825436       662   \n",
       "98        ko: 그리고 내가 당신의 계획을 두려워하는 거야  0.064566  0.809385  0.825436       662   \n",
       "99  ko: 영어로 연설하는 동안에는 모든 언어가 파열되었습니다.  0.064566  0.809385  0.825436       662   \n",
       "\n",
       "    참조 문장 길이  \n",
       "0        802  \n",
       "1        802  \n",
       "2        802  \n",
       "3        802  \n",
       "4        802  \n",
       "..       ...  \n",
       "95       802  \n",
       "96       802  \n",
       "97       802  \n",
       "98       802  \n",
       "99       802  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기계 번역 모델 평가\n",
    "\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "  processed_dataset[\"test\"].select(range(100)),\n",
    "  collate_fn=seq2seq2_collator,\n",
    "  batch_size=4,\n",
    "  shuffle=False\n",
    ")\n",
    "\n",
    "generated_translated = []\n",
    "\n",
    "true_translated_ids = processed_dataset[\"test\"].select(range(100))[\"labels\"]\n",
    "\n",
    "true_translated = tokenizer.batch_decode(true_translated_ids, skip_special_tokens=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)\n",
    "        outputs = model.generate(\n",
    "            **batch,\n",
    "            max_length=1026,\n",
    "            num_beams=4,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        batch_translated = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        generated_translated.extend(batch_translated)\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "blue_score = bleu.compute(predictions=generated_translated, references=true_translated)\n",
    "# 번역 평가 지표 로깅\n",
    "print(blue_score)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"source\": [tokenizer.decode(instance[\"input_ids\"], skip_special_tokens=True) for instance in processed_dataset[\"test\"].select(range(100))],\n",
    "    \"reference\": true_translated,\n",
    "    \"generated\": generated_translated,\n",
    "    \"blue_score\": blue_score[\"bleu\"],\n",
    "    \"brevity_penalty\": blue_score[\"brevity_penalty\"],\n",
    "    \"length_ratio\": blue_score[\"length_ratio\"],\n",
    "    \"translation_length\": blue_score[\"translation_length\"],\n",
    "    \"reference_length\": blue_score[\"reference_length\"],\n",
    "}).rename(columns={\n",
    "    \"source\": \"번역 대상 문장\",\n",
    "    \"reference\": \"참조 번역\",\n",
    "    \"generated\": \"생성된 번역\",\n",
    "    \"blue_score\": \"BLEU 점수\",\n",
    "    \"brevity_penalty\": \"단문 패널티\",\n",
    "    \"length_ratio\": \"길이 비율\",\n",
    "    \"translation_length\": \"번역 문장 길이\",\n",
    "    \"reference_length\": \"참조 문장 길이\",\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLFlow 설정을 잘못해서 정확히 얼마나 걸렸는지는 모르겠다. 로그도 지워서 더 짜증난다. (ㅋㅋㅋ) 그래도 일단 눈뜨기 전에는 학습이 끝났다. \n",
    "\n",
    "```python\n",
    "per_device_train_batch_size=16,  # 훈련 배치 크기\n",
    "per_device_eval_batch_size=24,  # 평가 배치 크기\n",
    "learning_rate=2e-5,  # 학습률\n",
    "num_train_epochs=20,  # 학습 에포크 수\n",
    "eval_steps=250,  # 평가 스텝\n",
    "logging_steps=250,  # 로그 스텝\n",
    "save_steps=250,  # 저장 스텝\n",
    "seed=42,  # 랜덤 시드\n",
    "fp16=True,  # 플로트 16 비트 연산 활성화\n",
    "gradient_accumulation_steps=2,  # 그래디언트 축적 단계를 4로 설정 (메모리 사용량 줄임)\n",
    "dataloader_num_workers=4,  # 데이터 로딩에 사용할 서브프로세스 수 (병렬화)\n",
    "dataloader_pin_memory=True,  # 텐서를 CUDA 고정 메모리에 할당하여 데이터 로딩 속도 향상\n",
    "```\n",
    "그나마 최적이라고 생각되는 하이퍼파라미터를 적용했다. 안타깝게도 뭘 잘못했는지는 모르겠지만 GPU를 늘리면 오히려 2배의 예상 시간이 걸린다. 그래서 1개로 돌렸다. (?)\n",
    "그래서 책에는 없는 내용을 용케 찾아서 적용했다. 번역 결과는 참으로 참담하기 그지없다. 그래도 이정도 결과가 나오는 것도 전기값은 하는 것 같다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
