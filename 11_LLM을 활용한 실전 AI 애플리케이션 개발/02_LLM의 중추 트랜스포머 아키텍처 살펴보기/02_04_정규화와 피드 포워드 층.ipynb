{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_text_list:  ['나는', '최근', '파리', '여행을', '다녀왔다.']\n",
      "str2idx:  {'나는': 0, '최근': 1, '파리': 2, '여행을': 3, '다녀왔다.': 4}\n",
      "idx2str:  {0: '나는', 1: '최근', 2: '파리', 3: '여행을', 4: '다녀왔다.'}\n",
      "input_ids:  [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "#예제 2.1 토큰화 코드\n",
    "\n",
    "# 띄어쓰기를 기준으로 토큰화\n",
    "\n",
    "input_text = \"나는 최근 파리 여행을 다녀왔다.\"\n",
    "input_text_list = input_text.split()\n",
    "print(\"input_text_list: \", input_text_list)\n",
    "\n",
    "# 토큰 -> 아이디 딕셔너리\n",
    "str2idx = {word: i for i, word in enumerate(input_text_list)}\n",
    "# 아이디 -> 토큰 딕셔너리\n",
    "idx2str = {i: word for i, word in enumerate(input_text_list)}\n",
    "print(\"str2idx: \", str2idx)\n",
    "print(\"idx2str: \", idx2str)\n",
    "\n",
    "# 토큰을 아이디로 변환\n",
    "input_ids = [str2idx[word] for word in input_text_list]\n",
    "print(\"input_ids: \", input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input embeddings:  tensor([[ 0.7770, -0.2625, -1.0974, -0.0044,  0.2460,  1.1503,  1.1895, -0.6989,\n",
      "         -0.8676,  1.3241, -0.2955,  0.1588,  0.4450,  1.3490,  1.2603,  0.5509],\n",
      "        [-0.0305, -0.0896, -0.6064,  0.1746, -0.5936, -0.9227,  0.8282, -0.6571,\n",
      "         -1.3590,  0.3900,  0.1317, -1.0009,  0.7452, -2.0369,  2.4439, -1.3522],\n",
      "        [-1.6635, -0.5571, -0.7051, -0.3117, -0.2677, -1.1479,  0.0657,  0.8396,\n",
      "          0.5488,  0.5365,  1.2385,  1.4215,  0.2737, -0.0441,  0.7447, -0.8945],\n",
      "        [-0.3179, -1.5514,  0.6538, -0.7509,  0.2890, -0.0280,  0.4640, -1.3025,\n",
      "          0.8520, -0.0179,  0.2255,  0.4587, -1.2693,  0.9345,  0.6715,  1.0444],\n",
      "        [ 1.2534, -0.3619, -0.9062, -1.9034,  2.3410, -0.5002,  0.6875,  2.0066,\n",
      "          0.5438,  1.3301,  1.3833, -1.2271,  1.3359, -1.3306, -1.0203,  0.3393]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "length of input embeddings:  5\n",
      "input embeddings shape torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "#예제 2.2 토큰 아이디에서 벡터로 변환\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "embedding_dim = 16 # 임베딩 차원\n",
    "\n",
    "# 임베딩 레이어 생성\n",
    "embed_layer = nn.Embedding(len(str2idx), embedding_dim)\n",
    "\n",
    "# 임베딩 레이어에 토큰 아이디를 입력하여 임베딩 벡터를 얻음\n",
    "input_embeddings = embed_layer(torch.tensor(input_ids))\n",
    "\n",
    "print(\"input embeddings: \", input_embeddings)\n",
    "print(\"length of input embeddings: \", len(input_embeddings))\n",
    "print(\"input embeddings shape\", input_embeddings.unsqueeze(0).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 16])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예제 2.3 절대적 위치 인코딩\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "embedding_dim = 16 # 임베딩 차원\n",
    "max_position = 12 # 최대 토큰 수\n",
    "\n",
    "embed_layer = nn.Embedding(len(str2idx), embedding_dim)\n",
    "position_embed_layer = nn.Embedding(max_position, embedding_dim)\n",
    "\n",
    "position_ids = torch.arange(len(input_ids), dtype=torch.long).unsqueeze(0)\n",
    "position_encodings = position_embed_layer(position_ids)\n",
    "\n",
    "token_embeddings = embed_layer(torch.tensor(input_ids))\n",
    "token_embeddings = token_embeddings.unsqueeze(0)\n",
    "\n",
    "input_embeddings = token_embeddings + position_encodings\n",
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input ids:  [0, 1, 2, 3, 4]\n",
      "position ids:  tensor([[0, 1, 2, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "print(\"input ids: \", input_ids)\n",
    "print(\"position ids: \", position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "querys shape:  torch.Size([1, 5, 16])\n",
      "keys shape:  torch.Size([1, 5, 16])\n",
      "values shape:  torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "# 쿼리, 키, 값 벡터를 만드는 nn.Linear 층\n",
    "\n",
    "# 쿼리, 키, 값을 계산하기 위한 변환\n",
    "weight_q = nn.Linear(embedding_dim, embedding_dim)\n",
    "weight_k = nn.Linear(embedding_dim, embedding_dim)\n",
    "weight_v = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "querys = weight_q(input_embeddings) \n",
    "keys = weight_k(input_embeddings)\n",
    "values = weight_v(input_embeddings)\n",
    "\n",
    "print(\"querys shape: \", querys.shape)\n",
    "print(\"keys shape: \", keys.shape)\n",
    "print(\"values shape: \", values.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 2.5 스케일 점곱 방식의 어텐션\n",
    "\n",
    "from math import sqrt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_attention(querys, keys, values, is_causal=False):\n",
    "    dim_k = querys.size(-1)\n",
    "    # 쿼리와 키의 내적을 구하고 스케일링\n",
    "    scores = querys @ keys.transpose(-2, -1) / sqrt(dim_k)\n",
    "    \n",
    "    # 소프트맥스 함수를 통해 어텐션 가중치 계산\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # 가중치와 값의 곱으로 어텐션 값 계산한다. 입력과 동일한 차원을 가진다.\n",
    "    outputs = weights @ values\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 입력 형태: torch.Size([1, 5, 16])\n",
      "어텐션 후 입력 형태: torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "# 예제 2.6 어텐션 연산의 입력과 출력\n",
    "\n",
    "print(\"원본 입력 형태:\", input_embeddings.shape)\n",
    "\n",
    "after_attention_embeddings = compute_attention(querys, keys, values)\n",
    "\n",
    "print(\"어텐션 후 입력 형태:\", after_attention_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.7 어텐션 연산을 수행하는 AttentionHead 클래스\n",
    "import torch.nn as nn\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, token_embed_dim, head_dim, is_causal=False):\n",
    "        super().__init__()\n",
    "        # 헤드 차원\n",
    "        self.is_causal = is_causal\n",
    "        # 쿼리 벡터 생성을 위한 선형 층\n",
    "        self.weight_q = nn.Linear(token_embed_dim, head_dim)\n",
    "        # 키 벡터 생성을 위한 선형 층\n",
    "        self.weight_k = nn.Linear(token_embed_dim, head_dim)\n",
    "        # 값 벡터 생성을 위한 선형 층\n",
    "        self.weight_v = nn.Linear(token_embed_dim, head_dim)\n",
    "\n",
    "    def forward(self, querys, keys, values):\n",
    "        outputs = compute_attention(\n",
    "            self.weight_q(querys),\n",
    "            self.weight_k(keys),\n",
    "            self.weight_v(values)\n",
    "        )\n",
    "        return outputs\n",
    "    \n",
    "attention_head = AttentionHead(embedding_dim, embedding_dim)\n",
    "after_attention_embeddings = attention_head(input_embeddings, input_embeddings, input_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4. 멀티 헤드 어텐션\n",
    "\n",
    "- 트랜스포머의 핵심 구성 요소 중 하나인 멀티 헤드 어텐션에 대해 알아보자.\n",
    "- 멀티 헤드 어텐션은 어텐션을 병렬로 수행하여 어텐션의 품질을 높이는 방법이다.\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    q[Query]\n",
    "    k[Key]\n",
    "    v[Value]\n",
    "\n",
    "    linear1[Linear]\n",
    "    linear2[Linear]\n",
    "    linear3[Linear]\n",
    "\n",
    "    q --> linear1\n",
    "    k --> linear2\n",
    "    v --> linear3\n",
    "\n",
    "    scale[Scale Dot Product * h]\n",
    "    connect[Connect]\n",
    "    final_linear[Final Linear]\n",
    "\n",
    "    linear1 --> scale\n",
    "    linear2 --> scale\n",
    "    linear3 --> scale\n",
    "\n",
    "    scale --> connect\n",
    "    connect --> final_linear\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 16])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 멀티 헤드 어텐션 구현\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, token_embed_dim, d_model, h_head, is_causal=False):\n",
    "        super().__init__()\n",
    "        self.n_head = h_head\n",
    "        self.is_causal = is_causal\n",
    "        self.weight_q = nn.Linear(token_embed_dim, d_model)\n",
    "        self.weight_k = nn.Linear(token_embed_dim, d_model)\n",
    "        self.weight_v = nn.Linear(token_embed_dim, d_model)\n",
    "        self.concat_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, querys, keys, values):\n",
    "        \n",
    "        B, T, C = querys.size()\n",
    "        querys = self.weight_q(querys).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # 1번째 선형 층 적용\n",
    "        keys = self.weight_k(keys).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # 2번째 선형 층 적용\n",
    "        values = self.weight_v(values).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # 3번째 선형 층 적용\n",
    "\n",
    "        attention = compute_attention(querys, keys, values, self.is_causal) # h번의 스케일 점곱 어텐션 수행\n",
    "        output = attention.transpose(1, 2).reshape(B, T, C) # 어텐션 결과를 연결\n",
    "        output = self.concat_linear(output) # 마지막 선형층 적용\n",
    "        return output\n",
    "    \n",
    "n_head = 4\n",
    "mh_attention = MultiHeadAttention(embedding_dim, embedding_dim, n_head)\n",
    "after_attention_embeddings = mh_attention(input_embeddings, input_embeddings, input_embeddings)\n",
    "after_attention_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. 정규화와 피드 포워드 층\n",
    "\n",
    "정규화란, 딥러닝 모델에서 입력이 일정한 분포를 갖도록 만들어 학습이 안정적이고 빠르게 이루어지도록 하는 방법이다. 과거에는 배치 정규화(Batch Normalization)이 주로 사용되었지만, 트랜스포머 아키텍처에서는 특징 차원에서 정규화를 수행하는 층 정규화(Layer Normalization)를 사용한다.\n",
    "어텐션 연산이 입력 단어 사이의 관계를 계산해 토큰 임베딩을 조정하는 역할을 한다면 전체 입력문장을 이해하는 연산은 완전 연결 층(Fully Connected Layer)이다. 이 층은 입력과 출력의 차원이 같은 행렬 곱셈을 수행하고, 이후에 활성화 함수를 통과시켜 비선형성을 추가한다. 이러한 층을 피드 포워드 층(Feed Forward Layer)이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 층 정규화 이해하기\n",
    "\n",
    "벡터 $x$를 정규화한 $norm_x$는 벡터 $x$에서 $x$의 평균을 빼고 $x$의 표준편차로 나눠 평균이 $0$이고 표준편차가 $1$인 분포로 만든다.\n",
    "$$\n",
    "\\text{norm}_x = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "**배치 정규화(Batch Normalization)**\n",
    "\n",
    "배치 정규화는 모델에 입력으로 들어가는 미니 배치 사이의 정규화를 수행한다. 자연어 처리에서는 배치 정규화를 사용하지 않는 이유는 자연어 처리에서 배치에 들어가는 입력의 길이가 다양하기 때문이다.\n",
    "\n",
    "**층 정규화(Layer Normalization)**\n",
    "\n",
    "층 정규화는 배치 정규화와 달리 각 토큰 임베딩의 평균과 표준편차를 구해 정규화를 수행한다. 이를 통해 입력의 분포를 안정화시킬 수 있다.\n",
    "이려면 배치 정규화의 단점을 보완할 수 있다.\n",
    "\n",
    "**층 정규화의 2가지 방식**\n",
    "\n",
    "**사후 정규화(Post-LN)**\n",
    "어텐션관 피드 포워드 층 이후에 층 정규화를 수행하는 방식이다.\n",
    "\n",
    "**사전 정규화(Pre-LN)**\n",
    "층 정규화를 수행한 후에 어텐션과 피드 포워드 층을 수행하는 방식이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_x shape:  torch.Size([1, 5, 16])\n",
      "tensor([[-1.8626e-09,  0.0000e+00,  0.0000e+00, -2.2352e-08, -2.9802e-08]]) tensor([[1.0327, 1.0326, 1.0326, 1.0328, 1.0327]])\n"
     ]
    }
   ],
   "source": [
    "# 2.9 층 정규화 (사전 정규화)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "norm = nn.LayerNorm(embedding_dim)\n",
    "norm_x = norm(after_attention_embeddings)\n",
    "print(\"norm_x shape: \", norm_x.shape)\n",
    "\n",
    "print(norm_x.mean(dim=-1).data, norm_x.std(dim=-1).data) # 실제로 평균과 표준편차 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
