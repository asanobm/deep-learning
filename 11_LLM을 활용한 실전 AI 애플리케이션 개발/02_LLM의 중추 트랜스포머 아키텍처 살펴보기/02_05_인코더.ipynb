{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. 인코더\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    input[입력]\n",
    "    token_embedding[토큰 임베딩]\n",
    "    position_embedding[위치 임베딩]\n",
    "    layer_norm[층 정규화]\n",
    "    multi_head_attention[멀티 헤드 어텐션]\n",
    "    layer_norm2[층 정규화]\n",
    "    feed_forward[피드 포워드 신경망]\n",
    "    layer_norm3[층 정규화]\n",
    "\n",
    "    input --> token_embedding\n",
    "    token_embedding --> position_embedding\n",
    "    position_embedding --> layer_norm\n",
    "    subgraph \"인코더 층 반복\"\n",
    "        layer_norm --> multi_head_attention\n",
    "    multi_head_attention --> A\n",
    "    A@{ shape: cross-circ, label: \"Summary\" } --> layer_norm2\n",
    "        layer_norm2 --> feed_forward\n",
    "    end\n",
    "    A --> layer_norm\n",
    "    feed_forward --> B\n",
    "    B@{ shape: cross-circ, label: \"Summary\" } --> layer_norm3\n",
    "    B --> A\n",
    "    layer_norm3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 2.11 인코더층\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class PreLayerNormFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward) # 1번째 선형 층\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model) # 2번째 선형 층\n",
    "        self.dropout1 = nn.Dropout(dropout) # 드롭아웃 1\n",
    "        self.dropout2 = nn.Dropout(dropout) # 드롭아웃 2\n",
    "        self.activation = nn.GELU() # 활성화 함수\n",
    "        self.norm = nn.LayerNorm(d_model) # 층 정규화\n",
    "\n",
    "    def forward(self, src):\n",
    "        x = self.norm(src)\n",
    "        x = x + self.linear2(self.activation(self.linear1(x))) # 2개의 선형 층과 활성화 함수\n",
    "        x = self.dropout2(x) # 드롭아웃 2\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    TransformerEncoderLayer 클래스는 트랜스포머 인코더의 한 층을 구현합니다.\n",
    "    초기화 메서드:\n",
    "    메서드:\n",
    "        forward(src):\n",
    "            주어진 입력에 대해 트랜스포머 인코더 층을 연산합니다.\n",
    "                src (Tensor): 입력 텐서.\n",
    "            Returns:\n",
    "                Tensor: 트랜스포머 인코더 층을 통과한 출력 텐서.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_head, dim_feedforward, dropout):\n",
    "        \"\"\"\n",
    "        초기화 메서드\n",
    "        Args:\n",
    "            d_model (int): 모델의 차원.\n",
    "            n_head (int): 멀티헤드 어텐션의 헤드 수.\n",
    "            dim_feedforward (int): 피드포워드 신경망의 차원.\n",
    "            dropout (float): 드롭아웃 확률.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.atten = nn.MultiheadAttention(d_model, d_model, n_head) # 멀티헤드 어텐션 층\n",
    "        self.norm1 = nn.LayerNorm(d_model) # 층 정규화\n",
    "        self.dropout1 = nn.Dropout(dropout) # 드롭아웃\n",
    "        self.feed_forward = PreLayerNormFeedForward(d_model, dim_feedforward, dropout) # 피드포워드 신경망\n",
    "\n",
    "    def forward(self, src):\n",
    "        norm_x = self.norm1(src) # norm1을 적용해 층 정규화\n",
    "        atten_output = self.atten(norm_x, norm_x, norm_x) # 멀티헤드 어텐션을 연산\n",
    "        x = src + self.dropout1(atten_output) # 드롭아웃을 적용해 어텐션 출력을 더함\n",
    "        x = self.feed_forward(x) # 피드포워드 신경망을 연산\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 2.12 인코더 구현\n",
    "\n",
    "import copy\n",
    "\n",
    "embedding_dim = 16 # 임베딩 차원\n",
    "norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src):\n",
    "        output = src\n",
    "        for mod in self.layers:\n",
    "            output = mod(output)\n",
    "        return self.norm(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
