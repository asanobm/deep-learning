{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. BERT, GPT, T5 등 트랜스포머를 활용한 아키텍처\n",
    "\n",
    "트랜스포머를 활용한 아키텍처 구분\n",
    "\n",
    "- 인코더만 활용해 자연어 이해(Natural Language Understanding, NLU)를 수행하는 아키텍처\n",
    "- 디코더만 활용해 자연어 생성(Natural Language Generation, NLG)을 수행하는 아키텍처\n",
    "- 인코더와 디코더를 모두 활용해 자연어 이해와 생성을 모두 수행하는 아키텍처\n",
    "\n",
    "| 모델 그룹 | 대표 모델 | 장점 | 단점 |\n",
    "|:---:|:---:|:---|:---|\n",
    "| 인코더 | BERT | 양방향 이해를 통해 자연어 이해에서 일반적으로 디코더 모델 대비 높은 성능 <br/> 입력에 대해 병렬 연산이 가능하므로 빠른 학습과 추론이 가능 <br/> 다양한 작업에 대한 다운스트림 성능이 뛰어남 | 자연어 생성 작업에 적합하지 않음 <br/> 컨텍스트의 길이가 제한적임 |\n",
    "| 디코더 | GPT | 생성 작업에서 뛰어난 성능을 보임 <br/> 비교적 긴 컨텍스트 길이에 대해서도 성능이 좋음 | 양방향이 아닌 단방향 방식으로 자연어 이해 작업에서 비교적 성능이 낮음 <br/> 모든 작업을 생성 작업으로 변환할 수 있으나 비효율적일 수 있음|\n",
    "| 인코더-디코더 | BART, T5 | 생성과 이해 작업 모두에서 성능이 좋음 <br/> 이해 작업에서 양방향 방식을 사용할 수 있고 인코더의 결과를 디코더에서 활용할 수 있어 문맥을 반영한 생성 능력이 좋음 | 인코더와 디코더를 모두 활용하기 때문에 복잡함 <br/> 학습에 더 많은 데이터와 컴퓨팅 자원이 필요함 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.1. 인코더를 활용한 BERT\n",
    "\n",
    "BERT는 Bidirectional Encoder Representations from Transformers의 약자로, 2018년 구글에서 발표한 트랜스포머 모델입니다. BERT는 인코더만을 활용해 자연어 이해(Natural Language Understanding, NLU) 작업을 수행하는 아키텍처입니다. BERT는 양방향 이해를 통해 자연어 이해에서 일반적으로 디코더 모델 대비 높은 성능을 보이며, 입력에 대해 병렬 연산이 가능하므로 빠른 학습과 추론이 가능합니다. 또한 다양한 작업에 대한 다운스트림 성능이 뛰어난 것으로 알려져 있습니다. 하지만 자연어 생성 작업에 적합하지 않으며, 컨텍스트의 길이가 제한적이라는 단점이 있습니다.\n",
    "\n",
    "### 2.7.2. 디코더를 활용한 GPT\n",
    "\n",
    "GPT는 Generative Pre-trained Transformer의 약자로, 2018년 오픈AI에서 발표한 트랜스포머 모델입니다. GPT는 디코더만을 활용해 자연어 생성(Natural Language Generation, NLG) 작업을 수행하는 아키텍처입니다. GPT는 생성 작업에서 뛰어난 성능을 보이며, 비교적 긴 컨텍스트 길이에 대해서도 성능이 좋습니다. 하지만 양방향이 아닌 단방향 방식으로 자연어 이해 작업에서 비교적 성능이 낮으며, 모든 작업을 생성 작업으로 변환할 수 있으나 비효율적일 수 있습니다.\n",
    "\n",
    "### 2.7.3. 인코더-디코더를 활용한 BART, T5\n",
    "\n",
    "BART와 T5는 모두 인코더와 디코더를 모두 활용해 자연어 이해와 생성을 모두 수행하는 아키텍처입니다. BART와 T5는 생성과 이해 작업 모두에서 성능이 좋으며, 이해 작업에서 양방향 방식을 사용할 수 있고 인코더의 결과를 디코더에서 활용할 수 있어 문맥을 반영한 생성 능력이 좋습니다. 하지만 인코더와 디코더를 모두 활용하기 때문에 복잡하며, 학습에 더 많은 데이터와 컴퓨팅 자원이 필요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
