{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch에서 변화도를 0으로 만들기\n",
    "\n",
    "신경망을 구축할 때 변화도를 0으로 만들어 주는 것이 좋다. (?)\n",
    "\n",
    "기본적으로 역전파`backward()`를 호출할 때마다 변화도가 버퍼에 쌓이기 때문이다.\n",
    "\n",
    "## 개요\n",
    "\n",
    "신경망을 학습시킬 때, 경사 하강법을 거쳐 모델 정확도를 높인다.\n",
    "\n",
    "`torch.Tensor`는 PyTorch의 핵심이 되는 클래스다. 텐서를 생성할 때`requires_grad`속성을 `True`로 설정하면, 텐서의 모든 연산을 추적한다. 역전파 단계에서도 마찬가지로 변화도가 누적된다.\n",
    "\n",
    "텐서의 변화도를 0으로 만들어 주어야 하는 경우도 있다. 학습 과정 반복문을 시작할 때, 누적되는 변화도를 정확하기 추적하기 위해서는 변화도를 0으로 만들어 주어야 한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "DEVICE = get_device()\n",
    "print(\"DEVICE: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "## Load dataset and nomalization.\n",
    "\n",
    "transform = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "  root='./data',\n",
    "  train=True,\n",
    "  download=True,\n",
    "  transform=transform\n",
    ")\n",
    "\n",
    "trainloader = data.DataLoader(\n",
    "  trainset,\n",
    "  batch_size=4,\n",
    "  shuffle=True,\n",
    "  num_workers=2\n",
    ")\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "  root='./data',\n",
    "  train=False,\n",
    "  download=True,\n",
    "  transform=transform\n",
    ")\n",
    "\n",
    "testloader = data.DataLoader(\n",
    "  testset,\n",
    "  batch_size=4,\n",
    "  shuffle=False,\n",
    "  num_workers=2\n",
    ")\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 신경망 구축하기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    \n",
    "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "    self.pool = nn.MaxPool2d(2, 2)\n",
    "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "    self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "    self.fc2 = nn.Linear(120, 84)\n",
    "    self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = self.pool(F.relu(self.conv1(x)))\n",
    "    x = self.pool(F.relu(self.conv2(x)))\n",
    "    x = x.view(-1, 16 * 5 * 5)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.fc3(x)\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 손실 함수와 옵티마이저 정의하기\n",
    "\n",
    "분류를 위한 Cross-Entropy 손실 함수와 확률적 경사 하강법(SGD)을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 신경망을 학습시키는 과정에서 변화도를 0으로 만들기\n",
    "\n",
    "반복적으로 신경망에 데이터를 입력하면서 최적화 한다. 이때 변화도를 0으로 만들어 주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.209\n",
      "[1,  4000] loss: 1.881\n",
      "[1,  6000] loss: 1.704\n",
      "[1,  8000] loss: 1.603\n",
      "[1, 10000] loss: 1.540\n",
      "[1, 12000] loss: 1.470\n",
      "[2,  2000] loss: 1.410\n",
      "[2,  4000] loss: 1.363\n",
      "[2,  6000] loss: 1.359\n",
      "[2,  8000] loss: 1.303\n",
      "[2, 10000] loss: 1.309\n",
      "[2, 12000] loss: 1.266\n",
      "[3,  2000] loss: 1.233\n",
      "[3,  4000] loss: 1.209\n",
      "[3,  6000] loss: 1.201\n",
      "[3,  8000] loss: 1.190\n",
      "[3, 10000] loss: 1.199\n",
      "[3, 12000] loss: 1.193\n",
      "[4,  2000] loss: 1.101\n",
      "[4,  4000] loss: 1.132\n",
      "[4,  6000] loss: 1.113\n",
      "[4,  8000] loss: 1.117\n",
      "[4, 10000] loss: 1.122\n",
      "[4, 12000] loss: 1.109\n",
      "[5,  2000] loss: 1.024\n",
      "[5,  4000] loss: 1.038\n",
      "[5,  6000] loss: 1.031\n",
      "[5,  8000] loss: 1.063\n",
      "[5, 10000] loss: 1.065\n",
      "[5, 12000] loss: 1.044\n",
      "[6,  2000] loss: 0.977\n",
      "[6,  4000] loss: 0.967\n",
      "[6,  6000] loss: 0.997\n",
      "[6,  8000] loss: 0.998\n",
      "[6, 10000] loss: 1.013\n",
      "[6, 12000] loss: 0.999\n",
      "[7,  2000] loss: 0.916\n",
      "[7,  4000] loss: 0.908\n",
      "[7,  6000] loss: 0.947\n",
      "[7,  8000] loss: 0.953\n",
      "[7, 10000] loss: 0.937\n",
      "[7, 12000] loss: 0.947\n",
      "[8,  2000] loss: 0.843\n",
      "[8,  4000] loss: 0.900\n",
      "[8,  6000] loss: 0.873\n",
      "[8,  8000] loss: 0.909\n",
      "[8, 10000] loss: 0.913\n",
      "[8, 12000] loss: 0.944\n",
      "[9,  2000] loss: 0.827\n",
      "[9,  4000] loss: 0.862\n",
      "[9,  6000] loss: 0.872\n",
      "[9,  8000] loss: 0.843\n",
      "[9, 10000] loss: 0.878\n",
      "[9, 12000] loss: 0.889\n",
      "[10,  2000] loss: 0.773\n",
      "[10,  4000] loss: 0.804\n",
      "[10,  6000] loss: 0.825\n",
      "[10,  8000] loss: 0.849\n",
      "[10, 10000] loss: 0.855\n",
      "[10, 12000] loss: 0.868\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "  running_loss = 0.0\n",
    "  for i, data in enumerate(trainloader, 0):\n",
    "    inputs, labels = data\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 통계 출력\n",
    "    running_loss += loss.item()\n",
    "    if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "      print('[%d, %5d] loss: %.3f' %\n",
    "            (epoch + 1, i + 1, running_loss / 2000))\n",
    "      running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
