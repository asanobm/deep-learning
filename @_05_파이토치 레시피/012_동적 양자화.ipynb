{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 동적 양자화\n",
    "\n",
    "동적 양자화(dynamic quantization)를 활용하여, LSTM과 유사한 형태의 순환 신경망이 좀더 빠르게 추론하도록 만드는 방법이 있댄다!\n",
    "\n",
    "이 방법은 모델에서 사용하는 가중치의 규모를 줄이고 수행 속도를 빠르게 만든다.\n",
    "\n",
    "## 도입\n",
    "\n",
    "양자화는 모델의 가중치를 정수로 변환하는 과정이다. 이는 모델의 가중치를 훨씬 작은 크기로 표현할 수 있게 해주며, 이는 모델의 메모리 사용량을 줄이고, 모델의 추론 속도를 빠르게 만든다.\n",
    "\n",
    "## 동적 양자화란 무엇인가?\n",
    "\n",
    "신경망을 양자화한다는 말의 의미는 가중치나 활성화 함수에서 정밀도가 낮은 정수 표현을 사용하도록 바꾼다는 것이다. 이를 통해 모델의 규모를 줄일 수 있다. \n",
    "\n",
    "부동소수점 실수 값을 정수로 바꾸는 것은 본질적으로 실수 값에 어떤 배울을 곱하여 그 결과값을 정수로 반올림 하는 것과 같다. 이 배율을 어떻게 정할 것이냐에 따라 양자화하는 방법도 여러가지로 나뉜다.\n",
    "\n",
    "핵심은 모델을 수행할 때 데이터의 범위를 보고 적절한 배율을 선택하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.quantization\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a very, very simple LSTM for demonstration purposes\n",
    "# in this case, we are wrapping ``nn.LSTM``, one layer, no preprocessing or postprocessing\n",
    "# inspired by\n",
    "# `Sequence Models and Long Short-Term Memory Networks tutorial <https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html`_, by Robert Guthrie\n",
    "# and `Dynamic Quanitzation tutorial <https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html>`__.\n",
    "class lstm_for_demonstration(nn.Module):\n",
    "  \"\"\"Elementary Long Short Term Memory style model which simply wraps ``nn.LSTM``\n",
    "     Not to be used for anything other than demonstration.\n",
    "  \"\"\"\n",
    "  def __init__(self,in_dim,out_dim,depth):\n",
    "     super(lstm_for_demonstration,self).__init__()\n",
    "     self.lstm = nn.LSTM(in_dim,out_dim,depth)\n",
    "\n",
    "  def forward(self,inputs,hidden):\n",
    "     out,hidden = self.lstm(inputs,hidden)\n",
    "     return out, hidden\n",
    "\n",
    "\n",
    "torch.manual_seed(29592)  # set the seed for reproducibility\n",
    "\n",
    "#shape parameters\n",
    "model_dimension=8\n",
    "sequence_length=20\n",
    "batch_size=1\n",
    "lstm_depth=1\n",
    "\n",
    "# random data for input\n",
    "inputs = torch.randn(sequence_length,batch_size,model_dimension)\n",
    "# hidden is actually is a tuple of the initial hidden state and the initial cell state\n",
    "hidden = (torch.randn(lstm_depth,batch_size,model_dimension), torch.randn(lstm_depth,batch_size,model_dimension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Didn't find engine for operation quantized::linear_prepack NoQEngine",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m float_lstm \u001b[38;5;241m=\u001b[39m lstm_for_demonstration(model_dimension, model_dimension,lstm_depth)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# this is the call that does the work\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m quantized_lstm \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize_dynamic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m   \u001b[49m\u001b[43mfloat_lstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLSTM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqint8\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# show the changes that were made\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHere is the floating point version of this module:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-study/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:465\u001b[0m, in \u001b[0;36mquantize_dynamic\u001b[0;34m(model, qconfig_spec, dtype, mapping, inplace)\u001b[0m\n\u001b[1;32m    463\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    464\u001b[0m propagate_qconfig_(model, qconfig_spec)\n\u001b[0;32m--> 465\u001b[0m \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-study/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:550\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[1;32m    549\u001b[0m     module \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(module)\n\u001b[0;32m--> 550\u001b[0m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_qconfig:\n\u001b[1;32m    554\u001b[0m     _remove_qconfig(module)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-study/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:590\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    587\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[1;32m    588\u001b[0m         _convert(mod, mapping, \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# inplace\u001b[39;00m\n\u001b[1;32m    589\u001b[0m                  is_reference, convert_custom_config_dict)\n\u001b[0;32m--> 590\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m \u001b[43mswap_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_module_class_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    593\u001b[0m     module\u001b[38;5;241m.\u001b[39m_modules[key] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-study/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:623\u001b[0m, in \u001b[0;36mswap_module\u001b[0;34m(mod, mapping, custom_module_class_mapping)\u001b[0m\n\u001b[1;32m    621\u001b[0m         new_mod \u001b[38;5;241m=\u001b[39m qmod\u001b[38;5;241m.\u001b[39mfrom_float(mod, weight_qparams)\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 623\u001b[0m         new_mod \u001b[38;5;241m=\u001b[39m \u001b[43mqmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_float\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m     swapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m swapped:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;66;03m# Preserve module's pre forward hooks. They'll be called on quantized input\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-study/lib/python3.11/site-packages/torch/ao/nn/quantized/dynamic/modules/rnn.py:495\u001b[0m, in \u001b[0;36mLSTM.from_float\u001b[0;34m(cls, mod)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_float\u001b[39m(\u001b[38;5;28mcls\u001b[39m, mod):\n\u001b[0;32m--> 495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_float\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-study/lib/python3.11/site-packages/torch/ao/nn/quantized/dynamic/modules/rnn.py:291\u001b[0m, in \u001b[0;36mRNNBase.from_float\u001b[0;34m(cls, mod)\u001b[0m\n\u001b[1;32m    289\u001b[0m qRNNBase: Union[LSTM, GRU]\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mod\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 291\u001b[0m     qRNNBase \u001b[38;5;241m=\u001b[39m \u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mod\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGRU\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    294\u001b[0m     qRNNBase \u001b[38;5;241m=\u001b[39m GRU(mod\u001b[38;5;241m.\u001b[39minput_size, mod\u001b[38;5;241m.\u001b[39mhidden_size, mod\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    295\u001b[0m                    mod\u001b[38;5;241m.\u001b[39mbias, mod\u001b[38;5;241m.\u001b[39mbatch_first, mod\u001b[38;5;241m.\u001b[39mdropout, mod\u001b[38;5;241m.\u001b[39mbidirectional, dtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-study/lib/python3.11/site-packages/torch/ao/nn/quantized/dynamic/modules/rnn.py:400\u001b[0m, in \u001b[0;36mLSTM.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLSTM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-study/lib/python3.11/site-packages/torch/ao/nn/quantized/dynamic/modules/rnn.py:119\u001b[0m, in \u001b[0;36mRNNBase.__init__\u001b[0;34m(self, mode, input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional, dtype)\u001b[0m\n\u001b[1;32m    116\u001b[0m w_ih \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mquantize_per_tensor(w_ih, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, zero_point\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mqint8)\n\u001b[1;32m    117\u001b[0m w_hh \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mquantize_per_tensor(w_hh, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, zero_point\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mqint8)\n\u001b[1;32m    118\u001b[0m packed_ih \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m--> 119\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_prepack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw_ih\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_ih\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m packed_hh \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m    121\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mlinear_prepack(w_hh, b_hh)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-study/lib/python3.11/site-packages/torch/_ops.py:692\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Didn't find engine for operation quantized::linear_prepack NoQEngine"
     ]
    }
   ],
   "source": [
    " # here is our floating point instance\n",
    "float_lstm = lstm_for_demonstration(model_dimension, model_dimension,lstm_depth)\n",
    "\n",
    "# this is the call that does the work\n",
    "quantized_lstm = torch.quantization.quantize_dynamic(\n",
    "    float_lstm, {nn.LSTM, nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# show the changes that were made\n",
    "print('Here is the floating point version of this module:')\n",
    "print(float_lstm)\n",
    "print('')\n",
    "print('and now the quantized version:')\n",
    "print(quantized_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fixme: https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html\n",
    "\n",
    "fixme: https://tutorials.pytorch.kr/recipes/recipes/dynamic_quantization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
