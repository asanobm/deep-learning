{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. 제로샷 이미지 분류:CLIP (Contrastive Language–Image Pre-training)\n",
    "\n",
    "제로샷 이미지 분류(Zero-shot image classification)는 학습 데이터세트에 존재하지 않는 새로운 레이블에 대해 이미지 분류를 수행할 수 있는 기술을 의미지한다. 전통적인 이미지 분류 모델은 대규모의 레이블링된 데이터세트가 필수적이었다. 또한, 이러한 데이터 레이블링 작업에는 많은 비용이 발생한다. 새로운 클래스가 추가되면 재학습이 필요하다.\n",
    "\n",
    "제로샷 이미지 분류는 사전 학습된 멀티모달 모델을 활용해, 레이블링되지 않은 이미지에 대해서도 분류가 가능하다. 이 기술은 이미지와 텍스트 간 연관성을 학습한다. 주어진 이미지에 대해 가장 적절한 텍스트를 찾거나 반대로 특정 텍스트에 대해 가장 적절한 이미지를 찾는 것이 가능하다.\n",
    "\n",
    "**대표적인 제로샷 이미지 분류 모델**\n",
    "\n",
    "* **CLIP (Contrastive Language–Image Pre-training)**: 대규모 이미지-텍스트 데이터세트로 사전 학습되어, 이미지와 자연어 설명 간의 관계를 효과적으로 포착한다.\n",
    "* **ALIGN (Adaptive Latent Information Network)**: EfficientNet을 비전 인코더로, BERT를 텍스트 인코더로 사용하는 듀얼 인코더 아키텍처를 갖는다. **대조 학습(Contrastive Learning)** 을 통해 이미지와 텍스트 간의 관계를 학습한다.\n",
    "* **SigLIP (Sign Language Image Pre-training)**: CLIP에서 사용된 손실 함수를 간단한 쌍별(pairwise) 손실 함수로 대체하여, 수화 이미지 분류를 수행한다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP (Contrastive Language–Image Pre-training)\n",
    "\n",
    "CLIP (Contrastive Language–Image Pre-training)은 2021년 OpenAI에서 발표한 멀티모달 모델로, 이미지와 텍스트 간의 연관성을 학습해 다양한 컴퓨터비전 및 자연어 처리 과제에서 높은 성능을 보여주고 있다.\n",
    "\n",
    "#### 특징\n",
    "\n",
    "1. **제로샷 학습 능력**: 교육되지 않은 새로운 클래스의 이미지에 대해서도 텍스트 설명을 바탕으로 이미지를 분류할 수 있습니다.\n",
    "2. **다양한 데이터 사용**: 웹에서 수집된 방대한 양의 이미지-텍스트 쌍으로 학습되어, 다양한 주제와 스타일의 데이터를 포함합니다.\n",
    "3. **범용성**: 특정 데이터셋에 국한되지 않고 다양한 이미지와 언어 데이터를 처리할 수 있습니다.\n",
    "\n",
    "#### 모델 구조\n",
    "\n",
    "##### 1. 이미지 인코더\n",
    "- **구조**: 변형된 ResNet 또는 Vision Transformer (ViT) 사용.\n",
    "- **작동 방식**: 입력 이미지를 고차원의 특징 벡터로 변환합니다.\n",
    "\n",
    "##### 2. 텍스트 인코더\n",
    "- **구조**: 트랜스포머(transformer) 아키텍처 사용.\n",
    "- **작동 방식**: 입력 문장을 특징 벡터로 변환하며, 문서 전체의 의미를 포착합니다.\n",
    "\n",
    "##### 3. 대조 학습\n",
    "- **협력 학습**: 이미지와 텍스트 인코더 출력 벡터를 동일한 잠재 공간에 배치하여 유사성을 학습합니다.\n",
    "- **목표**: 짝이 맞는 이미지-텍스트 쌍의 유사성을 극대화하고, 무작위 쌍 간의 유사성을 최소화합니다.\n",
    "\n",
    "#### 대칭적 대조 손실 함수\n",
    "\n",
    "- **기본 개념**: 이미지와 텍스트 쌍의 유사성을 비교하여 올바른 쌍과 잘못된 쌍 간의 차이를 학습합니다.\n",
    "- **코사인 유사도**: 이미지와 텍스트 벡터 간의 코사인 유사도를 사용합니다.\n",
    "- **핵심 특징**: 대칭적인 접근 방식으로 이미지 인코더와 텍스트 인코더가 동등하게 학습되어 서로의 관계를 효과적으로 규명합니다.\n",
    "\n",
    "#### 결론\n",
    "\n",
    "CLIP은 이미지와 텍스트 간의 의미적 관계를 잘 학습하여, 제로샷 이미지 분류 능력을 극대화하는 데 중요한 역할을 합니다. 이는 CLIP이 훈련된 후 새로운 객체나 개념을 이해하고 인식할 수 있게 돕습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. 제로샷 이미지 분류 수행\n",
    "\n",
    "허깅페이스 라이브러리에서는 CLIP 모델을 쉽게 활용할 수 있도록 CLIPModel과 CLIPProcessor를 제공한다. 이 클래스는 CLIP 모델의 구조와 입력 데이터 전처리 과정을 API 형태로 제공한다.\n",
    "\n",
    "OpenAI CLIP 모델\n",
    "\n",
    "|모델|아키텍처|입력크기|패치크기|매개변수|\n",
    "|---|---|---|---|---|\n",
    "|openai/clip-vit-base-patch32|ViT/32|224x224|32x32|약1.5억 개|\n",
    "|openai/clip-vit-base-patch16|ViT/16|224x224|16x16|약1.5억 개|\n",
    "|openai/clip-vit-large-patch14|ViT-L/14|224x224|14x14|약 4.3.억 개|\n",
    "|openai/clip-vit-large-patch14-336|ViT-L/14|336x336|14x14|약 4.3.억 개|\n",
    "\n",
    "CLIP모델은 ViT 아키텍처를 활용한다. 이 아키텍처는 기존의 합성곱 신경망과 달리 이미지를 여러 패치로 분할할 후, 각 패치를 시퀀스로 처리하는 트랜스포머 구조를 갖고 있다. ViT 아키텍처는 모델의 구조와 패칰 크기에 따라 다르게 구성된다. 예를 들어 BiT-B/32 아키텍처는 ViT의 기분 구조를 기반으로 하며, 패치크기가 32x32이다.\n",
    "\n",
    "입력 크기는 모델을 학습할 때 사용한 입력 이미지의 크기를 나타낸다. 입력 크기가 클수록 더 많은 정보를 처리할 수 있지만, 메모리 사용량과 계산 시간이 증가한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: tensor([[49406,  1929, 49407],\n",
      "        [49406,  1559, 49407]])\n",
      "attention_mask: tensor([[1, 1, 1],\n",
      "        [1, 1, 1]])\n",
      "pixel_values: tensor([[[[-0.0113, -0.0988, -0.1426,  ...,  0.1347,  0.1055,  0.0909],\n",
      "          [ 0.0471, -0.0113, -0.0405,  ...,  0.1347,  0.1055,  0.0909],\n",
      "          [ 0.1347,  0.1055,  0.0909,  ...,  0.1493,  0.0909,  0.0909],\n",
      "          ...,\n",
      "          [-0.3032, -0.2448, -0.3178,  ..., -0.1426, -0.8288,  0.0617],\n",
      "          [-0.4492, -0.4054, -0.2886,  ..., -0.5514, -0.6390, -0.3762],\n",
      "          [-0.3762, -0.5222, -0.4784,  ..., -0.9164, -1.0623, -0.0113]],\n",
      "\n",
      "         [[ 0.2740,  0.1689,  0.0939,  ..., -0.2813, -0.3114, -0.3264],\n",
      "          [ 0.3490,  0.2589,  0.2139,  ..., -0.2813, -0.3114, -0.3264],\n",
      "          [ 0.4390,  0.3940,  0.3490,  ..., -0.2813, -0.3264, -0.3264],\n",
      "          ...,\n",
      "          [-1.2869, -1.2118, -1.1968,  ..., -0.8216, -1.3919, -0.5065],\n",
      "          [-1.4219, -1.3469, -1.1818,  ..., -1.1968, -1.2718, -1.0918],\n",
      "          [-1.3319, -1.4369, -1.3919,  ..., -1.4069, -1.5570, -0.5965]],\n",
      "\n",
      "         [[ 0.6670,  0.5675,  0.4821,  ..., -0.2289, -0.2573, -0.2715],\n",
      "          [ 0.7381,  0.6528,  0.5390,  ..., -0.2289, -0.2573, -0.2715],\n",
      "          [ 0.8234,  0.7381,  0.6244,  ..., -0.2146, -0.2573, -0.2573],\n",
      "          ...,\n",
      "          [-1.3522, -1.3238, -1.3380,  ..., -0.8261, -1.2811, -0.6555],\n",
      "          [-1.3380, -1.3238, -1.2811,  ..., -1.1674, -1.2527, -1.2527],\n",
      "          [-1.2954, -1.3807, -1.4233,  ..., -1.1674, -1.3238, -0.6270]]],\n",
      "\n",
      "\n",
      "        [[[ 1.7260,  1.6968,  1.6530,  ...,  1.9303,  1.9303,  1.9303],\n",
      "          [ 1.6822,  1.6238,  1.6092,  ...,  1.9303,  1.9303,  1.9303],\n",
      "          [ 1.6238,  1.5508,  1.5362,  ...,  1.9303,  1.9303,  1.9303],\n",
      "          ...,\n",
      "          [ 1.8865,  1.8281,  1.8281,  ...,  1.9011,  1.8719,  1.8281],\n",
      "          [ 1.8865,  1.8573,  1.8135,  ...,  1.9157,  1.8865,  1.9157],\n",
      "          [ 1.8865,  1.8573,  1.7990,  ...,  1.8865,  1.9011,  1.9303]],\n",
      "\n",
      "         [[ 1.1894,  1.1444,  1.0694,  ...,  2.0749,  2.0749,  2.0749],\n",
      "          [ 1.1444,  1.0844,  1.0243,  ...,  2.0749,  2.0749,  2.0749],\n",
      "          [ 1.0844,  1.0093,  0.9793,  ...,  2.0749,  2.0749,  2.0749],\n",
      "          ...,\n",
      "          [ 2.0299,  1.9698,  1.9698,  ...,  2.0449,  2.0149,  1.9698],\n",
      "          [ 2.0299,  1.9998,  1.9548,  ...,  2.0599,  2.0299,  2.0449],\n",
      "          [ 2.0299,  1.9998,  1.9398,  ...,  2.0299,  2.0449,  2.0749]],\n",
      "\n",
      "         [[ 0.8377,  0.7950,  0.7381,  ...,  2.1459,  2.1459,  2.1459],\n",
      "          [ 0.7950,  0.7381,  0.6955,  ...,  2.1459,  2.1459,  2.1459],\n",
      "          [ 0.7381,  0.6670,  0.6386,  ...,  2.1459,  2.1459,  2.1459],\n",
      "          ...,\n",
      "          [ 2.1032,  2.0464,  2.0464,  ...,  2.1175,  2.0890,  2.0464],\n",
      "          [ 2.1032,  2.0748,  2.0321,  ...,  2.1317,  2.1032,  2.1317],\n",
      "          [ 2.1032,  2.0748,  2.0179,  ...,  2.1032,  2.1175,  2.1459]]]])\n",
      "image_shape: torch.Size([2, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# CLIP 프로세서를 통한 전처리\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "\n",
    "dataset = load_dataset(\"sasha/dog-food\")\n",
    "\n",
    "images = dataset[\"test\"][\"image\"][:2]\n",
    "labels = [\"dog\", \"food\"]\n",
    "\n",
    "inputs = processor(images=images, text=labels, return_tensors=\"pt\")\n",
    "\n",
    "print(\"input_ids:\", inputs[\"input_ids\"])\n",
    "print(\"attention_mask:\", inputs[\"attention_mask\"])\n",
    "print(\"pixel_values:\", inputs[\"pixel_values\"])\n",
    "print(\"image_shape:\", inputs['pixel_values'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CLIPProcessor`에는 이미지 전처리를 담당하는 `CLIPImageProcessor`와 텍스트 전처리를 담당하는 `ClIPTokenizer`가 내부적으로 포함되어 있다. `CLIPImageProcessor`는 입력 이미지를 모델이 처리할 수 있는 형태로 변환하며, `ClIPTokenizer`는 입력 텍스트를 모델이 이해할 수 있는 형태로 변환한다.\n",
    "\n",
    "데이터세트는 `sasha/dog-food` 데이터세트를 사용한다. 이 데이터세트는 개(dog), 음식(food) 클래스 간의 이진 이미지 분류를 위한 데이터세트다. 이 데이터세트는 `image, label`로 구성되어 잇다. label은 0:개, 1:음식으로 구성되어 있다.\n",
    "\n",
    "데이터에를 모델에 입력하기 위해 processor를 사용해 images와 labels를 전처리한다. 이 과정에서 processor는 이미지 데이터의 크기를 조정하고, 필셀 값을 정규화 한다. 또한 텍스트 데이터를 토큰화해 모델의 입력 형식으로 변환한다. 만약, 입력 데이터의 토큰 길이가 다르다면, padding=True옵션으로 패딩 토큰을 삽입해 입력 텍스트 토큰의 길이를 통일 한다.\n",
    "\n",
    "출력 결과를 확인하면, input_ids, attention_mask, pixel_values등의 데이터가 생성되어 있다. input_ids의 49506과 49507은 텍스트 데이터의 시작(<|startoftext|과 끝(<|endoftext|>)>을 나타내는 특별한 값으로, 각 텍스트 데이터 앞뒤에 추가된다.\n",
    "\n",
    "attention_mask는 변환된 토큰의 유형을 나타내며, 값이 1이면 해당 위치의 토큰이 실제 데이터 값을 나타내고, 값이 0이면 패딩 토큰임을 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: odict_keys(['logits_per_image', 'logits_per_text', 'text_embeds', 'image_embeds', 'text_model_output', 'vision_model_output'])\n",
      "logits_per_image: tensor([[23.3881, 18.8604],\n",
      "        [24.8627, 21.5765]])\n",
      "probs: tensor([[0.9893, 0.0107],\n",
      "        [0.9640, 0.0360]])\n",
      "- Image #0\n",
      "  dog: 0.99\n",
      "  food: 0.01\n",
      "- Image #1\n",
      "  dog: 0.96\n",
      "  food: 0.04\n"
     ]
    }
   ],
   "source": [
    "# 제로샷 이미지 분류\n",
    "\n",
    "import torch\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "    print(\"outputs:\", outputs.keys())\n",
    "    print(\"logits_per_image:\", logits_per_image)\n",
    "    print(\"probs:\", probs)\n",
    "\n",
    "for idx, prob in enumerate(probs):\n",
    "    print(f\"- Image #{idx}\")\n",
    "    for label, p in zip(labels, prob):\n",
    "        print(f\"  {label}: {p:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추론 결과인 `outputs`의 출력값을 보면 총 6개의 키가 반환된다. `logits_per_image`와 `logits_per_text`는 각각 이미지와 텍스트가 얼마나 잘 매칭 되었는지를 나타내는 로짓 점수를 의미한다. 높은 점수는 해당 이미지나 텍스트가 잘 매칭되었음을 의미한다. \n",
    "\n",
    "> 이 책은 학습을 싫어 한다. ㅋㅋㅋ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9977777777777778\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset[\"test\"],\n",
    "    batch_size=8,\n",
    "    collate_fn=lambda batch:([item[\"image\"] for item in batch],\n",
    "                              [item[\"label\"] for item in batch]))\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "predictions, references = [], []\n",
    "labels_names = dataset[\"test\"].features[\"label\"].names\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        inputs = processor(images=images, text=labels_names, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs)\n",
    "        probs = outputs.logits_per_image.softmax(dim=1)\n",
    "\n",
    "        predictions += probs.argmax(dim=1).cpu().tolist()\n",
    "        references += labels\n",
    "\n",
    "\n",
    "results = metric.compute(predictions=predictions, references=references)\n",
    "print(\"Accuracy:\", results[\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
