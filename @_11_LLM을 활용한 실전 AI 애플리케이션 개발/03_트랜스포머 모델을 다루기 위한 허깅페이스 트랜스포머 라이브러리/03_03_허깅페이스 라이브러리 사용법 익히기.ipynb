{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. 허깅페이스 라이브러리 사용법 익히기\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. 모델 활용하기\n",
    "\n",
    "허깅페이스에서 모델을 바디와 헤드로 구분한다. 이러면 같은 바디를 사용하지만 다른 작업에 사용할 수 있도록 헤드만 바꾸어 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AutoModel` 은 모델의 바디를 불러오는 클래스다. `from_pretrained()` 메소드를 사용하여 모델 ID에 맞는 적절한 클래스를 가져온다.\n",
    "`klue/roberta-base`의 모델 타입은 `roberta`로 모델의 config에서 확인할 수 있다. 토크나이저는 `BertTokenizer`를 사용한다. 이것도 `AutoTokenizer`를 사용하여 불러올 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Configuration:  RobertaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"klue/roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "Tokenizer Configuration:  BertTokenizerFast(name_or_path='klue/roberta-base', vocab_size=32000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 예제 3.2 모델 아이디로 모델 불러오기\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_id = 'klue/roberta-base'\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "print(\"Model Configuration: \", model.config)\n",
    "print(\"Tokenizer Configuration: \", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "Model Configuration:  RobertaConfig {\n",
    "  \"_attn_implementation_autoset\": true,\n",
    "  \"_name_or_path\": \"klue/roberta-base\",\n",
    "  \"architectures\": [\n",
    "    \"RobertaForMaskedLM\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"bos_token_id\": 0,\n",
    "  \"classifier_dropout\": null,\n",
    "  \"eos_token_id\": 2,\n",
    "  \"gradient_checkpointing\": false,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-05,\n",
    "  \"max_position_embeddings\": 514,\n",
    "  \"model_type\": \"roberta\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"pad_token_id\": 1,\n",
    "  \"position_embedding_type\": \"absolute\",\n",
    "  \"tokenizer_class\": \"BertTokenizer\",\n",
    "  \"transformers_version\": \"4.46.3\",\n",
    "  \"type_vocab_size\": 1,\n",
    "  \"use_cache\": true,\n",
    "  \"vocab_size\": 32000\n",
    "}\n",
    "\n",
    "Tokenizer Configuration:  BertTokenizerFast(name_or_path='klue/roberta-base', vocab_size=32000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
    "\t0: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "\t1: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "\t3: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"SamLowe/roberta-base-go_emotions\",\n",
       "  \"architectures\": [\n",
       "    \"RobertaForSequenceClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"admiration\",\n",
       "    \"1\": \"amusement\",\n",
       "    \"2\": \"anger\",\n",
       "    \"3\": \"annoyance\",\n",
       "    \"4\": \"approval\",\n",
       "    \"5\": \"caring\",\n",
       "    \"6\": \"confusion\",\n",
       "    \"7\": \"curiosity\",\n",
       "    \"8\": \"desire\",\n",
       "    \"9\": \"disappointment\",\n",
       "    \"10\": \"disapproval\",\n",
       "    \"11\": \"disgust\",\n",
       "    \"12\": \"embarrassment\",\n",
       "    \"13\": \"excitement\",\n",
       "    \"14\": \"fear\",\n",
       "    \"15\": \"gratitude\",\n",
       "    \"16\": \"grief\",\n",
       "    \"17\": \"joy\",\n",
       "    \"18\": \"love\",\n",
       "    \"19\": \"nervousness\",\n",
       "    \"20\": \"optimism\",\n",
       "    \"21\": \"pride\",\n",
       "    \"22\": \"realization\",\n",
       "    \"23\": \"relief\",\n",
       "    \"24\": \"remorse\",\n",
       "    \"25\": \"sadness\",\n",
       "    \"26\": \"surprise\",\n",
       "    \"27\": \"neutral\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"admiration\": 0,\n",
       "    \"amusement\": 1,\n",
       "    \"anger\": 2,\n",
       "    \"annoyance\": 3,\n",
       "    \"approval\": 4,\n",
       "    \"caring\": 5,\n",
       "    \"confusion\": 6,\n",
       "    \"curiosity\": 7,\n",
       "    \"desire\": 8,\n",
       "    \"disappointment\": 9,\n",
       "    \"disapproval\": 10,\n",
       "    \"disgust\": 11,\n",
       "    \"embarrassment\": 12,\n",
       "    \"excitement\": 13,\n",
       "    \"fear\": 14,\n",
       "    \"gratitude\": 15,\n",
       "    \"grief\": 16,\n",
       "    \"joy\": 17,\n",
       "    \"love\": 18,\n",
       "    \"nervousness\": 19,\n",
       "    \"neutral\": 27,\n",
       "    \"optimism\": 20,\n",
       "    \"pride\": 21,\n",
       "    \"realization\": 22,\n",
       "    \"relief\": 23,\n",
       "    \"remorse\": 24,\n",
       "    \"sadness\": 25,\n",
       "    \"surprise\": 26\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"problem_type\": \"multi_label_classification\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.46.3\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예제 3.4 분류 헤드가 포함된 모델 불러오기\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_id = \"SamLowe/roberta-base-go_emotions\"\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "classification_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"klue/roberta-base\",\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"tokenizer_class\": \"BertTokenizer\",\n",
       "  \"transformers_version\": \"4.46.3\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예제 3.7 분류 모델에 바디만 있는 모델을 불러울 때 경고가 표시된다.\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_id = \"klue/roberta-base\"\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "classification_model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 바디는 불러왔지만 분류 헤드가 없기 때문에 학습을 해야한다. 경고 내용은 분류 헤드에 대한 파라미터가 없어서 랜덤으로 초기화 되었다는 것이다.\n",
    "즉 `klue/roberta-base` 모델의 경우 분류 헤드가 없어서 AutoModelForSequenceClassification 클래스로 불러오더라도 분류 헤드가 추가 되지 않으며 id2label 같은 설정이 없다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 3.8 토크나이저 불러오기\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "model_id = 'klue/roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='klue/roberta-base', vocab_size=32000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 5891, 2205, 5971, 16, 9187, 4347, 2069, 3704, 2097, 2178, 2918, 2219, 3606, 18, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"안녕하세요, 한국어 모델을 사용해보겠습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input ids:  [0, 5891, 2205, 5971, 16, 9187, 4347, 2069, 3704, 2097, 2178, 2918, 2219, 3606, 18, 2]\n",
      "token type ids:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "attention mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " ['[CLS]', '안녕', '##하', '##세요', ',', '한국어', '모델', '##을', '사용', '##해', '##보', '##겠', '##습', '##니다', '.', '[SEP]']\n",
      "[CLS] 안녕하세요, 한국어 모델을 사용해보겠습니다. [SEP]\n",
      "안녕하세요, 한국어 모델을 사용해보겠습니다.\n",
      "input ids:  [0, 3686, 1564, 2170, 2259, 3890, 2069, 3645, 2085, 6301, 35, 2]\n",
      "token type ids:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "attention mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " ['[CLS]', '이번', '주', '##에', '##는', '무엇', '##을', '해야', '##할', '##까요', '?', '[SEP]']\n",
      "[CLS] 이번 주에는 무엇을 해야할까요? [SEP]\n",
      "이번 주에는 무엇을 해야할까요?\n",
      "input ids:  [0, 3822, 2073, 5792, 2116, 1560, 2203, 2182, 2]\n",
      "token type ids:  [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "attention mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " ['[CLS]', '오늘', '##은', '날씨', '##가', '좋', '##네', '##요', '[SEP]']\n",
      "[CLS] 오늘은 날씨가 좋네요 [SEP]\n",
      "오늘은 날씨가 좋네요\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_inputs = [\"안녕하세요, 한국어 모델을 사용해보겠습니다.\", \"이번 주에는 무엇을 해야할까요?\", \"오늘은 날씨가 좋네요\"]\n",
    "\n",
    "for input_text in test_inputs:\n",
    "    print(\"input ids: \", tokenizer(input_text)['input_ids'])\n",
    "    print(\"token type ids: \", tokenizer(input_text)['token_type_ids'])\n",
    "    print(\"attention mask: \",tokenizer(input_text)['attention_mask'])\n",
    "    print(\"\", tokenizer.convert_ids_to_tokens(tokenizer(input_text)['input_ids']))\n",
    "    print(tokenizer.decode(tokenizer(input_text)['input_ids']))\n",
    "    print(tokenizer.decode(tokenizer(input_text)['input_ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3. 데이터셋 활용하기\n",
    "\n",
    "허깅페이스에서는 데이터셋을 다루기 위한 클래스를 제공한다. 이 클래스를 사용하면 데이터셋을 쉽게 다룰 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "klue_dataset = load_dataset(\"klue\", \"mrc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers'],\n",
       "        num_rows: 17554\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers'],\n",
       "        num_rows: 5841\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klue_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
