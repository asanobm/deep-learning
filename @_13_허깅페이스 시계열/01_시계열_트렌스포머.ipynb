{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시계열 트랜스포머 \n",
    "\n",
    "시계열 트랜트포머 모델은 시계열 예측을 위한 기본적인 인코더-디코더 구조의 트랜스포머다.\n",
    "\n",
    "### 2개의 클래스\n",
    "\n",
    "* TimeSeriesTransformerModel: 상당에 헤드가 없는 기본적인 트랜스포머\n",
    "* TimeSeriesTransformerForPrediction: 상단에 분포 호데를 추가하여 시계열 예측에 사용할 수 있다. (확률적 예측 모델) 값을 직접 출력하지는 않는다.\n",
    "\n",
    "### TimeSeriesTransformerForPrediction의 2가지 블록\n",
    "\n",
    "인코더는`context_length`의 시계열 값을 입력(`past_values`)으로 받고, 디코더는 미래의 `prediction_length` 만큼의 시계열 값을 예측한다(`future_values`). 학습에는 `past_values`와 `future_values`를 제공해야한다.\n",
    "\n",
    "### 가공하지 않은 past_values와 future_values 이외의 추가 특징\n",
    "\n",
    "* `past_time_features`: 모델이 `future_values`에 추가할 시간적 특성. 트랜스포머 디코더의 \"위치 인코딩\" 역할을 한다.\n",
    "* `future_time_features`: 모델이 `future_values`에 추가할 시간적 특성. 트랜스포머 디코더의 \"위치 인코딩\" 역할을 한다.\n",
    "* `static_categorical_features`: 시간에 따라 변하지 않는 범주형 특성 모든 `past_values`, `future_values`에 동일한 값을 가진다. (ID, 카테고리 등)\n",
    "* `static_real_features`: 시간에 따라 변하지 않는 실수값 특성 모든 `past_values`, `future_values`에 동일한 값을 가진다. (온도, 고도 등) \n",
    "\n",
    "### 모델의 훈련\n",
    "시계열 트랜스포머 모델은 기계 번역을 위한 트랜스포머 훈련과 유사하게 \"교사 강제(teacher-forcing)\" 방식으로 훈련된다. 즉 훈련중에 `future_values`를 디코더의 입력으로 오른쪽으로 한 위치 이동시키고, `past_values`의 마지막 값을 앞에 붙인다. 각 시간 단계에서 모델은 다음 타겟을 예측해야 한다. 따라서 훈련 설정은 언어를 위한 GPT 모델과 유사하지만, `decoder_start_token_id` 개념이 없다. \n",
    "\n",
    "### 모델의 추론\n",
    "\n",
    "추론 시에는 `past_values`의 최종값을 디코더의 입력으로 제공한다. 그 다음, 모델에서 샘플링하여 다음 시간 단계에서의 예측을 만들고, 이를 디코더에 공급하여 다음 예측을 만든다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TimeSeriesTransformerConfig class\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "* `prediction_length`: (int), **예측길이**모델이 얼마나 멀리 미래를 예측할 지를 정의한다.\n",
    "* `context_length`: (int, optional, defaults to `prediction_length`),**컨텍스트 길이** 모델에게 더 많은 과거 정보를 제공하여 더 나은 예측을 할 수 있도록 도와준다. 데이터 세트의 특성에 따라 이를 조정하는 것이 성능 향상에 도움이 될 수 있다.\n",
    "* `distribution_output`: (string, optional, defaults \"student_t\"), **분포 출력** 올바른 분포를 선택하는 것(\"student_t\", \"normal\", \"negative_binomial\")은 모델이 예측의 불확실성과 꼬리를 추정하는 방식에 영향을 준다\n",
    "* `loss`: (string, optional, defaults to :\"nll\"), **손실함수** 현재는 음의 로그 우도(\"nagative log liklihood\")만 지원한다.\n",
    "* `input_size`: (int, optional, defaults to 1), **입력 크기** 단항 예측(univariate)에서 중요하다. 모델의 고려하는 타깃 변수의 수에 직접 영향을 미친다.\n",
    "* `scaling`: (string, or bool, optional, defaults to \"mean\"), **스케일링** 입력 특징을 적절히 스케일링(mean, standard deviation)하는 것은 모델 수렴의 및 성능에 상당한 영향을 준다.\n",
    "* `lags_sequence`: (list[int], optional, defaults to [1, 2, 3, 4, 5, 6, 7]), **지연 시퀀스** 시계열의 빈도에 따라 자연 특징을 맞춤 설정하면 시간적 의존성을 효과적으로 포착하는데 도움이 된다.\n",
    "* `num_time_features`: (int, optional, defaults to 0), **시간 특징 수** 시간 특징을 추가하면 모델이 시간적 특성을 더 잘 이해하고 예측할 수 있다.\n",
    "* `num_dynamic_real_features`: (int, optional, defaults to 0), **동적 실수 특징 수** 시간에 따라 변하는 실수값 특성\n",
    "* `num_static_real_features`: (int, optional, defaults to 0), **정적 실수 특징 수** 시간에 따라 변하지 않는 실수값 특성\n",
    "* `num_static_real_features`: (int, optional, defaults to 0), **정적 범주형 특징 수** 시간에 따라 변하지 않는 범주형 특성\n",
    "* `cardinality`: (list[int], optional) **카디널리티** 각 정적 범주형 특성에 대한 카디널리티 `num_static_categorical_features`와 같은 길이의 정수 목록이어야 한며, `num_static_categorical_features`가 0보다 큰 경우 None일 수 없다.\n",
    "* `embedding_dimension`: (list[int], optional), 각 정적 범주형 특성에 다한 임베딩차원 `num_static_categorical_features`와 같은 길이의 정수 목록이여야하며, `num_static_categorical_features`가 0보다 큰 경우 None일 수 없다.\n",
    "* `d_model`: (int, optional, defaults to 64) 변형기 레이어의 차원\n",
    "* `encoder_layers`: (int, optional, defaults to 2) 인코더 레이어의 수\n",
    "* `decoder_layers`: (int, optional, defaults to 2) 디코더 레이어의 수\n",
    "* `encoder_attention_heads`: (int, optional, defaults to 2) 변형 인코더의 각 주의 레이어에 대한 주의 헤드 수\n",
    "* `decoder_attention_heads`: (int, optional, defaults to 2) 변형 디코더의 각 주의 레이어에 대한 주의 헤드 수\n",
    "* `encoder_ffn_dim`: (int, optional, defaults to 32) 인코더의 \"중간 (feed forward)\" 차원\n",
    "* `decoder_ffn_dim`: (int, optional, defaults to 32) 디코더의 \"중간 (feed forward)\" 차원\n",
    "* `activation_function`: (str, function, optional, defaults to \"gelu\") 인코더와 디코더의 활성화 함수\n",
    "* `dropout`: (float, optional, defaults to 0.1) 인코더와 디코더의 드롭아웃\n",
    "* `encoder_layerdrop`: (float, optional, defaults to 0.1) 인코더 레이어 드롭\n",
    "* `decoder_layerdrop`: (float, optional, defaults to 0.1) 디코더 레이어 드롭\n",
    "* `attention_dropout`: (float, optional, defaults to 0.1) 어텐션 드롭아웃\n",
    "* `activation_dropout`: (float, optional, defaults to 0.1) 활성화 드롭아웃\n",
    "* `num_parallel_samples`: (int, optional, defaults to 100) 추론 시에 생성할 샘플 수\n",
    "* `init_std`: (float, optional, defaults to 0.02) 초기화 표준 편차\n",
    "* `use_cache`: (bool, optional, defaults to False) 캐시 사용 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeSeriesTransformerConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"activation_dropout\": 0.1,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"cardinality\": [\n",
       "    0\n",
       "  ],\n",
       "  \"context_length\": 12,\n",
       "  \"d_model\": 64,\n",
       "  \"decoder_attention_heads\": 2,\n",
       "  \"decoder_ffn_dim\": 32,\n",
       "  \"decoder_layerdrop\": 0.1,\n",
       "  \"decoder_layers\": 2,\n",
       "  \"distribution_output\": \"student_t\",\n",
       "  \"dropout\": 0.1,\n",
       "  \"embedding_dimension\": [\n",
       "    0\n",
       "  ],\n",
       "  \"encoder_attention_heads\": 2,\n",
       "  \"encoder_ffn_dim\": 32,\n",
       "  \"encoder_layerdrop\": 0.1,\n",
       "  \"encoder_layers\": 2,\n",
       "  \"feature_size\": 9,\n",
       "  \"init_std\": 0.02,\n",
       "  \"input_size\": 1,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"lags_sequence\": [\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4,\n",
       "    5,\n",
       "    6,\n",
       "    7\n",
       "  ],\n",
       "  \"loss\": \"nll\",\n",
       "  \"model_type\": \"time_series_transformer\",\n",
       "  \"num_dynamic_real_features\": 0,\n",
       "  \"num_parallel_samples\": 100,\n",
       "  \"num_static_categorical_features\": 0,\n",
       "  \"num_static_real_features\": 0,\n",
       "  \"num_time_features\": 0,\n",
       "  \"prediction_length\": 12,\n",
       "  \"scaling\": \"mean\",\n",
       "  \"transformers_version\": \"4.46.2\",\n",
       "  \"use_cache\": true\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TImeSeriesTransformerConfig Sample\n",
    "\n",
    "from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerModel\n",
    "\n",
    "# Initializing a Time Series Transformer configuration with 12 time steps for prediction\n",
    "\n",
    "config = TimeSeriesTransformerConfig(prediction_length=12)\n",
    "\n",
    "model = TimeSeriesTransformerModel(config)\n",
    "\n",
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TimeSeriesTransformerModel class\n",
    "\n",
    "* config: TimeSeriesTransformerConfig \n",
    "\n",
    "분류 헤드가 없는 기본적인 시계열 트랜스포머 모델. 이 모델은 PreTrained에서 상속된다. \n",
    "\n",
    "#### Parameters\n",
    "\n",
    "* `past_values` (`torch.FloatTensor` of shape (`batch_size`, `sequence_length`) or (`batch_size`, `sequence_length`, `input_size`)) — 미래를 예측하기 위해 컨텍스트 역할을 하는 시계열의 과거 값입니다. 이 텐서의 시퀀스 크기는 모델의 `context_length`보다 커야 하며, 모델은 더 큰 크기를 사용하여 지연 특징(추가 컨텍스트 역할을 하는 과거의 추가 값)을 구성합니다.\n",
    "  \n",
    "`sequence_length`는 `config.context_length` + `max(config.lags_sequence)`와 같으며, lags_sequence가 설정되지 않은 경우 기본적으로 `config.context_length` + 7과 같습니다(`config.lags_sequence`에서 가장 큰 look-back 인덱스는 7입니다). 속성 `_past_length`는 과거의 실제 길이를 반환합니다.\n",
    "\n",
    "`past_values`는 트랜스포머 인코더가 입력으로 받는 값입니다 (선택적으로 추가 특징, 예를 들어 `static_categorical_features`, `static_real_features`, `past_time_features` 및 lags 포함). 선택적으로, 결측값은 0으로 대체하고 `past_observed_mask`를 통해 표시해야 합니다.\n",
    "\n",
    "다변량 시계열의 경우, `input_size` > 1 차원이 필요하며 이는 시간 단계별 시계열의 변량 수에 해당합니다.\n",
    "\n",
    "* `past_time_features` (`torch.FloatTensor` of shape (`batch_size`, `sequence_length`, `num_features`)) — 모델이 `past_values`에 내부적으로 추가할 필수 시간 특징입니다. 이러한 특징은 \"월\", \"일\" 등 벡터로 인코딩된 것일 수 있습니다(for instance as Fourier features). 또한, 시계열이 \"어느 시점에 있는지\"를 모델이 알 수 있도록 돕는 \"age\" 특징일 수도 있습니다. age 특징은 먼 과거 시간 단계에서는 작은 값을 가지며 현재 시간 단계에 가까워질수록 단조롭게 증가합니다. 휴일 특징도 시간 특징의 좋은 예입니다.\n",
    "이러한 특징은 입력의 \"위치 인코딩\" 역할을 합니다. 따라서 BERT와 같은 모델에서는 위치 인코딩이 모델의 내부 매개변수로부터 처음부터 학습되는 반면, 시계열 트랜스포머는 추가 시간 특징을 제공해야 합니다. 시계열 트랜스포머는 `static_categorical_features`에 대한 추가 임베딩만 학습합니다.\n",
    "\n",
    "Additional dynamic real covariances can be concatenated to this tensor, with the caveat that these features must but known at prediction time.\n",
    "\n",
    "`num_features`는 `config.num_time_features` + `config.num_dynamic_real_features`와 같습니다.\n",
    "\n",
    "* `past_observed_mask` (`torch.BoolTensor` of shape (`batch_size`, `sequence_length`) or (`batch_size`, `sequence_length`, `input_size`), optional) — 과거 값이 관찰되었는지 또는 누락되었는지를 나타내는 부울 마스크입니다. 마스크 값은 [0, 1]에서 선택됩니다:\n",
    "  * 1은 관찰된 값\n",
    "  * 0은 누락된 값(즉, 0으로 대체된 NaN 값).\n",
    "\n",
    "* `static_categorical_features` (`torch.LongTensor` of shape (`batch_size`, number of static categorical features), optional) — Optional 모델이 임베딩을 학습하고 시계열 값에 추가할 정적 범주형 특징.\n",
    "정적 범주형 특징은 모든 시간 단계에 대해 동일한 값을 갖는 특징입니다(시간에 따라 정적).\n",
    "\n",
    "정적 범주형 피처의 일반적인 예는 시계열 ID입니다.\n",
    "\n",
    "`static_real_features` (`torch.FloatTensor` of shape (`batch_size`, number of static real features), optional) — Optional 모델이 시계열 값에 추가할 정적 범주형 특징.\n",
    "정적 범주형 특징은 모든 시간 단계에 대해 동일한 값을 갖는 특징입니다(시간에 따라 정적).\n",
    "\n",
    "정적 범주형 특징의 일반적인 예는 프로모션 정보입니다.\n",
    "\n",
    "`future_values` (`torch.FloatTensor` of shape (`batch_size`, `prediction_length`) or (`batch_size`, `prediction_length`, `input_size`), optional) — 모델의 레이블 역할을 하는 시계열의 미래 값입니다. `future_values는` 과거 값을 고려하여 출력하는 법을 배우기 위해 Transformer가 학습하는 동안 필요한 것입니다.\n",
    "여기서 시퀀스 길이는 `prediction_length`와 같습니다.\n",
    "\n",
    "See the demo notebook and code snippets for details.\n",
    "\n",
    "Optionally, during training any missing values need to be replaced with zeros and indicated via the `future_observed_mask`.\n",
    "\n",
    "For multivariate time series, the input_size > 1 dimension is required and corresponds to the number of variates in the time series per time step.\n",
    "\n",
    "* `future_time_features` (`torch.FloatTensor` of shape (`batch_size`, `prediction_length`, `num_features`)) — Required time features for the prediction window, which the model internally will add to future_values. These could be things like “month of year”, “day of the month”, etc. encoded as vectors (for instance as Fourier features). These could also be so-called “age” features, which basically help the model know “at which point in life” a time-series is. Age features have small values for distant past time steps and increase monotonically the more we approach the current time step. Holiday features are also a good example of time features.\n",
    "These features serve as the “positional encodings” of the inputs. So contrary to a model like BERT, where the position encodings are learned from scratch internally as parameters of the model, the Time Series Transformer requires to provide additional time features. The Time Series Transformer only learns additional embeddings for `static_categorical_features`.\n",
    "\n",
    "Additional dynamic real covariances can be concatenated to this tensor, with the caveat that these features must but known at prediction time.\n",
    "\n",
    "The num_features here is equal to `config.num_time_features`+`config.num_dynamic_real_features`.\n",
    "\n",
    "* `future_observed_mask` (`torch.BoolTensor` of shape (`batch_size`, `sequence_length`) or (`batch_size`, `sequence_length`, `input_size`), optional) — Boolean mask to indicate which future_values were observed and which were missing. Mask values selected in [0, 1]:\n",
    "\n",
    "  * 1 for values that are observed,\n",
    "  * 0 for values that are missing (i.e. NaNs that were replaced by zeros).\n",
    "\n",
    "This mask is used to filter out missing values for the final loss calculation.\n",
    "\n",
    "* `attention_mask` (`torch.Tensor` of shape (`batch_size`, `sequence_length`), optional) — Mask to avoid performing attention on certain token indices. Mask values selected in [0, 1]:\n",
    "\n",
    "  * 1 for tokens that are not masked,\n",
    "  * 0 for tokens that are masked.\n",
    "\n",
    "What are attention masks?\n",
    "\n",
    "* `decoder_attention_mask` (`torch.LongTensor` of shape (`batch_size`, `target_sequence_length`), optional) — Mask to avoid performing attention on certain token indices. By default, a causal mask will be used, to make sure the model can only look at previous inputs in order to predict the future.\n",
    "\n",
    "* `head_mask` (`torch.Tensor` of shape (`encoder_layers`, `encoder_attention_heads`), optional) — Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in [0, 1]:\n",
    "  * 1 indicates the head is not masked,\n",
    "  * 0 indicates the head is masked.\n",
    "\n",
    "* `decoder_head_mask` (`torch.Tensor` of shape (`decoder_layers`, `decoder_attention_heads`), optional) — Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in [0, 1]:\n",
    "  * 1 indicates the head is not masked,\n",
    "  * 0 indicates the head is masked.\n",
    "\n",
    "* `cross_attn_head_mask` (`torch.Tensor` of shape (`decoder_layers`, `decoder_attention_heads`), optional) — Mask to nullify selected heads of the cross-attention modules. Mask values selected in [0, 1]:\n",
    "  * 1 indicates the head is not masked,\n",
    "  * 0 indicates the head is masked.\n",
    "\n",
    "* `encoder_outputs` (tuple(tuple(`torch.FloatTensor`), optional) — Tuple consists of `last_hidden_state`, `hidden_states` (optional) and attentions (optional) `last_hidden_state` of shape (`batch_size`, `sequence_length`, `hidden_size`) (optional) is a sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n",
    "\n",
    "* `past_key_values` (tuple(tuple(`torch.FloatTensor`)), optional, returned when `use_cache=True` is passed or when `config.use_cache=True`) — Tuple of tuple(`torch.FloatTensor`) of length `config.n_layers`, with each tuple having 2 tensors of shape (`batch_size`, `num_heads`, `sequence_length`, `embed_size_per_head`)) and 2 additional tensors of shape (`batch_size`, `num_heads`, `encoder_sequence_length`, `embed_size_per_head`).\n",
    "Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention blocks) that can be used (see past_key_values input) to speed up sequential decoding.\n",
    "\n",
    "If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that don’t have their past key value states given to this model) of shape (`batch_size`, 1) instead of all decoder_input_ids of shape (`batch_size`, `sequence_length`).\n",
    "\n",
    "`inputs_embeds` (`torch.FloatTensor` of shape (batch_size, sequence_length, hidden_size), optional) — Optionally, instead of passing input_ids you can choose to directly pass an embedded representation. This is useful if you want more control over how to convert input_ids indices into associated vectors than the model’s internal embedding lookup matrix.\n",
    "\n",
    "* `use_cache` (bool, optional) — If set to True, past_key_values key value states are returned and can be used to speed up decoding (see past_key_values).\n",
    "\n",
    "* `output_attentions` (bool, optional) — Whether or not to return the attentions tensors of all attention layers. See attentions under returned tensors for more detail.\n",
    "\n",
    "* `output_hidden_states` (bool, optional) — Whether or not to return the hidden states of all layers. See hidden_states under returned tensors for more detail.\n",
    "\n",
    "* `return_dict` (bool, optional) — Whether or not to return a ModelOutput instead of a plain tuple.\n",
    "\n",
    "\n",
    "### Returns\n",
    "\n",
    "transformers.modeling.outputs.Seq2SeqModelOutput or tuple(torch.FloatTensor)\n",
    "\n",
    "* `last_hidden_state` (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size)) — Sequence of hidden-states at the output of the last layer of the decoder of the model.\n",
    "\n",
    "If past_key_values is used only the last hidden-state of the sequences of shape (batch_size, 1, hidden_size) is output.\n",
    "past_key_values (tuple(tuple(torch.FloatTensor)), optional, returned when use_cache=True is passed or when config.use_cache=True) — Tuple of tuple(torch.FloatTensor) of length config.n_layers, with each tuple having 2 tensors of shape (batch_size, num_heads, sequence_length, embed_size_per_head)) and 2 additional tensors of shape (batch_size, num_heads, encoder_sequence_length, embed_size_per_head).\n",
    "\n",
    "Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention blocks) that can be used (see past_key_values input) to speed up sequential decoding.\n",
    "\n",
    "* `decoder_hidden_states` (tuple(torch.FloatTensor), optional, returned when output_hidden_states=True is passed or when config.output_hidden_states=True) — Tuple of torch.FloatTensor (one for the output of the embeddings, if the model has an embedding layer, + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).\n",
    "\n",
    "Hidden-states of the decoder at the output of each layer plus the optional initial embedding outputs.\n",
    "\n",
    "* `decoder_attentions` (tuple(torch.FloatTensor), optional, returned when output_attentions=True is passed or when config.output_attentions=True) — Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads, sequence_length, sequence_length).\n",
    "\n",
    "Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "\n",
    "* `cross_attentions` (tuple(torch.FloatTensor), optional, returned when output_attentions=True is passed or when config.output_attentions=True) — Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads, sequence_length, sequence_length).\n",
    "\n",
    "Attentions weights of the decoder’s cross-attention layer, after the attention softmax, used to compute the weighted average in the cross-attention heads.\n",
    "\n",
    "* `encoder_last_hidden_state` (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size), optional) — Sequence of hidden-states at the output of the last layer of the encoder of the model.\n",
    "\n",
    "* `encoder_hidden_states` (tuple(torch.FloatTensor), optional, returned when output_hidden_states=True is passed or when config.output_hidden_states=True) — Tuple of torch.FloatTensor (one for the output of the embeddings, if the model has an embedding layer, + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).\n",
    "\n",
    "Hidden-states of the encoder at the output of each layer plus the optional initial embedding outputs.\n",
    "\n",
    "* `encoder_attentions` (tuple(torch.FloatTensor), optional, returned when output_attentions=True is passed or when config.output_attentions=True) — Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads, sequence_length, sequence_length).\n",
    "\n",
    "Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "loc (torch.FloatTensor of shape (batch_size,) or (batch_size, input_size), optional) — Shift values of each time series’ context window which is used to give the model inputs of the same magnitude and then used to shift back to the original magnitude.\n",
    "scale (torch.FloatTensor of shape (batch_size,) or (batch_size, input_size), optional) — Scaling values of each time series’ context window which is used to give the model inputs of the same magnitude and then used to rescale back to the original magnitude.\n",
    "static_features (torch.FloatTensor of shape (batch_size, feature size), optional) — Static features of each time series’ in a batch which are copied to the covariates at inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73765/2534584452.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  batch = torch.load(file)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[25553.8281, 32870.6406, 32968.9297,  ..., 33791.2969, 37489.9219,\n",
       "         28650.5059],\n",
       "        [24609.8496, 35064.0195, 47750.0938,  ..., 25898.7383, 26012.3457,\n",
       "         23417.2031],\n",
       "        [ 1107.4222,  1135.8021,  1128.0045,  ...,  2121.8477,  1870.3226,\n",
       "          1667.2338],\n",
       "        ...,\n",
       "        [ 2902.1611,  3778.1709,  3529.3625,  ...,  3357.0894,  3908.6863,\n",
       "          4102.9067],\n",
       "        [  782.9967,   607.5106,   328.7884,  ...,   586.4850,  1311.8656,\n",
       "          1181.1353],\n",
       "        [  387.5719,   619.5315,   423.6380,  ...,   436.6917,   407.5393,\n",
       "           528.8020]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample \n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "from transformers import TimeSeriesTransformerForPrediction\n",
    "\n",
    "file = hf_hub_download(\n",
    "    repo_id=\"hf-internal-testing/tourism-monthly-batch\",filename=\"train-batch.pt\", repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "batch = torch.load(file)\n",
    "\n",
    "model = TimeSeriesTransformerForPrediction.from_pretrained(\n",
    "    \"huggingface/time-series-transformer-tourism-monthly\"\n",
    ")\n",
    "\n",
    "outputs = model(\n",
    "    past_values=batch[\"past_values\"],\n",
    "    past_time_features=batch[\"past_time_features\"],\n",
    "    past_observed_mask=batch[\"past_observed_mask\"],\n",
    "    static_categorical_features=batch[\"static_categorical_features\"],\n",
    "    static_real_features=batch[\"static_real_features\"],\n",
    "    future_values=batch[\"future_values\"],\n",
    "    future_time_features=batch[\"future_time_features\"],\n",
    ")\n",
    "\n",
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "\n",
    "outputs = model.generate(\n",
    "    past_values=batch[\"past_values\"],\n",
    "    past_time_features=batch[\"past_time_features\"],\n",
    "    past_observed_mask=batch[\"past_observed_mask\"],\n",
    "    static_categorical_features=batch[\"static_categorical_features\"],\n",
    "    static_real_features=batch[\"static_real_features\"],\n",
    "    future_time_features=batch[\"future_time_features\"],\n",
    ")\n",
    "\n",
    "mean_prediction = outputs.sequences.mean(dim=1)\n",
    "\n",
    "# Sample\n",
    "mean_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['p (mbar)', 'T (degC)', 'Tpot (K)', 'Tdew (degC)', 'rh (%)', 'VPmax (mbar)', 'VPact (mbar)', 'VPdef (mbar)', 'sh (g/kg)', 'H2OC (mmol/mol)', 'rho (g/m**3)', 'Wx', 'Wy', 'max Wx', 'max Wy', 'Day sin', 'Day cos', 'Year sin', 'Year cos'],\n",
       "        num_rows: 49063\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['p (mbar)', 'T (degC)', 'Tpot (K)', 'Tdew (degC)', 'rh (%)', 'VPmax (mbar)', 'VPact (mbar)', 'VPdef (mbar)', 'sh (g/kg)', 'H2OC (mmol/mol)', 'rho (g/m**3)', 'Wx', 'Wy', 'max Wx', 'max Wy', 'Day sin', 'Day cos', 'Year sin', 'Year cos'],\n",
       "        num_rows: 14018\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['p (mbar)', 'T (degC)', 'Tpot (K)', 'Tdew (degC)', 'rh (%)', 'VPmax (mbar)', 'VPact (mbar)', 'VPdef (mbar)', 'sh (g/kg)', 'H2OC (mmol/mol)', 'rho (g/m**3)', 'Wx', 'Wy', 'max Wx', 'max Wy', 'Day sin', 'Day cos', 'Year sin', 'Year cos'],\n",
       "        num_rows: 7010\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"asanobm/jena_climate_2009_2016\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p (mbar)</th>\n",
       "      <th>T (degC)</th>\n",
       "      <th>Tpot (K)</th>\n",
       "      <th>Tdew (degC)</th>\n",
       "      <th>rh (%)</th>\n",
       "      <th>VPmax (mbar)</th>\n",
       "      <th>VPact (mbar)</th>\n",
       "      <th>VPdef (mbar)</th>\n",
       "      <th>sh (g/kg)</th>\n",
       "      <th>H2OC (mmol/mol)</th>\n",
       "      <th>rho (g/m**3)</th>\n",
       "      <th>Wx</th>\n",
       "      <th>Wy</th>\n",
       "      <th>max Wx</th>\n",
       "      <th>max Wy</th>\n",
       "      <th>Day sin</th>\n",
       "      <th>Day cos</th>\n",
       "      <th>Year sin</th>\n",
       "      <th>Year cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.945308</td>\n",
       "      <td>-1.982473</td>\n",
       "      <td>-2.041888</td>\n",
       "      <td>-1.918973</td>\n",
       "      <td>1.117102</td>\n",
       "      <td>-1.302851</td>\n",
       "      <td>-1.477323</td>\n",
       "      <td>-0.790424</td>\n",
       "      <td>-1.480036</td>\n",
       "      <td>-1.482697</td>\n",
       "      <td>2.218524</td>\n",
       "      <td>0.193409</td>\n",
       "      <td>0.221161</td>\n",
       "      <td>0.111140</td>\n",
       "      <td>0.217928</td>\n",
       "      <td>0.366111</td>\n",
       "      <td>1.366069</td>\n",
       "      <td>-0.061052</td>\n",
       "      <td>1.428434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.959770</td>\n",
       "      <td>-2.078372</td>\n",
       "      <td>-2.138166</td>\n",
       "      <td>-2.060964</td>\n",
       "      <td>1.044617</td>\n",
       "      <td>-1.330143</td>\n",
       "      <td>-1.534354</td>\n",
       "      <td>-0.786272</td>\n",
       "      <td>-1.536190</td>\n",
       "      <td>-1.539035</td>\n",
       "      <td>2.325708</td>\n",
       "      <td>0.172987</td>\n",
       "      <td>0.222101</td>\n",
       "      <td>0.109458</td>\n",
       "      <td>0.227798</td>\n",
       "      <td>0.707200</td>\n",
       "      <td>1.224794</td>\n",
       "      <td>-0.060029</td>\n",
       "      <td>1.428424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.986284</td>\n",
       "      <td>-2.070284</td>\n",
       "      <td>-2.132435</td>\n",
       "      <td>-2.045187</td>\n",
       "      <td>1.062738</td>\n",
       "      <td>-1.328843</td>\n",
       "      <td>-1.527225</td>\n",
       "      <td>-0.788348</td>\n",
       "      <td>-1.528703</td>\n",
       "      <td>-1.531992</td>\n",
       "      <td>2.323998</td>\n",
       "      <td>0.207983</td>\n",
       "      <td>0.276266</td>\n",
       "      <td>0.111218</td>\n",
       "      <td>0.324078</td>\n",
       "      <td>1.000100</td>\n",
       "      <td>1.000059</td>\n",
       "      <td>-0.059006</td>\n",
       "      <td>1.428412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.004362</td>\n",
       "      <td>-2.098014</td>\n",
       "      <td>-2.161090</td>\n",
       "      <td>-2.096820</td>\n",
       "      <td>1.008375</td>\n",
       "      <td>-1.336641</td>\n",
       "      <td>-1.546235</td>\n",
       "      <td>-0.782121</td>\n",
       "      <td>-1.547420</td>\n",
       "      <td>-1.553119</td>\n",
       "      <td>2.358913</td>\n",
       "      <td>0.270343</td>\n",
       "      <td>0.195267</td>\n",
       "      <td>0.246907</td>\n",
       "      <td>0.145176</td>\n",
       "      <td>1.224850</td>\n",
       "      <td>0.707179</td>\n",
       "      <td>-0.057983</td>\n",
       "      <td>1.428400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.061006</td>\n",
       "      <td>-2.165028</td>\n",
       "      <td>-2.232152</td>\n",
       "      <td>-2.187178</td>\n",
       "      <td>0.984214</td>\n",
       "      <td>-1.353535</td>\n",
       "      <td>-1.579503</td>\n",
       "      <td>-0.782121</td>\n",
       "      <td>-1.581113</td>\n",
       "      <td>-1.585982</td>\n",
       "      <td>2.446320</td>\n",
       "      <td>0.112264</td>\n",
       "      <td>0.350818</td>\n",
       "      <td>0.048640</td>\n",
       "      <td>0.402053</td>\n",
       "      <td>1.366133</td>\n",
       "      <td>0.366112</td>\n",
       "      <td>-0.056960</td>\n",
       "      <td>1.428388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49058</th>\n",
       "      <td>0.169167</td>\n",
       "      <td>0.904892</td>\n",
       "      <td>0.886564</td>\n",
       "      <td>1.571997</td>\n",
       "      <td>0.947972</td>\n",
       "      <td>0.776492</td>\n",
       "      <td>1.984916</td>\n",
       "      <td>-0.493604</td>\n",
       "      <td>1.979016</td>\n",
       "      <td>1.977352</td>\n",
       "      <td>-0.882480</td>\n",
       "      <td>-0.166720</td>\n",
       "      <td>-1.508412</td>\n",
       "      <td>-0.155728</td>\n",
       "      <td>-1.363530</td>\n",
       "      <td>-0.955374</td>\n",
       "      <td>1.042725</td>\n",
       "      <td>-0.855026</td>\n",
       "      <td>-1.154613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49059</th>\n",
       "      <td>0.266787</td>\n",
       "      <td>0.879473</td>\n",
       "      <td>0.853325</td>\n",
       "      <td>1.405624</td>\n",
       "      <td>0.621793</td>\n",
       "      <td>0.742703</td>\n",
       "      <td>1.680752</td>\n",
       "      <td>-0.283962</td>\n",
       "      <td>1.672043</td>\n",
       "      <td>1.672192</td>\n",
       "      <td>-0.822662</td>\n",
       "      <td>-0.824901</td>\n",
       "      <td>-0.641039</td>\n",
       "      <td>-0.704391</td>\n",
       "      <td>-0.475387</td>\n",
       "      <td>-0.652949</td>\n",
       "      <td>1.254471</td>\n",
       "      <td>-0.855883</td>\n",
       "      <td>-1.154062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49060</th>\n",
       "      <td>0.217374</td>\n",
       "      <td>0.740824</td>\n",
       "      <td>0.719224</td>\n",
       "      <td>1.425703</td>\n",
       "      <td>1.086900</td>\n",
       "      <td>0.559461</td>\n",
       "      <td>1.716396</td>\n",
       "      <td>-0.605690</td>\n",
       "      <td>1.709479</td>\n",
       "      <td>1.707403</td>\n",
       "      <td>-0.716211</td>\n",
       "      <td>-0.030746</td>\n",
       "      <td>0.645277</td>\n",
       "      <td>0.013960</td>\n",
       "      <td>0.600050</td>\n",
       "      <td>-0.306022</td>\n",
       "      <td>1.380734</td>\n",
       "      <td>-0.856739</td>\n",
       "      <td>-1.153511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49061</th>\n",
       "      <td>0.213759</td>\n",
       "      <td>0.710783</td>\n",
       "      <td>0.689423</td>\n",
       "      <td>1.412795</td>\n",
       "      <td>1.147303</td>\n",
       "      <td>0.521772</td>\n",
       "      <td>1.692633</td>\n",
       "      <td>-0.645127</td>\n",
       "      <td>1.687018</td>\n",
       "      <td>1.683929</td>\n",
       "      <td>-0.689842</td>\n",
       "      <td>0.014300</td>\n",
       "      <td>-0.102616</td>\n",
       "      <td>-0.095117</td>\n",
       "      <td>-0.198426</td>\n",
       "      <td>0.061765</td>\n",
       "      <td>1.412909</td>\n",
       "      <td>-0.857595</td>\n",
       "      <td>-1.152959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49062</th>\n",
       "      <td>0.166757</td>\n",
       "      <td>0.653013</td>\n",
       "      <td>0.636700</td>\n",
       "      <td>1.352556</td>\n",
       "      <td>1.177505</td>\n",
       "      <td>0.450295</td>\n",
       "      <td>1.588077</td>\n",
       "      <td>-0.670035</td>\n",
       "      <td>1.582198</td>\n",
       "      <td>1.580644</td>\n",
       "      <td>-0.646139</td>\n",
       "      <td>0.519065</td>\n",
       "      <td>-0.328126</td>\n",
       "      <td>0.627259</td>\n",
       "      <td>-0.494435</td>\n",
       "      <td>0.425349</td>\n",
       "      <td>1.348804</td>\n",
       "      <td>-0.858451</td>\n",
       "      <td>-1.152406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49063 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       p (mbar)  T (degC)  Tpot (K)  Tdew (degC)    rh (%)  VPmax (mbar)  \\\n",
       "0      0.945308 -1.982473 -2.041888    -1.918973  1.117102     -1.302851   \n",
       "1      0.959770 -2.078372 -2.138166    -2.060964  1.044617     -1.330143   \n",
       "2      0.986284 -2.070284 -2.132435    -2.045187  1.062738     -1.328843   \n",
       "3      1.004362 -2.098014 -2.161090    -2.096820  1.008375     -1.336641   \n",
       "4      1.061006 -2.165028 -2.232152    -2.187178  0.984214     -1.353535   \n",
       "...         ...       ...       ...          ...       ...           ...   \n",
       "49058  0.169167  0.904892  0.886564     1.571997  0.947972      0.776492   \n",
       "49059  0.266787  0.879473  0.853325     1.405624  0.621793      0.742703   \n",
       "49060  0.217374  0.740824  0.719224     1.425703  1.086900      0.559461   \n",
       "49061  0.213759  0.710783  0.689423     1.412795  1.147303      0.521772   \n",
       "49062  0.166757  0.653013  0.636700     1.352556  1.177505      0.450295   \n",
       "\n",
       "       VPact (mbar)  VPdef (mbar)  sh (g/kg)  H2OC (mmol/mol)  rho (g/m**3)  \\\n",
       "0         -1.477323     -0.790424  -1.480036        -1.482697      2.218524   \n",
       "1         -1.534354     -0.786272  -1.536190        -1.539035      2.325708   \n",
       "2         -1.527225     -0.788348  -1.528703        -1.531992      2.323998   \n",
       "3         -1.546235     -0.782121  -1.547420        -1.553119      2.358913   \n",
       "4         -1.579503     -0.782121  -1.581113        -1.585982      2.446320   \n",
       "...             ...           ...        ...              ...           ...   \n",
       "49058      1.984916     -0.493604   1.979016         1.977352     -0.882480   \n",
       "49059      1.680752     -0.283962   1.672043         1.672192     -0.822662   \n",
       "49060      1.716396     -0.605690   1.709479         1.707403     -0.716211   \n",
       "49061      1.692633     -0.645127   1.687018         1.683929     -0.689842   \n",
       "49062      1.588077     -0.670035   1.582198         1.580644     -0.646139   \n",
       "\n",
       "             Wx        Wy    max Wx    max Wy   Day sin   Day cos  Year sin  \\\n",
       "0      0.193409  0.221161  0.111140  0.217928  0.366111  1.366069 -0.061052   \n",
       "1      0.172987  0.222101  0.109458  0.227798  0.707200  1.224794 -0.060029   \n",
       "2      0.207983  0.276266  0.111218  0.324078  1.000100  1.000059 -0.059006   \n",
       "3      0.270343  0.195267  0.246907  0.145176  1.224850  0.707179 -0.057983   \n",
       "4      0.112264  0.350818  0.048640  0.402053  1.366133  0.366112 -0.056960   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "49058 -0.166720 -1.508412 -0.155728 -1.363530 -0.955374  1.042725 -0.855026   \n",
       "49059 -0.824901 -0.641039 -0.704391 -0.475387 -0.652949  1.254471 -0.855883   \n",
       "49060 -0.030746  0.645277  0.013960  0.600050 -0.306022  1.380734 -0.856739   \n",
       "49061  0.014300 -0.102616 -0.095117 -0.198426  0.061765  1.412909 -0.857595   \n",
       "49062  0.519065 -0.328126  0.627259 -0.494435  0.425349  1.348804 -0.858451   \n",
       "\n",
       "       Year cos  \n",
       "0      1.428434  \n",
       "1      1.428424  \n",
       "2      1.428412  \n",
       "3      1.428400  \n",
       "4      1.428388  \n",
       "...         ...  \n",
       "49058 -1.154613  \n",
       "49059 -1.154062  \n",
       "49060 -1.153511  \n",
       "49061 -1.152959  \n",
       "49062 -1.152406  \n",
       "\n",
       "[49063 rows x 19 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 Config\n",
    "\n",
    "from transformers import TimeSeriesTransformerConfig\n",
    "\n",
    "config = TimeSeriesTransformerConfig(\n",
    "    prediction_length=None, #  예측 길이 (int) — 디코더의 예측 길이. 다시 말해, 모델의 예측 지평선. 이 값은 일반적으로 데이터 세트에 의해 결정되며 적절하게 설정하는 것이 좋습니다.\n",
    "    context_length=None, # Context length (int, 선택 사항, 기본값은 prediction_length) — 인코더의 컨텍스트 길이. None인 경우, 컨텍스트 길이는 prediction_length와 동일합니다.\n",
    "    distribution_output=None, # 분배 출력 (문자열, 선택 사항, 기본값은 \"student_t\") — 모델의 분포 방출 헤드. \"Student_t\", \"normal\" 또는 \"negative_binomial\"일 수 있습니다.\n",
    "    loss=None, # 손실 (문자열, 선택 사항, 기본값은 \"nll\") - distribution_output 헤드에 해당하는 모델의 손실 함수. 파라메트릭 분포의 경우 음의 로그 가능성(nll)입니다 - 현재 유일하게 지원되는 것입니다.\n",
    "    input_size=None # input_size (int, optional, defaults to 1) — The size of the target variable which by default is 1 for univariate targets. Would be > 1 in case of multivariate targets.\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
