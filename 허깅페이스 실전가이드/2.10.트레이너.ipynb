{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10. íŠ¸ë ˆì´ë„ˆ\n",
    "\n",
    "í—ˆê¹…í˜ì´ìŠ¤ íŠ¸ë ˆì´ë„ˆ(Trainer)ëŠ” ëª¨ë¸ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ ì „ë°˜ì„ ì¶”ìƒí™”í•´ ê°„ì†Œí™”ëœ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•œë‹¤. ì´ë¥¼ í†µí•´ ëª‡ ì¤„ì˜ ì½”ë“œë§Œìœ¼ë¡œë„ ë³µì¡í•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "**íŠ¸ë ˆì´ë„ˆì˜ ì¥ì **\n",
    "\n",
    "---\n",
    "* **ì½”ë“œ ì¬ì‚¬ìš©ì„± ë° ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ**: íŠ¸ë ˆì´ë„ˆëŠ” ì˜ í…ŒìŠ¤íŠ¸ë˜ê³  ê²€ì¦ëœ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ, ì§ì ‘ êµ¬í˜„í•˜ëŠ” ê²ƒë³´ë‹¤ ì•ˆì •ì ì´ê³  ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì½”ë“œë¥¼ ì‘ì„±í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ í†µí•´ ì½”ë“œì˜ í’ˆì§ˆê³¼ ì¼ê´€ì„±ì„ ë†’ì¼ ìˆ˜ ìˆë‹¤.\n",
    "* **ë²”ìš©ì„±**: íŠ¸ë ˆì´ë„ˆëŠ” ë‹¤ì–‘í•œ ìœ í˜•ì˜ ëª¨ë¸ê³¼ ì‘ì—…ì— ë²”ìš©ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ë“± ë‹¤ì–‘í•œ ë°ì´í„° ìœ í˜•ì„ ì§€ì›í•˜ë©°, ë¶„ë¥˜, íšŒê·€, ì‹œí€€ìŠ¤ ë ˆì´ë¸”ë§ ë“± ë‹¤ì–‘í•œ ì‘ì—…ì„ ì§€ì›í•œë‹¤.\n",
    "* **ëª¨ë‹ˆí„°ë§ ë° ì œì–´**: ì½œë°± í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê³  ì œì–´í•  ìˆ˜ ìˆë‹¤. íŠ¹ì • ì´ë²¤íŠ¸ê°€ ë°œìƒí–ˆì„ ë•Œ ì›í•˜ëŠ” ì‘ë™ ë°©ì‹ì„ ì €ìœ¼ì´í•  ìˆ˜ ìˆë‹¤. \n",
    "\n",
    "í—ˆê¹…í˜ì´ìŠ¤ì˜ íŠ¸ë ˆì´ë„ˆëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ë¥¼ ê°„ì†Œí™”í•˜ê³  ê°•ë ¥í•œ ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤. íŠ¸ë ˆì´ë„ˆëŠ” ëª¨ë¸, ë°ì´í„°ì„¸íŠ¸, ìµœì í™” ê¸°ë²•, í•™ìŠµë¥  ìŠ¤ì¼€ì¥´ëŸ¬ ë“±ì„ í†µí•©ì ìœ¼ë¡œ ê´€ë¦¬í•´ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ ì „ë°˜ì— ê±¸ì¹œ ì„¤ì •ì„ ì†ì‰½ê²Œ ì¡°ì •í•  ìˆ˜ ìˆëŒ€ìš” ....\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10.1. íŠ¸ë ˆì´ë„ˆ í´ë˜ìŠ¤\n",
    "\n",
    "í—ˆê¹…í˜ì´ìŠ¤ì˜ íŠ¸ë ˆì´ë„ˆ í´ë˜ìŠ¤ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì‘ì—…ì„ ë‹¨ìˆœí™”í•˜ê³  ì²´ê³„í™”í–ˆë‹¤. \n",
    "\n",
    "**íŠ¸ë ˆì´ë„ˆ í´ë˜ìŠ¤ì˜ ì£¼ìš” ë§¤ê°œë³€ìˆ˜**\n",
    "\n",
    "---\n",
    "* **model**: í•™ìŠµ, í‰ê°€ ë˜ëŠ” ì˜ˆì¸¡ì— ì‚¬ìš©í•  ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë‹¤ `PreTrainedModel` ë˜ëŠ” `torch.nn.Module` í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ì—¬ì•¼ í•œë‹¤. ëª¨ë¸ì„ ì œê³µí•˜ì§€ ì•Šìœ¼ë©´ `model_init`ì„ ì „ë‹¬í•´ì•¼ í•œë‹¤.\n",
    "* **args**: í•™ìŠµ ê´€ë ¨ ì„¤ì •ì„ ì¡°ì •í•˜ê¸° ìœ„í•œ íŠ¸ë ˆì´ë‹ ì¸ìˆ˜ë‹¤. ì…ë ¥í•˜ì§€ ì•Šìœ¼ë©´ ì§€ë§˜ëŒ€ë¡œ í•œë‹¤. ì¶œë ¥ ë””ë ‰í† ë¦¬ëŠ” í˜„ì¬ ë””ë ‰í† ë¦¬ì— tmp_trainer ë””ë ‰í† ë¦¬ë¥¼ ë§Œë“¤ì–´ ì €ì¥í•œë‹¤. ì •ë§ ì§€ë§˜ëŒ€ë¡œ í•œë‹¤.\n",
    "* **data_collator**: í•™ìŠµ ë˜ëŠ” í‰ê°€ ë°ì´í„°ì„¸íŠ¸ì—ì„œ ì¶”ì¶œëœ ë°°ì¹˜ë¥¼ ëª¨ë¸ì— ì…ë ¥í•˜ê¸° ì „ì— íŒ¨ë”©, ë§ˆìŠ¤í‚¹ ë“±ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ í•¨ìˆ˜ë¥¼ ì…ë ¥í•œë‹¤.\n",
    "* **train_dataset**: í•™ìŠµì— ì‚¬ìš©í•  ë°ì´í„°ì„¸íŠ¸ë‹¤. ëª¨ë¸ì˜ ìˆœë°©í–¥ ë©”ì„œë“œì— í—ˆìš©í•˜ì§€ ì•ŠëŠ” ì—´ì€ ìë™ìœ¼ë¡œ ì œê±°ëœë‹¤. ë¶„ì‚° í™˜ê²½ì—ì„œ í•™ìŠµí•  ê²½ìš°, ë°ì´í„°ì„¸íŠ¸ëŠ” ëª¨ë“  í”„ë¡œì„¸ìŠ¤ì—ì„œ ë™ì¼í•œ ì‹œë“œë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ `set_epoch` ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ì•¼ í•œë‹¤.\n",
    "* **eval_dataset**: í‰ê°€ì— ì‚¬ìš©í•  ë°ì´í„°ì„¸íŠ¸ë‹¤. ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° ê° ë°ì´í„°ì„¸íŠ¸ì— ëŒ€í•´ í‚¤ë¥¼ ì§€í‘œ ì´ë¦„ ì•ì— ì¶”ê°€í•´ í‰ê°€í•œë‹¤.\n",
    "* **tokenizer**: ë°ì´í„° ì „ì²˜ë¦¬ì— ì‚¬ìš©í•  `PreTrainedTokenizer`ì¸ìŠ¤í„´ìŠ¤ë‹¤. ë°°ì¹˜ ìƒì„± ì‹œ ìë™ìœ¼ë¡œ ì…ë ¥ì„ ìµœëŒ€ ê¸¸ì´ë¡œ íŒ¨ë”©í•œë‹¤. í† í¬ë‚˜ì´ì €ëŠ” ëª¨ë¸ê³¼ í•¨ê»˜ ì €ì¥ë˜ì–´ í•™ìŠµì´ ì¤‘ë‹¨ ë¼ë”ë¼ë„ ì¬ì‚¬ìš© í•  ìˆ˜ ìˆë‹¤.\n",
    "* **model_init**: ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì„¤ì •í•œë‹¤. `train`ë©”ì„œë“œê°€ í˜¸ì¶œë  ë•Œë§ˆë‹¤ ìƒˆë¡œìš´ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ê°€ ìƒì„±ëœë‹¤.\n",
    "* **compute_metrics**: í‰ê°€ ì‹œ ì§€í‘œë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì…ë ¥í•œë‹¤. ì •í™•ë„ë‚˜ F1 ì ìˆ˜ ë“±ì„ ì´ìš©í•´ ëª¨ë¸ì„ í‰ê°€í•œë‹¤.\n",
    "* **callbacks**: í•™ìŠµ í”„ë¡œì„¸ìŠ¤ ì¤‘ì— íŠ¹ì • ì´ë²¤íŠ¸ê°€ ë°œìƒí–ˆì„ ë•Œ ì‹¤í–‰ë  ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ë¥¼ ë“±ë¡í•œë‹¤. ì—í­ì´ ëë‚  ë•Œë§ˆë‹¤ ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ì‹¶ì„ ë•Œë‚˜ í•™ìŠµ ì¤‘ íŠ¹ì • ì¡°ê±´ì— ë”°ë¼ ì¡°ê¸° ì¢…ë£Œí•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©í•œë‹¤.\n",
    "* **optimizers**: ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ì„¤ì •í•œë‹¤. `AdamW`, `Adafactor`, `Adamax` ë“±ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ê¸°ë³¸ì€ `AdamW`ë‹¤.ì—­ì‹œ ì§€ë§˜ëŒ€ë¡œ í•œë‹¤.\n",
    "* **preprocess_logits_for_metrics**: ì§€í‘œ ê³„ì‚°ì„ ìœ„í•´ ëª¨ë¸ ì¶œë ¥ì„ ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì…ë ¥í•œë‹¤. ê¸°ë³¸ê°’ì€ `None`ì´ë‹¤.\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10.2. íŠ¸ë ˆì´ë‹ ì¸ìˆ˜\n",
    "\n",
    "íŠ¸ë ˆì´ë‹ ì¸ìˆ˜ëŠ” í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ì„¤ì •ì„ ìº¡ìŠí™”í•œ í´ë˜ìŠ¤ë‹¤. íŠ¸ë ˆì´ë„ˆ í´ë˜ìŠ¤ì— ì „ë‹¬ë˜ì–´ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ë¥¼ ì œì–´í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤. ì´ë ‡ê²Œ ëª©ì ì„ ë¶„ë¦¬í•´ íŠ¹ì • ëª¨ë¸ì´ë‚˜ ê³¼ì œì— êµ¬ì• ë°›ì§€ ì•Šê³  ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì¸í„°í˜ì´ìŠ¤ë¡œ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "```python\n",
    "training_args = transformers.TrainingArguments(...)\n",
    "```\n",
    "* **output_dir**: ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ì™€ ì²´í¬í¬ì¸íŠ¸ê°€ ì €ì¥ë  ì¶œë ¥ ê²½ë¡œë¥¼ ì§€ì •í•œë‹¤.\n",
    "* **overwrite_output_dir**: ì¶œë ¥ ê²½ë¡œì— ì´ë¯¸ íŒŒì¼ì´ ìˆì„ ë•Œ ë®ì–´ì“¸ì§€ ì—¬ë¶€ë¥¼ ì§€ì •í•œë‹¤. ì´ì „ì— í•™ìŠµì„ ì¤‘ë‹¨í–ˆë˜ ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ í•™ìŠµì„ ì¬ê°œí•˜ê³  ì‹¶ì€ ê²½ìš°ì—ë„ `True`ë¡œ ì„¤ì • í• ë•Œê°€ ìˆëŒ„ë‹¤. ê¸°ë³¸ê°’ì€ `False`ë‹¤.\n",
    "* **do_train**: í•™ìŠµì„ ìˆ˜í–‰í• ì§€ ì—¬ë¶€ë¥¼ ì§€ì •í•œë‹¤. `False`ë¡œ ì„¤ì •í•˜ë©´ í•™ìŠµì„ ìˆ˜í–‰í•˜ì§€ ì•ŠëŠ”ë‹¤. ê¸°ë³¸ê°’ì€ `False`ë‹¤.\n",
    "* **do_eval**: ëª¨ë¸ í•™ìŠµì¤‘ ê²€ì¦ ë°ì´í„°ì„¸íŠ¸ì— ëŒ€í•œ í‰ê°€ ìˆ˜í–‰ ì—¬ë¶€ë¥¼ ì„¤ì •í•œë‹¤. eval_strategyê°€ noê°€ ì•„ë‹Œê²½ìš° Trueë¡œ ì„¤ì •ëœë‹¤. ê¸°ë³¸ê°’ì€ `False`ë‹¤.\n",
    "* **do_predict**: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì„¸íŠ¸ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í• ì§€ ì—¬ë¶€ë¥¼ ì§€ì •í•œë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ëª¨ë¸ í‰ê°€ì—ì„œ ì‚¬ìš©ëœë‹¤. ê¸°ë³¸ê°’ì€ `False`ë‹¤.\n",
    "* **eval_strategy**: ëª¨ë¸ í•™ìŠµ ì¤‘ í‰ê°€ ì „ëµì„ ì„¤ì •í•œë‹¤. `no`ë¡œ ì„¤ì •í•˜ë©´ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ì§€ ì•ŠëŠ”ë‹¤. stepsë¡œ ì„¤ì •í•œ ê²½ìš° eval_steps ê°„ê²©ë§ˆë‹¤ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ê³  ë¡œê·¸ë¥¼ ê¸°ë¡í•œë‹¤. epochë¡œ ì„¤ì •í•˜ëŠ” ê²½ìš° ê° ì—í­ ì¢…ë£Œ í›„ í‰ê°€ë¥¼ ìˆ˜í–‰í•œë‹¤.\n",
    "* **per_device_train_batch_size**: í•™ìŠµ ì¤‘ ê° ì¥ì¹˜ì— í• ë‹¹ëœ ë°°ì¹˜ í¬ê¸°ë¥¼ ì„¤ì •í•œë‹¤. ê¸°ë³¸ê°’ì€ 8ì´ë‹¤. GPUê°€ 2ê°œ ì´ë©´ 16ìœ¼ë¡œ ì„¤ì •ë˜ëŠ” ê²ƒì´ë‹¤.\n",
    "* **per_device_batch_size**: ëª¨ë¸ í‰ê°€ ì‹œ ì¥ì¹˜ë‹¹ ë°°ì¹˜ í¬ê¸°ë¥¼ ì˜ë¯¸í•œë‹¤. per_device_train_batch_sizeì™€ ë™ì¼í•˜ê²Œ ì„¤ì •í•œë‹¤.\n",
    "* **eval_delay**: ì²« í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ê¸° ì „ì— ê¸°ë‹¤ë ¤ì•¼í•˜ëŠ ì—í­ ë˜ëŠ” ìŠ¤í… ìˆ˜ë¥¼ ì„¤ì •í•œë‹¤.\n",
    "* **learning_rate**: í•™ìŠµë¥ ì„ ì„¤ì •í•œë‹¤. ê¸°ë³¸ê°’ì€ 5e-5ë‹¤.\n",
    "* **weight_decay**: ê°€ì¤‘ì¹˜ ê°ì‡  ê°’ì„ ì„¤ì •í•œë‹¤. ê¸°ë³¸ê°’ì€ 0.0ì´ë‹¤.\n",
    "* **adam_beta1, adam_beta2, adam_epsilon**: Adam ì˜µí‹°ë§ˆì´ì €ì˜ ë² íƒ€1, ë² íƒ€2, ì—¡ì‹¤ë¡  ê°’ì„ ì„¤ì •í•œë‹¤. ê¸°ë³¸ê°’ì€ 0.9, 0.999, 1e-8ì´ë‹¤.\n",
    "* **max_grad_norm**: ê·¸ë ˆì´ë””ì–¸íŠ¸ í´ë¦¬í•‘ì„ ìœ„í•œ ìµœëŒ€ ê·¸ë ˆì´ë””ì–¸íŠ¸ ë…¸ë¦„ ê°’ì„ ì„¤ì •í•œë‹¤. ê¸°ë³¸ê°’ì€ 1.0ì´ë‹¤.\n",
    "* **num_train_epochs**: ìˆ˜í–‰í•  ì´ ì—í­ ìˆ˜ë¥¼ ì„¤ì •í•œë‹¤. ê¸°ë³¸ê°’ì€ 3ì´ë‹¤.\n",
    "* **max_steps**: ìµœëŒ€ í•™ìŠµ ìŠ¤í… ìˆ˜ë¥¼ ì„¤ì •í•œë‹¤. ì–‘ì˜ ì •ìˆ˜ë¡œ ì„¤ì •í•˜ë©´ num_train_epochsë¥¼ ë¬´ì‹œí•˜ê³  í•´ë‹¹ ìŠ¤í…ê¹Œì§€ë§Œ í•™ìŠµí•œë‹¤. ê¸°ë³¸ê°’ì€ -1ì´ë‹¤.\n",
    "* **warmup_steps**: í•™ìŠµë¥  ìŠ¤ì¼€ì¥´ëŸ¬ì—ì„œ ì›œì—… ìŠ¤í… ìˆ˜ë¥¼ ì„¤ì •í•œë‹¤. í•™ìŠµ ì´ˆê¸°ì— í•™ìŠµë¥ ì„ ì ì§„ì ìœ¼ë¡œ ì¦ê°€ì‹œí‚¤ëŠ” ê¸°ë²•ìœ¼ë¡œ ì›œì—… ë‹¨ê³„ë™ì•ˆ í•œìŠµë¥ ì€ ì¼ì • ìŠ¤ì¼€ì¥´ì— ë”°ë¼ ì ì§„ì ìœ¼ë¡œ ì¦ê°€í•œë‹¤. ê¸°ë³¸ê°’ì€ 0ì´ë‹¤.\n",
    "* **logging_dir**: í…ì„œë³´ë“œ ë¡œê·¸ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ì§€ì •í•œë‹¤. ê¸°ë³¸ê°’ì€ `runs`ë‹¤.\n",
    "* **logging_strategy**: ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•  ë¡œê¹… ì „ëµìœ¼ë¡œ no, epoch, stepsë¥¼ ì§€ì •í•  ìˆ˜ ìˆë‹¤. ê¸°ë³¸ê°’ì€ stepsë‹¤.\n",
    "* **logging_steps**: ë¡œê¹… ê°„ê²©ì„ ì„¤ì •í•œë‹¤. ê¸°ë³¸ê°’ì€ 500ì´ë‹¤.\n",
    "* **save_strategy**: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì „ëµì„ ì„¤ì •í•œë‹¤. no, epoch, stepsë¥¼ ì§€ì •í•  ìˆ˜ ìˆë‹¤. ê¸°ë³¸ê°’ì€ stepsë‹¤.\n",
    "* **save_steps**: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê°„ê²©ì„ ì„¤ì •í•œë‹¤. `save_strategy`ê°€ `steps`ë¡œ ì„¤ì •ëœ ê²½ìš°ì— ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•  ì—…ë°ì´íŠ¸ ìŠ¤í… ê°„ê²©ì„ ì˜ë¯¸í•œë‹¤. 0ê³¼ 1ì‚¬ì´ì— ì‹¤ìˆ˜ë¥¼ ì…ë ¥í•˜ë©´ ì´ í•™ìŠµ ìŠ¤í…ì˜ ë¹„ìœ¨ë¡œ ì§€ì •ëœë‹¤.\n",
    "* **save_total_limit**: ì €ì¥í•  ì²´í¬í¬ì¸íŠ¸ì˜ ìµœëŒ€ ê°œìˆ˜ë¥¼ ì„¤ì •í•œë‹¤. ê¸°ë³¸ê°’ì€ 5ë‹¤. ì§€ì •ìˆ˜ë¥¼ ì´ˆê³¼í•˜ë©´ ê°€ì¥ ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚­ì œí•œë‹¤. `load_best_model_at_end`ê°€ `True`ë¡œ ì„¤ì •ëœ ê²½ìš° ìµœê³  ì„±ëŠ¥ì˜ ëª¨ë¸ì€ ì‚­ì œë˜ì§€ ì•ŠëŠ”ë‹¤.\n",
    "* **seed**: ëœë¤ ì‹œë“œë¥¼ ì„¤ì •í•œë‹¤. í•™ìŠµì„ ì¬í˜„í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•œë‹¤. ê¸°ë³¸ê°’ì€ 42ë‹¤.\n",
    "* **fp16**: 16ë¹„íŠ¸ í˜¼í•© ì •ë°€ë„ í•™ìŠµ ìˆ˜í–‰ ì—¬ë¶€ë¥¼ ì„¤ì •í•œë‹¤. Trueë¡œ ì„¤ì •í•˜ë©´ 32ë¹„íŠ¸ ëŒ€ì‹  16ë¹„íŠ¸ í˜¼í•© ì •ë°€ë„ë¡œ í•™ìŠµì„ ìˆ˜í–‰í•œë‹¤.\n",
    "* **fp16_opt_level**: pf16 í•™ìŠµ ì‹œ Apex AMP(Nvidiaì—ì„œ ì œê³µí•˜ëŠ” ìë™ í˜¼í•© ì •ë°€ë„)ìµœì í™” ìˆ˜ì¤€ì„ ì„¤ì •í•œë‹¤. 01, 02ë“±ì„ ì§€ì •í•  ìˆ˜ ìˆë‹¤.\n",
    "* **run_name**: WandB, MLflowë“±ì˜ ë¡œê¹… ë„êµ¬ì—ì„œ ì‚¬ìš©í•  ë ˆì´ë¸”ì„ ì„¤ì •í•œë‹¤. ì„¤ì •í•˜ì§€ ì•Šìœ¼ë©´ output_dirê³¼ ë™ì¼í•˜ê²Œ í• ë‹¹ëœë‹¤.\n",
    "* **load_best_model_at_end**: í•™ìŠµ ì¢…ë£Œ í›„ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ì§€ ì—¬ë¶€ë¥¼ ì„¤ì •í•œë‹¤. Trueë¥¼ ì„¤ì •í•˜ë©´ ìµœì  ì²´í¬í¬ì¸íŠ¸ê°€ í•­ìƒ ë³´ì¡´ëœë‹¤.\n",
    "* **metric_for_best_model**: í‰ê°€ì§€í‘œì—ì„œ í° ê°’ì´ ë” ì¢‹ì€ ëª¨ë¸ì¸ì§€ ì—¬ë¶€ë¥¼ ì§€ì •í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ `metric_for_best_model=eval_accuracy`ë¡œ ì§€ì •í–ˆë‹¤ë©´ `greater_is_better=True`ë¡œ í•´ì•¼ ì •í™•ë„ê°€ ë†’ì€ ëª¨ë¸ì´ ìµœì  ëª¨ë¸ë¡œ ì„ íƒë˜ë©°, ë°˜ëŒ€ë¡œ `metric_for_best_model=\"eval_loss\"`ë¡œ ì§€ì •í–ˆë‹¤ë©´ `greater_is_better=False`ë¡œ ì„¤ì •í•´ì•¼ ì†ì‹¤ì´ ë‚®ì€ ëª¨ë¸ì´ ìµœì  ëª¨ë¸ë¡œ ì„ íƒëœë‹¤.\n",
    "* **optim**: ìµœì í™” í•¨ìˆ˜ë¥¼ ì„¤ì •í•œë‹¤. `AdamW`, `Adafactor`, `Adamax` ë“±ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ê¸°ë³¸ì€ `AdamW`ë‹¤.\n",
    "* **optim_args**: ìµœì í™” í•¨ìˆ˜ì— ì „ë‹¬í•  ì¶”ê°€ ì¸ìˆ˜ë¥¼ í• ë‹¹í•œë‹¤.\n",
    "* **report_to**: ê²°ê³¼ì™€ ë¡œê·¸ë¥¼ ì „ë‹¬í•  í”Œë«í¼ì„ ì„¤ì •í•œë‹¤. wandb, tensorboard, mlflow ë“±ì„ ì§€ì •í•  ìˆ˜ ìˆë‹¤.\n",
    "* **push_to_hub**: ëª¨ë¸ì´ ì €ì¥ë  ë•Œë§ˆë‹¤ í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì— ëª¨ë¸ì„ ì—…ë¡œë“œí• ì§€ ì„¤ì •í•œë‹¤.\n",
    "* **resume_from_checkpoint**: ì´ì „ì— ì €ì¥í•œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ í•™ìŠµì„ ì¬ê°œí• ì§€ ì—¬ë¶€ë¥¼ ì„¤ì •í•œë‹¤. ê²½ë¡œë¥¼ ì§€ì •í•˜ë©´ í•´ë‹¹ ì²´í¬í¬ì¸íŠ¸ì—ì„œ í•™ìŠµì„ ì¬ê°œí•œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10.3. í† í° ë¶„ë¥˜ - ê°œì²´ëª… ì¸ì‹\n",
    "\n",
    "í† í° ë¶„ë¥˜(Token classification)ëª¨ë¸ì€ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ ë¬¸ì¥ì„ êµ¬ì„±í•˜ëŠ” ê° í† í°ì„ í• ë‹¹í•˜ëŠ” ëª¨ë¸ì„ ì˜ë¯¸í•œë‹¤. ê°œì²´ëª… ì¸ì‹ ëª¨ë¸ì€ ë¬¸ì¥ì—ì„œ ì‚¬ëŒ, ì¡°ì§, ìœ„ì¹˜ ë“±ì˜ ê°œì²´ëª…ì„ ì¸ì‹í•˜ê³  í•´ë‹¹ ê°œì²´ëª…ì— ëŒ€í•œ ë²”ì£¼ë¥¼ ë ˆì´ë¸”ë¡œ í• ë‹¹í•œë‹¤.\n",
    "\n",
    "**í•œêµ­ì •ë³´í†µì‹ ê¸°ìˆ í˜‘íšŒì—ì„œ í‘œì¤€ìœ¼ë¡œ ì œì‹œí•œ ê°œì²´ëª… ì¸ì‹ ë ˆì´ë¸”**\n",
    "\n",
    "... ~ ì–´ë”” ìˆëŠ”ê±´ì§€ ì¶œì²˜ê°€ .....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KLUE ë°ì´í„°ì„¸íŠ¸ë¥¼ ì´ìš©í•œ ê°œì±„ëª… ì¸ì‹**\n",
    "\n",
    "KLUE(Korean Language Understanding Evaluation) ë°ì´í„°ì„¸íŠ¸ëŠ” í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë°ì´í„°ì„¸íŠ¸ë‹¤. KLUE ë°ì´í„°ì„¸íŠ¸ëŠ” ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì„ ìœ„í•œ ë°ì´í„°ì„¸íŠ¸ë¡œ êµ¬ì„±ë¼ ìˆë‹¤. ì´ ë°ì´í„°ì„¸íŠ¸ë¥¼ ì´ìš©í•´ì„œ ê°œì±„ëª… ì¸ì‹ì„ í•œëŒ„ë‹¤. load_datasetì„ ì´ìš©í•´ ë°ì´í„°ì„¸íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤. nerëŠ” (Named Entity Recognition)ê°œì²´ëª… ì¸ì‹ì„ ì˜ë¯¸í•œë‹¤. ì˜ˆì œ ì½”ë“œì—ë„ ê°ì²´ëª… ì¸ì‹ì„ ìœ„í•œ ëª¨ë¸ì´ ì‚¬ìš©ëœë‹¤. ì¦‰ ëª©ì ì— ë§ëŠ” ë°ì´í„°ì„¸íŠ¸ê°€ ìˆê³  ê·¸ ë°ì´í„°ì„¸íŠ¸ë¥¼ ì´ìš©í•˜ëŠ” ëª¨ë¸ì„ ì‚¬ìš©í–ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset klue (/home/asanobm/.cache/huggingface/datasets/klue/ner/1.0.0/e0fc3bc3de3eb03be2c92d72fd04a60ecc71903f821619cb28ca0e1e29e4233e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138de44591b940a39ac3ca130caa2d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'tokens', 'ner_tags'],\n",
      "        num_rows: 21008\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'tokens', 'ner_tags'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at Leo97/KoELECTRA-small-v3-modu-ner and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([31]) in the checkpoint and torch.Size([13]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([31, 256]) in the checkpoint and torch.Size([13, 256]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'tokens', 'ner_tags'],\n",
      "    num_rows: 21008\n",
      "})\n",
      "{'sentence': 'íŠ¹íˆ <ì˜ë™ê³ ì†ë„ë¡œ:LC> <ê°•ë¦‰:LC> ë°©í–¥ <ë¬¸ë§‰íœ´ê²Œì†Œ:LC>ì—ì„œ <ë§Œì¢…ë¶„ê¸°ì :LC>ê¹Œì§€ <5ã:QT> êµ¬ê°„ì—ëŠ” ìŠ¹ìš©ì°¨ ì „ìš© ì„ì‹œ ê°“ê¸¸ì°¨ë¡œì œë¥¼ ìš´ì˜í•˜ê¸°ë¡œ í–ˆë‹¤.', 'tokens': ['íŠ¹', 'íˆ', ' ', 'ì˜', 'ë™', 'ê³ ', 'ì†', 'ë„', 'ë¡œ', ' ', 'ê°•', 'ë¦‰', ' ', 'ë°©', 'í–¥', ' ', 'ë¬¸', 'ë§‰', 'íœ´', 'ê²Œ', 'ì†Œ', 'ì—', 'ì„œ', ' ', 'ë§Œ', 'ì¢…', 'ë¶„', 'ê¸°', 'ì ', 'ê¹Œ', 'ì§€', ' ', '5', 'ã', ' ', 'êµ¬', 'ê°„', 'ì—', 'ëŠ”', ' ', 'ìŠ¹', 'ìš©', 'ì°¨', ' ', 'ì „', 'ìš©', ' ', 'ì„', 'ì‹œ', ' ', 'ê°“', 'ê¸¸', 'ì°¨', 'ë¡œ', 'ì œ', 'ë¥¼', ' ', 'ìš´', 'ì˜', 'í•˜', 'ê¸°', 'ë¡œ', ' ', 'í–ˆ', 'ë‹¤', '.'], 'ner_tags': [12, 12, 12, 2, 3, 3, 3, 3, 3, 12, 2, 3, 12, 12, 12, 12, 2, 3, 3, 3, 3, 12, 12, 12, 2, 3, 3, 3, 3, 12, 12, 12, 8, 9, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]}\n",
      "['B-DT', 'I-DT', 'B-LC', 'I-LC', 'B-OG', 'I-OG', 'B-PS', 'I-PS', 'B-QT', 'I-QT', 'B-TI', 'I-TI', 'O']\n"
     ]
    }
   ],
   "source": [
    "# KLUE ë°ì´í„°ì„¸íŠ¸ ë° ê°œì²´ëª… ì¸ì‹ ëª¨ë¸ ì´ˆê¸°í™”\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "# KLUE ë°ì´í„°ì„¸íŠ¸ \n",
    "dataset = load_dataset(\"klue\", \"ner\")\n",
    "print(dataset)\n",
    "labels = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "\n",
    "model_name = \"Leo97/KoELECTRA-small-v3-modu-ner\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "  model_name,\n",
    "  num_labels=len(labels),\n",
    "  ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "print(dataset[\"train\"])\n",
    "print(dataset[\"train\"][0])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ê°œì²´ëª… ì¸ì‹ì„ ìœ„í•œ ë°ì´í„° ì „ì²˜ë¦¬**\n",
    "\n",
    "preprocess_data í•¨ìˆ˜ëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ í† í¬ë‚˜ì´ì €ì— ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜í•œë‹¤. example[\"token\"]ì—ì„œ \"\".joinì„ í†µí•´ ë¬¸ìì—´ì„ ì´ì–´ ë¶™ì´ê³  `\\xa0`ë¬¸ìë¥¼ ê³µë°±ë¬¸ìë¡œ ë³€ê²½í•œë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì¤„ë°”ê¿ˆ ì—†ëŠ” ê³µë°±(No-Break Space)ë¬¸ìë¥¼ ê³µë°±ë¬¸ìë¡œ ë³€ê²½í•  ìˆ˜ ìˆë‹¤. ì´í›„ í† í¬ë‚˜ì´ì €ë¥¼ ì´ìš©í•´ ì…ë ¥ ë°ì´í„°ë¥¼ í† í°í™”í•œë‹¤. ì´ë•Œ í† í°í™”ëœ ë°ì´í„°ëŠ” í† í°ê³¼ ë ˆì´ë¸”ë¡œ êµ¬ì„±ë¼ ìˆë‹¤. tokenizerí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ í•´ë‹¹ ë¬¸ìì—´ì„ í† í°í™”í•˜ê³ , ê° í† í°ì˜ ì‹œì‘/ë ìœ„ì¹˜ ì •ë³´ë¥¼ offset_mappingìœ¼ë¡œ ë°›ëŠ”ë‹¤. í† í°í™”ëœ ê²°ê³¼ì— ëŒ€í•´ labelsë¥¼ ìƒì„±í•œë‹¤. ê° í† í°ì˜ offsetê°’ì„ í™•ì¸í•´, ì‹œì‘ê³¼ ë ìœ„ì¹˜ê°€ ê°™ë‹¤ë©´ -100(íŒ¨ë”©í† í°)ì„ í• ë‹¹í•œë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset klue (/home/asanobm/.cache/huggingface/datasets/klue/ner/1.0.0/e0fc3bc3de3eb03be2c92d72fd04a60ecc71903f821619cb28ca0e1e29e4233e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4e932153874cc3b6193e0d812b60bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x7fa0eabfa3e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a797779d975e4a9fb8b2652311ec0b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21008 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186fa22f4aff4038bd58fda762181adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'labels'],\n",
      "        num_rows: 21008\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'labels'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n",
      "[12, 12, 12, 2, 3, 3, 3, 3, 3, 12, 2, 3, 12, 12, 12, 12, 2, 3, 3, 3, 3, 12, 12, 12, 2, 3, 3, 3, 3, 12, 12, 12, 8, 9, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n",
      "[[0, 2], [3, 9], [10, 12], [13, 15], [16, 17], [17, 18], [18, 19], [19, 20], [20, 21], [21, 22], [22, 23], [24, 25], [25, 26], [26, 27], [27, 28], [28, 29], [29, 30], [30, 31], [32, 33], [33, 34], [35, 37], [37, 38], [38, 39], [40, 43], [44, 46], [47, 49], [50, 51], [51, 52], [52, 53], [53, 54], [54, 55], [55, 56], [57, 59], [59, 60], [60, 61], [61, 62], [63, 64], [64, 65], [65, 66]]\n",
      "[12, 2, 2, 12, 2, 3, 3, 3, 3, 12, 12, 2, 3, 3, 3, 3, 12, 12, 8, 9, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"klue\", \"ner\")\n",
    "\n",
    "model_name = \"Leo97/KoELECTRA-small-v3-modu-ner\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_data(example, tokenizer):\n",
    "  sentence = \"\".join(example[\"tokens\"]).replace(\"\\xa0\", \" \")\n",
    "  encoded = tokenizer(\n",
    "    sentence,\n",
    "    return_offsets_mapping=True,\n",
    "    add_special_tokens=False,\n",
    "    padding=False,\n",
    "    truncation=False\n",
    "  )\n",
    "\n",
    "  labels = []\n",
    "\n",
    "  for offset in encoded.offset_mapping:\n",
    "    if offset[0] == offset[1]:\n",
    "      labels.append(-100)\n",
    "    else:\n",
    "      labels.append(example[\"ner_tags\"][offset[0]])\n",
    "  \n",
    "  encoded[\"labels\"] = labels\n",
    "  return encoded\n",
    "\n",
    "\n",
    "processed_dataset = dataset.map(\n",
    "  lambda example: preprocess_data(example, tokenizer),\n",
    "  batched=False,\n",
    "  remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "print(processed_dataset)\n",
    "print(dataset[\"train\"][0][\"ner_tags\"])\n",
    "print(processed_dataset[\"train\"][0][\"offset_mapping\"])\n",
    "print(processed_dataset[\"train\"][0][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ê°œì²´ëª… ì¸ì‹ì„ ìœ„í•œ ëª¨ë¸ í•™ìŠµ**\n",
    "\n",
    "`TrainingArguments`ë¥¼ ì´ìš©í•´ í•™ìŠµì— í•„ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•œë‹¤. ì—¬ê¸°ì„œëŠ” ì¶œë ¥ ë””ë ‰í† ë¦¬(output_dir), í‰ê°€ ì „ëµ(evlauation_strategy:ì´ê±° ì´ì œ ì•ˆì“´ëŒ€ìš”.), ë°°ì¹˜ í¬ê¸°(per_device_train_batch_size, per_device_eval_batch_size), í•™ìŠµë¥ (learning_rate), ê°€ì¤‘ì¹˜ ê°ì†Œ(weight_decay), ì—í­ ìˆ˜(num_train_epochs), ì‹œë“œ(seed) ê°’ë“±ì˜ ì˜µì…˜ì„ ì„¤ì •í–ˆë‹¤.\n",
    "\n",
    "íŠ¸ë ˆì´ë„ˆ ê°ì²´ëŠ” í•™ìŠµì‹œí‚¬ ëª¨ë¸, ì„¤ì •í•œ `TrainingArguments`ê°ì²´ í•™ìŠµ ë°ì´í„°ì„¸íŠ¸, í‰ê°€ ë°ì´í„°ì„¸íŠ¸, ë°ì´í„° ì½œë ‰í„°ë¥¼ ì „ë‹¬ í–ˆë‹¤. ë°ì´í„° ì½œë ‰í„°ëŠ” `DatacollatorForTokenClassification` ê°ì²´ë¥¼ ì‚¬ìš©í•˜ê³  í† í¬ë‚˜ì´ì €ì™€ íŒ¨ë”© ì˜µì…˜ì„ ì¶”ê°€í–ˆë‹¤.\n",
    "\n",
    "í•™ìŠµ ê³¼ì •ì—ì„œ íŠ¸ë ˆì´ë„ˆëŠ” ì„¤ì •ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ë°ì´í„°ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ê°±ì‹ í•œë‹¤. ê° ì—í­ë§ˆë‹¤ í‰ê°€ ë°ì´í„°ì„¸íŠ¸ì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ê³ , ìµœì ì˜ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•œë‹¤.\n",
    "\n",
    "> ì±…ì—ëŠ” ì—í­ì„ 5ë²ˆ ì¤¬ë‹¤. ê¶ê¸ˆí•´ì„œ í•œ 10ë²ˆ ì¤˜ë´¤ë‹¤. ê²ë‚˜ê²Œ ì˜¤ë˜ ê±¸ë¦°ë‹¤ ... ã… ã… \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asanobm/miniforge3/envs/ai/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/asanobm/miniforge3/envs/ai/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1650' max='1650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1650/1650 01:15, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.192619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.192404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.205167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.204603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.201046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.205458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.209963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.210582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.209986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.211019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asanobm/miniforge3/envs/ai/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/asanobm/miniforge3/envs/ai/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/asanobm/miniforge3/envs/ai/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/asanobm/miniforge3/envs/ai/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1650, training_loss=0.0021054671930544303, metrics={'train_runtime': 75.2039, 'train_samples_per_second': 2793.473, 'train_steps_per_second': 21.94, 'total_flos': 877012588572192.0, 'train_loss': 0.0021054671930544303, 'epoch': 10.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ê°œì±„ëª… ì¸ì‹ ëª¨ë¸ í•™ìŠµ\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.data.data_collator import DataCollatorForTokenClassification\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"runs/token-classification\",\n",
    "  evaluation_strategy=\"epoch\",\n",
    "  per_device_train_batch_size=64,\n",
    "  per_device_eval_batch_size=64,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=0.01,\n",
    "  num_train_epochs=10,\n",
    "  seed=42,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=processed_dataset[\"train\"],\n",
    "  eval_dataset=processed_dataset[\"validation\"],\n",
    "  data_collator=DataCollatorForTokenClassification(tokenizer, padding=True),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ê°œì²´ëª… ì¸ì‹ ëª¨ë¸ í…ŒìŠ¤íŠ¸**\n",
    "\n",
    "model.evalì„ í˜¸ì¶œí•´ ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜í•œë‹¤. ì´ì œ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ëŠ” ê³ ì •ë˜ê³  ë“œë¡­ì•„ì›ƒê³¼ ê°™ì€ ì •ê·œí™” ê¸°ë²•ì´ ë¹„í™œì„±í™”ë˜ë„ë¡ í•œë‹¤. ì´í›„ ì˜ˆì¸¡í•  í…ìŠ¤íŠ¸ë¥¼ ë„£ì–´ì£¼ë©´ ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤.\n",
    "***ê·¸ëŸ°ë° ê²°ê³¼ë¥¼ ë°›ê¸°ëŠ” ì •ë§ë¡œ ê¹Œë‹¤ë¡œìš´ ì½”ë“œë¥¼ ì‘ì„±í•´ì•¼ í•œë‹¤. ì´ë ¬ë ¤êµ¬ í—ˆê¹…í˜ì´ìŠ¤ ê³µë¶€í•˜ëŠ”ê²Œ ì•„ë‹Œë° ì´ë”´ ì½”ë“œë¥¼ ì‘ì„±í•˜ê²Œ í•˜ëŠ” ì €ìì˜ ì €ì˜ë¥¼ ì´í•´í•  ìˆ˜ ì—†ë‹¤.***\n",
    "\n",
    "**ê°œì²´ëª… ì¸ì‹ì˜ í•œê³„**\n",
    "\n",
    "* **ë°ì´í„° ë¶€ì¡±**: í•™ìŠµ ë°ì´í„°ì— 'ìœ„í‚¤ë¶ìŠ¤'ê°€ ì¡°ì§ìœ¼ë¡œ ë ˆì…ë¥´ë§ëœ ì˜ˆì‹œê°€ ì¶©ë¶„í•˜ì§€ ì•Šê±°ë‚˜ ì „í˜€ ì—†ì„ ìˆ˜ ìˆë‹¤. ì´ëŸ° ê²½ìš° ëª¨ë¸ì´ 'ìœ„í‚¤ë¶ìŠ¤'ë¥¼ ì¡°ì§ìœ¼ë¡œ í•™ìŠµí•˜ì§€ ëª»í•´(?)\n",
    "* **ì»¨í…ìŠ¤íŠ¸ ë¶€ì¡±**: ë¬¸ì¥ì—ì„œ'ìœ„í‚¤ë¶ìŠ¤'ê°€ ì¡°ì§ì„ì„ ëª…í™•íˆ ì•Œ ìˆ˜ ìˆëŠ” ë§¥ë½ì´ ë¶€ì¡±í•  ìˆ˜ ìˆë‹¤.(ì´ê²Œ ì •ë‹µì¸ê±° ê°™ê¸°ë„ í•˜ë‹¤)\n",
    "* **ëª¨ë¸ì˜ í•œê³„**: ëª¨ë¸ì˜ ì„±ëŠ¥ì´ í›„ë‹¬ë ¤ì„œ 'ìœ„í‚¤ë¶ìŠ¤'ë¥¼ ì¡°ì§ìœ¼ë¡œ ì¸ì‹í•˜ì§€ ëª»í•  ìˆ˜ ìˆë‹¤. ìƒˆë¡œìš´ ë‹¨ì–´ì´ê±°ë‚˜ í•™ìŠµ ë°ì´í„°ì—ì„œ ë§ì´ ë“±ì¥í•˜ì§€ ì•Šì„ ê²½ìš° ëª¨ë¸ì´ ì¸ì‹í•˜ì§€ ëª»í•  ìˆ˜ ìˆë‹¤.\n",
    "* **ì„œë¸Œí† í° ì²˜ë¦¬ ë¬¸ì œ**: ì˜ˆì‹œì—ì„œ 'ìœ„í‚¤ë¶ìŠ¤'ëŠ” 'ìœ„í‚¤'ì™€ '##ë¶', '##ìŠ¤'ë¡œ í† í°í™”ëë‹¤. ì´ëŸ° ê²½ìš° ëª¨ë¸ì´ 'ìœ„í‚¤ë¶ìŠ¤'ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì¸ì‹í•˜ì§€ ëª»í•  ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ìœ„í‚¤', 'O'), ('##ë¶', 'O'), ('##ìŠ¤', 'O'), ('##ì˜', 'O'), ('ìœ¤', 'B-PS'), ('##ëŒ€', 'I-PS'), ('##í¬', 'I-PS'), (',', 'O'), ('ê¹€ë™', 'B-PS'), ('##í™”', 'I-PS'), (',', 'O'), ('ì†¡', 'B-PS'), ('##ì¢…', 'I-PS'), ('##ë¯¼', 'I-PS'), ('ê·¸ë¦¬ê³ ', 'O'), ('ì§„', 'B-PS'), ('##í˜„', 'I-PS'), ('##ë‘', 'I-PS'), ('##ëŠ”', 'O'), ('2025', 'B-DT'), ('##ë…„', 'I-DT'), ('ì„œìš¸', 'B-LC'), ('##ì—', 'O'), ('##ì„œ', 'O'), ('2', 'B-TI'), ('##ì‹œê°„', 'I-TI'), ('ë™ì•ˆ', 'O'), ('ì‹ ê°„', 'O'), ('1', 'B-QT'), ('##ê¶Œ', 'I-QT'), ('##ì—', 'O'), ('ê´€í•œ', 'O'), ('ë…¼ì˜', 'O'), ('##ë¥¼', 'O'), ('ì§„í–‰', 'O'), ('##í–ˆ', 'O'), ('##ë‹¤', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# ê°œì²´ëª… ì¸ì‹ ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "text = \"\"\"\n",
    "ìœ„í‚¤ë¶ìŠ¤ì˜ ìœ¤ëŒ€í¬, ê¹€ë™í™”, ì†¡ì¢…ë¯¼ ê·¸ë¦¬ê³  ì§„í˜„ë‘ëŠ” 2025ë…„ ì„œìš¸ì—ì„œ 2ì‹œê°„ ë™ì•ˆ ì‹ ê°„ 1ê¶Œì— ê´€í•œ ë…¼ì˜ë¥¼ ì§„í–‰í–ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "  tokenized = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    add_special_tokens=False,\n",
    "  )\n",
    "\n",
    "  logits = model(**tokenized.to(device)).logits.cpu()\n",
    "\n",
    "predictions = logits.argmax(dim=-1)[0].tolist()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(list(zip(tokens, [labels[i] for i in predictions])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ëƒ¥ ì•„ë¬´ê±°ë‚˜ ë„£ì–´ ë´¤ë‹¤. ê²°ê³¼ê°€ ì•„ì£¼ ì˜ ë‚˜ì™”ë‹¤. ë¡¤ë§íŒŒìŠ¤íƒ€ëŠ” ê°œì²´ëª…ìœ¼ë¡œ ì¸ì‹í•œê²Œ ì•„ë‹ˆë‹¤. ë”êµ°ë‹¤ë‚˜ ì—°ì‹ ë‚´ ì¡°ì°¨ ì œëŒ€ë¡œ ì¸ì‹í•˜ì§€ ëª»í–ˆë‹¤. ì•ˆíƒ€ê¹ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ì—°ì‹ ', 'O'), ('##ë‚´', 'O'), ('##ì—', 'O'), ('ìœ„ì¹˜', 'O'), ('í–ˆ', 'O'), ('##ë˜', 'O'), ('ë¡¤ë§', 'O'), ('##íŒŒ', 'O'), ('##ìŠ¤íƒ€', 'O'), ('##ëŠ”', 'O'), ('í', 'O'), ('##ì ', 'O'), ('##í–ˆ', 'O'), ('##ë‹¤', 'O'), ('.', 'O'), ('ê·¸ë˜ì„œ', 'O'), ('ë‚˜', 'O'), ('##ëŠ”', 'O'), ('ì•„ì§', 'O'), ('##ë„', 'O'), ('ë‹¨', 'O'), ('í•œ', 'B-QT'), ('##ë²ˆ', 'I-QT'), ('##ë„', 'O'), ('ë¡¤ë§', 'O'), ('##íŒŒ', 'O'), ('##ìŠ¤íƒ€', 'O'), ('##ë¥¼', 'O'), ('ë¨¹', 'O'), ('##ì§€', 'O'), ('ëª»', 'O'), ('##í–ˆ', 'O'), ('##ë‹¤', 'O'), ('.', 'O'), ('ì•ˆíƒ€ê¹', 'O'), ('##ì§€ë§Œ', 'O'), ('ê°€ê°€', 'O'), ('##ìš´', 'O'), ('ë¡¤ë§', 'O'), ('##íŒŒ', 'O'), ('##ìŠ¤íƒ€', 'O'), ('##ë¥¼', 'O'), ('ë¨¹', 'O'), ('##ê¸°', 'O'), ('ìœ„í•´ì„œ', 'O'), ('##ëŠ”', 'O'), ('ë©€ë¦¬', 'O'), ('##ê¹Œ', 'O'), ('##ì§€', 'O'), ('ê°€ì•¼', 'O'), ('##í•œë‹¤', 'O'), ('.', 'O'), ('í•˜ì§€', 'O'), ('##ë§Œ', 'O'), ('ë‚˜', 'O'), ('##ëŠ”', 'O'), ('ê·€ì°®', 'O'), ('##ì•„', 'O'), ('##ì„œ', 'O'), ('ê°€ì§€', 'O'), ('ì•Š', 'O'), ('##ì„', 'O'), ('##êº¼', 'O'), ('##ë‹¤', 'O'), ('.', 'O'), ('ì•ˆíƒ€ê¹Œìš´', 'O'), ('í˜„ì‹¤', 'O'), ('##ì´ë‹¤', 'O'), ('.', 'O'), ('ì§€ê¸ˆ', 'O'), ('##ì€', 'O'), ('ìŠ¤íƒ€', 'B-OG'), ('##ë²…', 'I-OG'), ('##ìŠ¤', 'I-OG'), ('##ì—', 'O'), ('##ì„œ', 'O'), ('ì½”ë”©', 'O'), ('##ì§ˆ', 'O'), ('##ì„', 'O'), ('í•˜ê³ ', 'O'), ('ìˆ', 'O'), ('##ë‹¤', 'O'), ('.', 'O'), ('ì•ˆì“°', 'O'), ('##ë˜', 'O'), ('í‚¤ë³´ë“œ', 'O'), ('##ë¥¼', 'O'), ('ì¼', 'O'), ('##ë”ë‹ˆ', 'O'), ('ì†ëª©', 'O'), ('##ì•„ì§€', 'O'), ('##ê°€', 'O'), ('ì•„', 'O'), ('##ì‘', 'O'), ('##ì´', 'O'), ('ë‚¬', 'O'), ('##ë‹¤', 'O'), ('.', 'O'), ('ë˜', 'O'), ('íŒŒìŠ¤', 'O'), ('##ë¥¼', 'O'), ('ë¶™', 'O'), ('##í˜”', 'O'), ('##ë‹¤', 'O'), ('.', 'O'), ('ì˜¤ëŠ˜', 'B-DT'), ('##ì˜', 'O'), ('ì¼ê¸°', 'O'), ('##ëŠ”', 'O'), ('ì—¬ê¸°', 'O'), ('##ê¹Œ', 'O'), ('##ì§€', 'O'), ('ã…‹ã…‹ã…‹', 'O'), ('##ã…‹', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# ê°œì²´ëª… ì¸ì‹ ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "text = \"\"\"\n",
    "ì—°ì‹ ë‚´ì— ìœ„ì¹˜ í–ˆë˜ ë¡¤ë§íŒŒìŠ¤íƒ€ëŠ” íì í–ˆë‹¤.\n",
    "ê·¸ë˜ì„œ ë‚˜ëŠ” ì•„ì§ë„ ë‹¨ í•œë²ˆë„ ë¡¤ë§íŒŒìŠ¤íƒ€ë¥¼ ë¨¹ì§€ ëª»í–ˆë‹¤.\n",
    "ì•ˆíƒ€ê¹ì§€ë§Œ ê°€ê°€ìš´ ë¡¤ë§íŒŒìŠ¤íƒ€ë¥¼ ë¨¹ê¸° ìœ„í•´ì„œëŠ” ë©€ë¦¬ê¹Œì§€ ê°€ì•¼í•œë‹¤.\n",
    "í•˜ì§€ë§Œ ë‚˜ëŠ” ê·€ì°®ì•„ì„œ ê°€ì§€ ì•Šì„êº¼ë‹¤. ì•ˆíƒ€ê¹Œìš´ í˜„ì‹¤ì´ë‹¤.\n",
    "ì§€ê¸ˆì€ ìŠ¤íƒ€ë²…ìŠ¤ì—ì„œ ì½”ë”©ì§ˆì„ í•˜ê³  ìˆë‹¤. ì•ˆì“°ë˜ í‚¤ë³´ë“œë¥¼ ì¼ë”ë‹ˆ ì†ëª©ì•„ì§€ê°€ ì•„ì‘ì´ ë‚¬ë‹¤. ë˜ íŒŒìŠ¤ë¥¼ ë¶™í˜”ë‹¤. \n",
    "ì˜¤ëŠ˜ì˜ ì¼ê¸°ëŠ” ì—¬ê¸°ê¹Œì§€ ã…‹ã…‹ã…‹ã…‹\n",
    "\"\"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "  tokenized = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    add_special_tokens=False,\n",
    "  )\n",
    "\n",
    "  logits = model(**tokenized.to(device)).logits.cpu()\n",
    "\n",
    "predictions = logits.argmax(dim=-1)[0].tolist()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(list(zip(tokens, [labels[i] for i in predictions])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ë˜ì„œ ì´ëŸ° ì €ëŸ° ì •ë³´ë¥¼ ë” ì¤˜ ë´¤ë‹¤. ì—°ì‹ ë‚´ëŠ” ì§€ëª…ì— í•´ë‹¹í•˜ì§€ë§Œ ì—°ì‹ ë‚´ì—­ìœ¼ë¡œ ë³€ê²½í•˜ê³  ì€í‰êµ¬ê¹Œì§€ ë„£ì–´ì£¼ê³  ë§› ì—†ë‹¤ ë¼ê³  \"ê°œì†Œë¦¬\"ë„ ì¢€ë” ë„£ì–´ ì¤¬ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ì„œìš¸', 'B-LC'), ('íŠ¹ë³„ì‹œ', 'B-LC'), ('ì€í‰êµ¬', 'I-LC'), ('##ì˜', 'O'), ('ì—°ì‹ ', 'B-LC'), ('##ë‚´', 'I-LC'), ('##ì—­', 'I-LC'), ('ê·¼ì²˜', 'O'), ('##ì—', 'O'), ('ìœ„ì¹˜', 'O'), ('í–ˆ', 'O'), ('##ë˜', 'O'), ('ë¡¤ë§', 'O'), ('##íŒŒ', 'O'), ('##ìŠ¤íƒ€', 'O'), ('##ëŠ”', 'O'), ('í', 'O'), ('##ì ', 'O'), ('##í–ˆ', 'O'), ('##ë‹¤', 'O'), ('.', 'O'), ('ì™œ', 'O'), ('í', 'O'), ('##ì ', 'O'), ('##í–ˆ', 'O'), ('##ëŠ”ì§€', 'O'), ('##ëŠ”', 'O'), ('ëª¨ë¥´', 'O'), ('##ì§€ë§Œ', 'O'), ('ì•„ë¬´ë˜ë„', 'O'), ('ë§›', 'O'), ('##ì´', 'O'), ('ì—†', 'O'), ('##ì—ˆ', 'O'), ('##ë‚˜', 'O'), ('##ë³´', 'O'), ('##ë‹¤', 'O'), ('.', 'O'), ('ì•„ë‹Œê°€', 'O'), ('(', 'O'), ('?', 'O'), (')', 'O'), ('ë‚˜', 'O'), ('##ëŠ”', 'O'), ('ì•ˆ', 'O'), ('##ë¨¹', 'O'), ('##ì–´', 'O'), ('##ë´', 'O'), ('##ì„œ', 'O'), ('ëª¨ë¥¸ë‹¤', 'O'), ('.', 'O'), ('ê·¸ë˜ì„œ', 'O'), ('ë‚˜', 'O'), ('##ëŠ”', 'O'), ('ì•„ì§', 'O'), ('##ë„', 'O'), ('ë‹¨', 'O'), ('í•œ', 'B-QT'), ('##ë²ˆ', 'I-QT'), ('##ë„', 'O'), ('ë¡¤ë§', 'O'), ('##íŒŒ', 'O'), ('##ìŠ¤íƒ€', 'O'), ('##ë¥¼', 'O'), ('ë¨¹', 'O'), ('##ì§€', 'O'), ('ëª»', 'O'), ('##í–ˆ', 'O'), ('##ë‹¤', 'O'), ('.', 'O'), ('ì•ˆíƒ€ê¹', 'O'), ('##ì§€ë§Œ', 'O'), ('ê°€ê°€', 'O'), ('##ìš´', 'O'), ('ë¡¤ë§', 'O'), ('##íŒŒ', 'O'), ('##ìŠ¤íƒ€', 'O'), ('##ë¥¼', 'O'), ('ë¨¹', 'O'), ('##ê¸°', 'O'), ('ìœ„í•´ì„œ', 'O'), ('##ëŠ”', 'O'), ('ë©€ë¦¬', 'O'), ('##ê¹Œ', 'O'), ('##ì§€', 'O'), ('ê°€ì•¼', 'O'), ('##í•œë‹¤', 'O'), ('.', 'O'), ('í•˜ì§€', 'O'), ('##ë§Œ', 'O'), ('ë‚˜', 'O'), ('##ëŠ”', 'O'), ('ê·€ì°®', 'O'), ('##ì•„', 'O'), ('##ì„œ', 'O'), ('ê°€ì§€', 'O'), ('ì•Š', 'O'), ('##ì„', 'O'), ('##êº¼', 'O'), ('##ë‹¤', 'O'), ('.', 'O'), ('ì•ˆíƒ€ê¹Œìš´', 'O'), ('í˜„ì‹¤', 'O'), ('##ì´ë‹¤', 'O'), ('.', 'O'), ('ì§€ê¸ˆ', 'O'), ('##ì€', 'O'), ('ìŠ¤íƒ€', 'B-OG'), ('##ë²…', 'I-OG'), ('##ìŠ¤', 'I-OG'), ('##ì—', 'O'), ('##ì„œ', 'O'), ('ì½”ë”©', 'O'), ('##ì§ˆ', 'O'), ('##ì„', 'O'), ('í•˜ê³ ', 'O'), ('ìˆ', 'O'), ('##ë‹¤', 'O'), ('.', 'O'), ('ì•ˆì“°', 'O'), ('##ë˜', 'O'), ('í‚¤ë³´ë“œ', 'O'), ('##ë¥¼', 'O'), ('ì¼', 'O'), ('##ë”ë‹ˆ', 'O'), ('ì†ëª©', 'O'), ('##ì•„ì§€', 'O'), ('##ê°€', 'O'), ('ì•„', 'O'), ('##ì‘', 'O'), ('##ì´', 'O'), ('ë‚¬', 'O'), ('##ë‹¤', 'O'), ('.', 'O'), ('ë˜', 'O'), ('íŒŒìŠ¤', 'O'), ('##ë¥¼', 'O'), ('ë¶™', 'O'), ('##í˜”', 'O'), ('##ë‹¤', 'O'), ('.', 'O'), ('ì˜¤ëŠ˜', 'B-DT'), ('##ì˜', 'O'), ('ì¼ê¸°', 'O'), ('##ëŠ”', 'O'), ('ì—¬ê¸°', 'O'), ('##ê¹Œ', 'O'), ('##ì§€', 'O'), ('ã…‹ã…‹ã…‹', 'O'), ('##ã…‹', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# ê°œì²´ëª… ì¸ì‹ ëª¨ë¸ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "text = \"\"\"\n",
    "ì„œìš¸ íŠ¹ë³„ì‹œ ì€í‰êµ¬ì˜ ì—°ì‹ ë‚´ì—­ ê·¼ì²˜ì— ìœ„ì¹˜ í–ˆë˜ ë¡¤ë§íŒŒìŠ¤íƒ€ëŠ” íì í–ˆë‹¤.\n",
    "ì™œ íì í–ˆëŠ”ì§€ëŠ” ëª¨ë¥´ì§€ë§Œ ì•„ë¬´ë˜ë„ ë§›ì´ ì—†ì—ˆë‚˜ë³´ë‹¤. ì•„ë‹Œê°€(?) ë‚˜ëŠ” ì•ˆë¨¹ì–´ë´ì„œ ëª¨ë¥¸ë‹¤.\n",
    "ê·¸ë˜ì„œ ë‚˜ëŠ” ì•„ì§ë„ ë‹¨ í•œë²ˆë„ ë¡¤ë§íŒŒìŠ¤íƒ€ë¥¼ ë¨¹ì§€ ëª»í–ˆë‹¤.\n",
    "ì•ˆíƒ€ê¹ì§€ë§Œ ê°€ê°€ìš´ ë¡¤ë§íŒŒìŠ¤íƒ€ë¥¼ ë¨¹ê¸° ìœ„í•´ì„œëŠ” ë©€ë¦¬ê¹Œì§€ ê°€ì•¼í•œë‹¤.\n",
    "í•˜ì§€ë§Œ ë‚˜ëŠ” ê·€ì°®ì•„ì„œ ê°€ì§€ ì•Šì„êº¼ë‹¤. ì•ˆíƒ€ê¹Œìš´ í˜„ì‹¤ì´ë‹¤.\n",
    "ì§€ê¸ˆì€ ìŠ¤íƒ€ë²…ìŠ¤ì—ì„œ ì½”ë”©ì§ˆì„ í•˜ê³  ìˆë‹¤. ì•ˆì“°ë˜ í‚¤ë³´ë“œë¥¼ ì¼ë”ë‹ˆ ì†ëª©ì•„ì§€ê°€ ì•„ì‘ì´ ë‚¬ë‹¤. ë˜ íŒŒìŠ¤ë¥¼ ë¶™í˜”ë‹¤. \n",
    "ì˜¤ëŠ˜ì˜ ì¼ê¸°ëŠ” ì—¬ê¸°ê¹Œì§€ ã…‹ã…‹ã…‹ã…‹\n",
    "\"\"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "  tokenized = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    add_special_tokens=False,\n",
    "  )\n",
    "\n",
    "  logits = model(**tokenized.to(device)).logits.cpu()\n",
    "\n",
    "predictions = logits.argmax(dim=-1)[0].tolist()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(list(zip(tokens, [labels[i] for i in predictions])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
