{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2. 모델 설정\n",
    "\n",
    "모델 설정은 딥러닝 모델의 성능과 작동 방식에 직접적인 영향을 미치는 부분이다.\n",
    "딥러닝 모델을 학습시키거나 추론할때, 모델의 구조와 하이퍼파라미터를 올바르게 설정하는 것이 중요하다.\n",
    "모델 설정은 모델의 구조, 학습률, 배치 사이즈, 에폭 수, 정규화 방법, 손실 함수, 최적화 알고리즘 등을 포함한다.\n",
    "\n",
    "모델 설정의 이점\n",
    "\n",
    "* **성능 향상**: 적절한 학습률, 드롭아웃 비율, 배치 크기 등을 설정해 일반적인 성능을 개선한다.\n",
    "* **과대 적합 방지**: 드롭아웃과 같은 정규화 기법을 사용해 과대적합을 방지한다.\n",
    "* **하이퍼파라미터 최적화**: 계층 수, 은닉 노드 수, 학습률, 배치 크기 등의 하이퍼파라미터를 조정해 최적의 모델을 찾는다.\n",
    "* **모델 해석**: 모델의 구조와 하이퍼파라미터를 설정해 모델을 해석하고 설명할 수 있다.\n",
    "* **하드웨어 제약 조건**: 모델의 크기와 복잡도를 조정해 하드웨어 제약 조건을 준수한다.\n",
    "\n",
    "\n",
    "다루는 클래스\n",
    "* 2.2.1. PretrainedConfig Class\n",
    "* 2.2.2. ModelConfig Class\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. PretrainedConfig Class\n",
    "\n",
    "`PretrainedConfig` 클래스는 사전 학습된 모델의 설정을 저장하는 클래스이다.\n",
    "`PretrainedConfig` 클래스는 모델의 구조, 하이퍼파라미터, 토크나이저, 입력 크기, 출력 크기 등을 저장한다.\n",
    "`PretrainedConfig` 클래스는 `from_pretrained` 메서드를 사용해 사전 학습된 모델의 설정을 불러오거나 새로운 모델의 설정을 저장한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PretrainedConfig {\n",
      "  \"hidden_size\": 768,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"transformers_version\": \"4.46.2\",\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import PretrainedConfig\n",
    "config = PretrainedConfig(\n",
    "  model_type='bert', # 모델 타입을 지정한다. bert, gpt, .... 등이 온다.\n",
    "  vocab_size=32000, # 모델의 어휘 사전 크기다. 모델이 인식할 수 있는 고유 토큰의 수를 결정한다.\n",
    "  hidden_size=768, # 모델의 *은닉 게층에 있는 노드 수다. 이 값이 클수록 모델의 표현 능력이 높아진다.\n",
    "  num_attention_heads=12, # 모델의 멀티 헤드 어텐션에서 사용되는 어텐션 헤드의 수다.\n",
    "  num_hidden_layers=12, # 모델의 *트랜스포머 계층 수다. 계층 수가 많을수록 모델의 표현 능력이 높아진다.\n",
    "  output_hidden_states=False, # 모델이 모든 은닉 상태를 출력할지를 결정한다.\n",
    "  output_attentions=False, # 모델이 모든 어텐션 값을 출력할지를 결정한다.\n",
    "  return_dict=True, # 모델이 출력을 딕셔너리 형태로 반환할지를 결정한다.\n",
    "  is_encoder_decoder=False, # 모델이 인코더-디코더 모델인지를 결정한다.\n",
    "  is_decoder=False, # 모델이 디코더 모델인지를 결정한다.\n",
    ")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 매개변수를 통해 모델의 구조, 입출력 형태, 작동 방식 등을 정의할 수 있다. 특히 vocab_size, hidden_size, num_attention_heads, num_hidden_layers등은 모델의 크기와 성능에 직접적인 영향을 미치는 매개변수이다.\n",
    "\n",
    "또한, 각 모델별 모델 설정 클래스는 이러한 공통 속성 외에도 모델 아키텍처에 따른 고유한 속성을 추가로 정의하고 있다. hidden_dropout_prob매개변수는 BertConfig 클래스에서 추가로 정의된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PretrainedConfig {\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"transformers_version\": \"4.46.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = PretrainedConfig(\n",
    "  model_type=\"bert\",\n",
    "  vocab_size=30522,\n",
    "  hidden_size=768,\n",
    "  num_attention_heads=12,\n",
    "  num_hidden_layers=12,\n",
    "  intermediate_size=3072,\n",
    "  hidden_act=\"gelu\",\n",
    "  hidden_dropout_prob=0.1,\n",
    "  initializer_range=0.02,\n",
    "  )\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PretrainedConfig Class**를 직접 인스턴스화 하면 model_type과 원하는 하이퍼파라미터 값을 직접 지정해 사전 학습된 설정값이 아닌 완전히 새로운 설정값으로 BERT모델을 구성할 수 있다. 직접 인스턴스화하는 방식은 주로 새로운 모델 아키텍처를 정의하거나 실험적인 설정을 적용할 때 사용된다.\n",
    "\n",
    "설정값이 잘못되면 모델의 성능이 크게 떨어진다. 따라서 일반적으로는 **from_pretrained**메서드를 사용해 검증된 설정값을 사용하는 것이 좋다.\n",
    "\n",
    "PretrainedConfig.from_pretrained() 메서드를 사용하면 사전 학습된 모델의 설정값을 불러올 수 있다. 이 메서드는 사전 학습된 모델의 이름을 인자로 받아 해당 모델의 설정값을 불러온다. 이때, 사전 학습된 모델의 이름은 모델 클래스의 이름과 동일하다. 예를 들어, BERT 모델의 설정값을 불러오려면 'bert'를 인자로 전달하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PretrainedConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"swish\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "config = PretrainedConfig.from_pretrained(\n",
    "  \"google-bert/bert-base-uncased\",\n",
    "  hidden_act=\"swish\",\n",
    "  )\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. ModelConfig Class\n",
    "\n",
    "허깅페이스에서는 각 모델 아키텍처별로 전용 모델 설정 클래스를 제공한다. 이 클래스는 모델의 구조와 하이퍼파라미터를 정의하고, 모델의 설정값을 저장하고 불러오는 기능을 제공한다. 모델 설정 클래스는 PretrainedConfig 클래스를 상속받아 구현되어 있으며, 모델 아키텍처에 따라 다양한 속성을 추가로 정의하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "config = BertConfig(\n",
    "  num_hidden_layers=6,\n",
    "  num_attention_heads=12,\n",
    ")\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
