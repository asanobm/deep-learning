{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. 모델\n",
    "\n",
    "사전 학습된 모델은 대규모 데이터세트에서 일반적인 패턴과 표현을 사전에 학습한 모델이다. 이러한 모델은 이후 특정 목적에 맞게 미세 조정(fine-tuning)하여 사용한다. 사전 학습을 통해 모델이 풍부한 지식을 습득함으로써, 상대적으로 적은 양의 데이터로도 우수한 성능을 달성할 수 있다. 이게 바로 전이 학습(transfer learning)이다.\n",
    "\n",
    "텍스트 분약에서는 BERT, RoBERTa, GPT 등의 모델이 대표적이다. 이 모델은 트랜스포머 아키텍처를 기반으로 셀프 어텐션 매커니즘을 통해 입력 시퀀스의 의존성을 효과적으로 모델링한다. 이러한 모델은 다양한 자연어 처리 태스크에 대해 뛰어난 성능을 보이고 있으며, 특히 토큰 수준의 임베딩을 통해 단어의 의미를 효과적으로 표현한다.\n",
    "\n",
    "이미지 분야에서는 합성곱 신경망(CNN) 기반의 모델인 ResNet, EfficientNet 등이 대표적이다. 이러한 모델은 이미지의 특징을 효과적으로 추출하고, 이를 기반으로 이미지 분류, 객체 검출, 분할 등의 작업을 수행한다. 최근에는 비전 트랜스포머 (Vision Transformer, ViT) 와 같이 트랜스포머 기반 모델도 주목받고 있다. 사전 학습된 이미지 모델은 물체인식, 세그먼테이션, 이미지 켭셔닝 등의 과제에 활용된다.\n",
    "\n",
    "텍스트와 이미지를 동시에 처리하는 멀티모달 분야에서도 사전 학습된 모델이 등장하고 있다. CLIP, ALIGN, Flamingo 등이 대표적이다. 이러한 모델들은 대규모 이미지-텍스트 페어 데이터에서 사전 학습되어 이미지와 텍스트의 상호 관계를 효과적으로 학습한다.\n",
    "\n",
    "사전 학습된 모델들은 주로 트랜스포머나 합성곱(Convolution)과 같은 딥러닝 아키텍처를 기반으로 구성된다. 사전 학습 후에는 목적에 맞게 전이 학습되어 해당 분야에 특화된 모델로 사용된다. 이러한 모델들은 다양한 자연어 처리, 이미지 처리, 멀티모달 분야에서 활용되며, 최신 연구나 실무에서도 널리 사용되고 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1. PreTrainedModel Class\n",
    "\n",
    "PretrainedModel 클래스는 사전 학습된 모델을 효율적으로 불러오고 사용할 수 있다. PreTrainedModel 클래스는 다양한 사전 학습된 모델을 위한 기반 클래스로, 모델 불러오기, 전이 학습, 모델 저장 등의 기능을 제공한다. \n",
    "\n",
    "**PreTrainedModel 클래스의 주요 메서드**\n",
    "\n",
    "* **from_pretrained()**: 사전 학습된 모델을 불러온다.\n",
    "* **save_pretrained()**: 모델을 저장한다.\n",
    "* **forward()**: 모델의 순전파를 수행한다.\n",
    "* **train()**: 모델을 학습 모드로 전환한다.\n",
    "* **eval()**: 모델을 평가 모드로 전환한다.\n",
    "* **to()**: 모델을 지정한 장치로 이동한다.\n",
    "* **parameters()**: 모델의 파라미터를 반환한다.\n",
    "* **named_parameters()**: 모델의 이름과 파라미터를 반환한다.\n",
    "* **state_dict()**: 모델의 상태 사전을 반환한다.\n",
    "* **load_state_dict()**: 모델의 상태 사전을 불러온다.\n",
    "* **zero_grad()**: 모델의 그래디언트를 초기화한다.\n",
    "* **backward()**: 모델의 역전파를 수행한다.\n",
    "* **optimizer.step()**: 옵티마이저를 업데이트한다.\n",
    "* **optimizer.zero_grad()**: 옵티마이저의 그래디언트를 초기화한다.\n",
    "* **optimizer.state_dict()**: 옵티마이저의 상태 사전을 반환한다.\n",
    "* **optimizer.load_state_dict()**: 옵티마이저의 상태 사전을 불러온다.\n",
    "* **scheduler.step()**: 스케줄러를 업데이트한다.\n",
    "* **scheduler.state_dict()**: 스케줄러의 상태 사전을 반환한다.\n",
    "* **scheduler.load_state_dict()**: 스케줄러의 상태 사전을 불러온다.\n",
    "* **get_input_embeddings()**: 입력 임베딩을 반환한다.\n",
    "* **get_output_embeddings()**: 출력 임베딩을 반환한다.\n",
    "* **resize_token_embeddings()**: 토큰 임베딩을 조정한다.\n",
    "* **tie_weights()**: 가중치를 공유한다.\n",
    "* **init_weights()**: 가중치를 초기화한다.\n",
    "* **prepare_inputs_for_generation()**: 생성을 위한 입력을 준비한다.\n",
    "* **adjust_logits_during_generation()**: 생성 중 로짓을 조정한다.\n",
    "* **prepare_decoder_input_ids_from_labels()**: 레이블로부터 디코더 입력을 준비한다.\n",
    "* **prepare_labels()**: 레이블을 준비한다.\n",
    "* **compute_loss()**: 손실을 계산한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2. ModelModel Class\n",
    "\n",
    "사전 학습된 언어 모델을 효과적으로 활용하려면 적절한 전처리와 모델이 필요하다. 자연어 처리에서는 토크나이저와 사전 학습된 모델이 필요하다. 토크나이저를 통해 원시 텍스트데이터를 모델이 이해할 수 있는 형태로 전처리하고, 사전 학습된 모델에서 순전파를 수행해 출력을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 5672, 2033, 2011, 2151, 3793, 2017, 1005, 1040, 2066, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([1, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "input = tokenizer(text, return_tensors=\"pt\")\n",
    "print(input)\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "output = model(**input)\n",
    "print(output.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BertTokenizer, BertModel을 사용해 텍스트를 BERT모델의 입력으로 전처리하고, 모델의 출력을 생성하는 전체 과정이다. \n",
    "  * return_tensors='pt'로 설정하면 토크나이저의 출력을 파이토치 텐서로 반환한다. \n",
    "  * 출력된 input은 모델에 입력될 텐서들의 딕셔너리이다.\n",
    "  * BertModel을 불러오고 인스턴스화한 후, forward() 메서드를 호출해 순전파를 수행한다. **input은 토큰화된 입력 딕셔러리를 모델에 전달하는 방식이다. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 예시에서는 Bert모델에 드롭아웃계층과 선형계층을 추가해 텍스트 분류기를 구성한다. 이렇게 구축된 모델은 특정 텍스트 분류 데이터세트에 대해 미세조정 될 수 있다. 미세 조정 과정에서BERT의 가중치도 함께 업데이트되어 해당 과제에 특화된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertModel 다운스트림 예시\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "  def __init__(self, num_classes):\n",
    "    super().__init__()\n",
    "    self.bert=BertModel.from_pretrained('bert-base-uncased')\n",
    "    self.dropout=nn.Dropout(0.3)\n",
    "    self.classifier=nn.Linear(self.bert.config.cross_attention_hidden_size, num_classes)\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    pooled_output = output.pooler_output\n",
    "    dropout_output = self.dropout(pooled_output)\n",
    "    logits = self.classifier(dropout_output)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 예제는 BertForMaskedLM클래스를 활용해 마스크된 단어를 예측한다. 이 예제에서는 BertForMaskedLM 클래스를 사용해 문장에서 마스크된 단어를 예측한다. 먼저 BERT토크나이저와 BertForMaskedLM 모델을 불러온 후, 마스크 토큰이 포함된 문장을 생성한다. 이후 토크나이저를 사용해 문장을 토큰화하고, 마스크 토큰의 인덱스를 찾는다. 마스크 토큰의 인덱스를 모델에 입력하고, 모델의 출력을 확인한다. 출력은 각 토큰의 로짓이며, 마스크 토큰의 로짓이 가장 높은 토큰이 예측 결과이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was learning about tokenizers.\n",
      "I am learning about tokenizers.\n",
      "I remember learning about tokenizers.\n",
      "I started learning about tokenizers.\n",
      "I keep learning about tokenizers.\n",
      "I kept learning about tokenizers.\n",
      "I like learning about tokenizers.\n",
      "I liked learning about tokenizers.\n",
      "I loved learning about tokenizers.\n",
      "I love learning about tokenizers.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "\n",
    "text=f\"I {tokenizer.mask_token} learning about tokenizers.\"\n",
    "input = tokenizer(text, return_tensors=\"pt\")\n",
    "output = model(**input)\n",
    "\n",
    "mask_index = torch.where(input[\"input_ids\"][0]==tokenizer.mask_token_id)\n",
    "softmax = F.softmax(output.logits, dim=-1)\n",
    "mask_word = softmax[0, mask_index, :]\n",
    "top10 = torch.topk(mask_word, 10, dim=1)[1][0]\n",
    "\n",
    "for token in top10:\n",
    "  word = tokenizer.decode([token])\n",
    "  new_sentence = text.replace(tokenizer.mask_token, word)\n",
    "  print(new_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
