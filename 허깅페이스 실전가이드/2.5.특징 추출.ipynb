{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5. 특징 추출\n",
    "\n",
    "특징 추출은 이미지나 오디오와 같은 원시 데이터에서 딥러닝 모델의 입력으로 사용될 수 있는 특징(feature)을 추출하는 역할을 한다.\n",
    "**특징이란 데이터에서 유용한 정보를 나타내는 부분이나 속성**을 말한다. \n",
    "\n",
    "* 이미지에서는 가장자리(Edge), 질감(Texture), 색상(Color) 등이 특징이 될 수 있다.\n",
    "* 오디오에서는 주파수(Frequency), 진폭(Magnitude), 음량(Amplitude) 등이 특징이 될 수 있다.\n",
    "\n",
    "이미지 분야에서는 이미지를 픽셀 값을 표현하는 대신 CNN 등의 딥러닝 모델을 활용해 고수준의 의미 있는 특징을 추출할 수 있다. ResNet과 같은 사전 학습된 모델의 중간 계층 출력이나 마지막 계층의 출력을 활용하면 이미지의 시각적 특징을 효과적으로 추출할 수 있다.\n",
    "\n",
    "오디오 분야에서는 원시 오디오 파형(Waveform)에서 직접 특징을 추출하기 어려우므로 멜스펙트로그램(Mel Spectrogram)과 같은 중간 표현(Intermediate Representation)을 활용해 특징을 추출한다. 이러한 특징 추출 방법을 통해 오디오의 주파수, 진폭, 음량 등의 특징을 추출할 수 있다.\n",
    "\n",
    "**특징 추출 주요 기능**\n",
    "\n",
    "* **이미지/오디오 전처리**: 입력 데이터에 대한 전처리 기능을 제공한다. 이미지의 경우 크기 조절, 정규화 등의 작업을 오디오의 경우 리샘플링, 패딩 등의 작업을 수행한다.\n",
    "* **특징 추출**: 전처리된 데이터에서 CNN, 오디오 모델 등을 활용해 특징을 추출한다. 이미지의 경우 CNN출력, 오디오의 경우 멜 스펙트로그램 등이 특징 벡터로 사용된다.\n",
    "* **특징 후처리**: 추출된 특징에 대한 후처리 기능을 제공한다. 이를 통해 모델 입력에 적합한 형태로 특징 벡터를 변환할 수 있다.\n",
    "* **일관된 인터페이스**: 이미지, 오디오 등 다양한 모달리터에 대해 일관된 API를 제공해 코드 재사용성을 높인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1. ImageFeatureExtractor 클래스\n",
    "ImageFeatureExtractor는 허깅페이스에서 이미지 데이터를 전처리하고 시각적 특징을 추출하는 데 사용되는 클래스다.\n",
    "\n",
    "이미지 크기 조절, 정규화, 데이터 증강 등의 전처리 작업을 수행한다. 합성곱 신경망, 비전 트랜스포머 등의 모델을 활용해 이미지의 시각적 특징을 추출한다. 추출된 특징 벡터에 대한 후처리 작업을 수행해 모델 입력에 적합한 형태로 변환한다. \n",
    "\n",
    "CLIPFeatureExtractor.from_pretrained()를 통해 \"openai/clip-vit-base-patch32\" 모델의 특징 추출기를 불러온다. feature_extractor()에 이미지를 전달하면서 전처리 옵션을 지정한다. 여기서는 이미지를 512*512크기로 변경하고 중심 자르기를 적용한다. \n",
    "\n",
    "출력된 inputs는 딕셔너리 형태이며, pixel_values에는 전처리된 이미지 텐서가 포함된다. 이렇게 추출된 특징 벡터(pixel_values)를 지정해 파이토치 텐서 형태로 출력한다.\n",
    "\n",
    "출력된 inputs는 딕셔너리 형태이며, ㅔ는ㅇㄹㄴㅇㄹㅁㅇㄹ 전처리된 이미지 텐서가 포함된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512, 512])\n",
      "{'pixel_values': tensor([[[[ 0.5143,  0.5727,  0.6603,  ..., -0.0259, -0.1280, -0.0696],\n",
      "          [ 0.5435,  0.6165,  0.6603,  ...,  0.0325,  0.0179, -0.0113],\n",
      "          [ 0.5435,  0.5873,  0.6019,  ...,  0.0179,  0.0179,  0.1347],\n",
      "          ...,\n",
      "          [ 1.8135,  1.8573,  1.9157,  ...,  1.4778,  1.4486,  1.5654],\n",
      "          [ 1.9157,  1.8573,  1.8865,  ...,  1.2588,  1.1712,  1.6238],\n",
      "          [ 1.8719,  1.8573,  1.9011,  ...,  1.1712,  1.4486,  1.5654]],\n",
      "\n",
      "         [[-1.4069, -1.3469, -1.2568,  ..., -1.4970, -1.5870, -1.4970],\n",
      "          [-1.3769, -1.2718, -1.2268,  ..., -1.4519, -1.4369, -1.4519],\n",
      "          [-1.3469, -1.2718, -1.2418,  ..., -1.5120, -1.4669, -1.3769],\n",
      "          ...,\n",
      "          [ 0.0789,  0.0939,  0.1389,  ..., -0.6565, -0.6565, -0.5665],\n",
      "          [ 0.1689,  0.1089,  0.0789,  ..., -0.8816, -0.9117, -0.4164],\n",
      "          [ 0.0939,  0.0789,  0.1239,  ..., -0.9117, -0.5965, -0.4464]],\n",
      "\n",
      "         [[-0.6839, -0.5275, -0.3426,  ..., -0.8545, -0.8830, -0.8972],\n",
      "          [-0.4279, -0.4137, -0.4137,  ..., -0.8972, -0.8688, -0.7977],\n",
      "          [-0.4279, -0.4422, -0.4564,  ..., -0.8972, -0.8403, -0.6981],\n",
      "          ...,\n",
      "          [ 1.6340,  1.5771,  1.6482,  ...,  0.8803,  0.6528,  0.7523],\n",
      "          [ 1.6340,  1.5913,  1.7193,  ...,  0.9656,  0.5248,  0.7808],\n",
      "          [ 1.7193,  1.6340,  1.6055,  ...,  0.8661,  0.8661,  0.9941]]]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asanobm/miniforge3/envs/ai/lib/python3.11/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import CLIPFeatureExtractor\n",
    "\n",
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "image = dataset[\"test\"]['image'][0]\n",
    "\n",
    "feature_extractor = CLIPFeatureExtractor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "inputs = feature_extractor(\n",
    "  images=image,\n",
    "  do_resize=True,\n",
    "  size=512,\n",
    "  do_center_crop=True,\n",
    "  crop_size=512,\n",
    "  return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "print(inputs[\"pixel_values\"].shape)\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
