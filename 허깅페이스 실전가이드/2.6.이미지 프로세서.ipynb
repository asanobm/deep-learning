{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. 이미지 프로세서\n",
    "\n",
    "이미지 프로세서는 이미지 데이터를 딥러닝 모델에 입력하기 전에 필요한 전처리 작업을 수행하는 클래스다. 이 클래스는 이미지 데이터를 모델이 이해할 수 있는 형태로 변환하고, 모델 추론에 적합한 크기와 형식으로 조정한다.\n",
    "\n",
    "이미지 프로세서는 텍스트 데이터를 전처리하는 토크나이저와 유사한 역할을 한다. 토크나이저가 텍스트를 토큰 시퀀스로 변환하는 것처럼, 이미지 프로세서는 이미지를 모델 입력에 적합한 형태로 전처리한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1. ImageProcessor 클래스\n",
    "\n",
    "ImageProcessor는 이미지 데이터를 전처리하고 딥러닝 모델의 입력으로 변환하는 역할을 하는 허깅페이스의 핵심 클래스다. 이 클래스는 이미지 전처리(크기 조절, 중심 자르기, 패딩, 정규화)를 수행하며, 추가로 회전, 반전 등의 데이터 증강(augmentation)을 적용할 수 있다.\n",
    "\n",
    "ImageProcessor와 ImageFeatureExtractor 클래스는 모두 이미지 데이터를 딥러닝 모델에 사용하기 위해 전처리하는 역할을 하지만, 몇가지 중요한 차이점이 있다. ImageProcessor는 이미지 전처리와 모델 입력 준비를 주 목적으로 수행되지만 ImageFeatureExtractor는 이미지로부터 의미 있는 특징 벡터를 추출하는 것이 주 목적이다.\n",
    "\n",
    "ImageProcessor는 전처리된 이미지 텐서를 출력하며, 모델의 직접적인 입력으로 사용된다. 반면에 ImageFeatureExtractor는 이미지의 특징 벡터를 다운스트림 작업을 위한 중간 표현으로 사용된다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "{'pixel_values': tensor([[[[ 0.5873,  0.5873,  0.6165,  ...,  0.0617,  0.0471, -0.0259],\n",
      "          [ 0.5727,  0.5727,  0.6603,  ...,  0.1201,  0.0763,  0.0909],\n",
      "          [ 0.5873,  0.5435,  0.6165,  ...,  0.0325,  0.1201,  0.0617],\n",
      "          ...,\n",
      "          [ 1.8719,  1.8573,  1.8719,  ...,  1.3902,  1.4340,  1.4194],\n",
      "          [ 1.8281,  1.8719,  1.8427,  ...,  1.4486,  1.4340,  1.5070],\n",
      "          [ 1.8573,  1.9011,  1.8281,  ...,  1.3756,  1.3610,  1.4486]],\n",
      "\n",
      "         [[-1.3169, -1.3019, -1.3169,  ..., -1.4970, -1.4369, -1.4820],\n",
      "          [-1.2418, -1.2718, -1.2268,  ..., -1.4369, -1.4669, -1.4519],\n",
      "          [-1.2568, -1.3169, -1.2268,  ..., -1.4669, -1.4069, -1.4519],\n",
      "          ...,\n",
      "          [ 0.1239,  0.1089,  0.1239,  ..., -0.7016, -0.6865, -0.6865],\n",
      "          [ 0.0789,  0.0939,  0.0488,  ..., -0.6565, -0.6865, -0.6115],\n",
      "          [ 0.0939,  0.1089,  0.0038,  ..., -0.7766, -0.7316, -0.6115]],\n",
      "\n",
      "         [[-0.4848, -0.4137, -0.3853,  ..., -0.9541, -0.8545, -0.8545],\n",
      "          [-0.4137, -0.4706, -0.3711,  ..., -0.8119, -0.8545, -0.7834],\n",
      "          [-0.3284, -0.4422, -0.3853,  ..., -0.8688, -0.8119, -0.8830],\n",
      "          ...,\n",
      "          [ 1.5771,  1.6482,  1.6340,  ...,  0.9088,  0.9514,  0.8945],\n",
      "          [ 1.6198,  1.6055,  1.6055,  ...,  0.8661,  0.8092,  0.7950],\n",
      "          [ 1.6624,  1.6766,  1.5487,  ...,  0.7950,  0.8661,  0.8519]]]])}\n"
     ]
    }
   ],
   "source": [
    "# 이미지 전처리를 수행하는 예제\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import CLIPImageProcessor\n",
    "\n",
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "pixel_values = image_processor(\n",
    "  images=image,\n",
    "  image_mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "  image_std=[0.26862954, 0.26130258, 0.27577711],\n",
    "  do_convert_rgb=True,\n",
    "  return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(pixel_values[\"pixel_values\"].shape)\n",
    "print(pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
