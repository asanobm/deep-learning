{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. 요약문 생성: BART\n",
    "\n",
    "요약문 생성(summary generation)은 텍스트의 중요한 내용을 간결하게 요약하는 작업이다. 이번 노트북에서는 BART 모델을 사용하여 요약문을 생성하는 방법을 알아보자. BART는 Bidirectional and Auto-Regressive Transformers의 약자로, 양방향과 자기회귀 모델을 결합한 모델이다. BART는 텍스트 생성, 요약, 번역 등 다양한 자연어 처리 태스크에 사용할 수 있다.\n",
    "\n",
    "효과적인 요약을 위해서는 원문 텍스트의 의미와 맥락을 정확히 파악하고, 문서 전체에서 중요하고 관련성이 높은 정보를 선별할 수 있어야 한다. 또한 선별된 정보를 압축하고 재구성해 자연스러운 문장으로 생성할 수 있는 능력과 문서의 주제, 목적, 대상 독자 등 다양한 요소를 고려할 수 있어야 한다.\n",
    "\n",
    "텍스트 요약에는 크게 2가지 방식이 있다. **추상적 요약(Abstractivate summarization) 방식**은 원문의 내용을 이해하고 해석한 후 새로운 문장을 생성하여 요약하는 방식이다. 반면 **추출적 요약(Extractive summarization) 방식**은 원문에서 중요한 문장이나 단어구를 추출하여 요약하는 방식이다. BART는 추상적 요약 방식을 사용한다.\n",
    "\n",
    "추상적 요약과 추출적 요약 각각의 장단점이 있기 때문에, 실제 응용 분야에 따라 적절한 방식을 선택하는 것이 중요하다. 가령 추상적 요약은 뉴스 요약, 과학 논문 요약등 높은 수준의 자연어 이해와 생성 능력이 요구되는 분야에 적합하다. 원문의 의미를 정확히 파악하고 재구성해야 하므로 전문적인 지식이 필요한 경우가 많다.\n",
    "\n",
    "추출적 요약은 특허 문서, 법률 문서, 회의록 등 길고 반복적인 텍스트에서 중요한 문장을 추출하는 데 유용하다. 중요 정보를 놓치지 않고 효율적으로 요약할 수 있지만, 생성된 요약문이 부자연스러울 수 있다. 따라서 요약 대상과 목적에 따라 적절한 방식을 선택하는 것이 중요하다. 높은 수준의 이해와 생성이 필요하다면 추상적 요약을, 중요 정보 추출에 초점을 둔다면 추출적 요약을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. BART 모델\n",
    "\n",
    "BART(Bidirectional and Auto-Regressive Transformers)는 메타의 FAIR(Facebook AI Research)에서 개발한 언어 모델로 트랜스포머 기반의 인코더-디코더 아키텍처를 갖춘 **시퀀스-투-시퀀스(Sequence-to-Sequence) 모델**이다. BART는 양항향 컨텍스트를 활용해 언어 이해 및 생성 성능이 뛰어나다.\n",
    "\n",
    "BART의 핵심 구조는 인코더와 디코더로 이루어져 있다. 인코더는 입력 텍스트를 인코딩해 문맥 정보를 숫자 벡터로 표현하는 역할을 한다. BART는 양방향 인코더를 사용하므로 각 단어가 문장 전체의 좌우 컨텍스트를 모두 참조할 수 있다.\n",
    "\n",
    "디코더는 인코더에서 생성된 벡터를 입력받아 순차적으로 출력 텍서트를 생성한다. 디코더는 자기회귀(Auto-regressive)방식을 사용해 이전에 생성된 단어를 참조하며 다음 단어를 예측한다. 이 과정에서 자연스러운 문장 구조와 의미 전달이 간으해진다. 디코더는 기계 번역, 요약, 질의응답 등 다양한 자연어 생성 과제에 활용된다. \n",
    "\n",
    "BART의 인코더와 디코더는 상호작용을 통해 입력 텍스트에 대한 이해가 높다. BART는 사전 학습 과정에서 노이즈 제거 오토인코더(Denoising autoencoder)방식으로 학습한다. 이러한 방식은 입력 문장에 임의로 노이즈를 추가한 후 원래 문장을 복원하도록 학습하는 것이다. 노이즈가 추가된 텍스트를 인코더에 입력하고, 원본 텍스트를 디코더의 출력으로 생성하도록 한다.\n",
    "\n",
    "BART의 학습 방식은 BERT가 마스킹을 활용하는 것과 유사한 측면이 있으며, 문장 구조와 의미를 보존하면서 다양한 변형을 학습할 수 있다. BART의 사전 학습 전략은 입력 문장에 큰 제약 없이 노이즈 기법을 적용할 수 있으므로, 더욱 풍부한 언어적 지식을 습득할 수 있게 한다. 또한 인코더를 사용함으로써 양방향 문맥 정보를 반영할 수 있다. 동시에 디코더를 사용함으로써BERT보다 문장 생성 능력이 뛰어나다.\n",
    "\n",
    "**BART 사전 학습 노이즈 기법**\n",
    "\n",
    "* **텍스트 채우기(Text Infilling)**: 입력 텍스트에서 일부 토큰을[MASK] 토큰으로 대체하고, 모델이 문맥을 참조해 마스크된 부분을 복원하게 한다. 이를 통해 문맥 이해 및 생성 능력을 기를 수 있다.\n",
    "* **문장 순열(Sentence permutation)**: 입력 문서 내 문장 순서를 무작위로 섞어서 원래 문장 순서대로 복원하도록 학습한다. 이렇게 함으로써 전체 문맥을 파악하는 능력을 기를 수 있다.(정말?)\n",
    "* **문서 회전(Document rotation)**: 문장 순서는 유지한 채 문서의 시작과 끝 지점을 무작위로 변경해, 모델이 원래 문서 순서로 복원하도록 한다. 이를 통해 문서 구조 이해 능력을 기를 수 있다. 오호...\n",
    "* **토큰 삭제(Token deletion)**: 입력 텍스트에서 랜덤한 위치의 토큰을 삭제하고, 모델이 삭제된 토큰의 위치와 내용을 유추하도록 한다. 이를 통해 문맥 이해 및 누락된 정보 복원 능력을 기를 수 있다.\n",
    "* **토큰 마스킹(Token masking)**: BERT의 MLM(Masked Language Model)과 유사하게 입력 텍스트에서 일부 토큰을 마스킹하고, 모델이 마스킹된 토큰을 복원하도록 학습한다. 이를 통해 문맥 이해 및 노이즈 처리 능력을 기를 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. BartTokenizer (BERT가 아니다)\n",
    "\n",
    "BART는 BPE 토크나이저를 사용한다. BPE는 가장 빈번한 바이트 페어를 병합해 새로운 토큰을 만드는 방식으로, 낮은 계산 복잡도와 높은 처리 속도를 제공한다. 예를 들어 \"unaffable\"이라는 단어는 하나의 토큰으로 간주하여 ['u', 'n', 'a', 'f', 'f', 'a', 'b', 'l', 'e']로 분리된다. 이러한 토큰화 방식은 단어의 부분을 유지하면서 언어의 다양성을 효과적으로 처리할 수 있다.\n",
    "\n",
    "첫 번째 단계에서 'a'와'f'가 가장 빈번하므로 'af'토큰을 만들어 ['u','n','af','f','a','b','l','e']로 분리한다. 이 과정을 반복하면 'unaffable'은 ['u','n','af','f','a','b','l','e']로 토큰화된다. BPE 토크나이저는 단어의 부분을 유지하면서 언어의 다양성을 효과적으로 처리할 수 있다.\n",
    "\n",
    "`BartTokenizerFast`는 러스트로 구현된 빠른 토크나이저로 파이썬 기반의 BartTokenizer에 비해 훨씬 빠른 속도로 작동한다. 정밀한 토큰화와 높은 효율성을 제공한다. 빠른 토크나이저는 또한 보다 정교한 에러 처리가 가능하며, 다중 스레딩을 지원해 성능을 향상시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [15085, 264, 281, 283, 9698, 26200, 24224, 16935, 21763, 19061, 15931, 262], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# BART 토크나이저를 이용한 토큰화\n",
    "import json as JSON\n",
    "from transformers import BartTokenizerFast\n",
    "\n",
    "tokenizer = BartTokenizerFast.from_pretrained('gogamza/kobart-base-v2')\n",
    "\n",
    "text = \"BART는 요약 모델을 학습하기에 적합하다?\"\n",
    "\n",
    "# 토큰화\n",
    "encoded = tokenizer(text)\n",
    "print(encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. BartModel\n",
    "\n",
    "BART는 텍스트 요약과 같은 조건부 생성(Conditional Generation) 과제를 수행하기 위해 디코더의 출력 계층에 언어 모델 헤드(Language model head)를 추가한 BartForconditionalGeneration 모델을 사용한다. 이 모델은 BartModel에서 디코더의 출력 계층에 언어 모델 헤드가 추가된 구조다. 임베딩 계층, 인코더 계층, 디코더 계층은 사전 학습된 가중치를 그대로 사용하지만, 언어모델 헤드는 무작위로 초기화된 가중치를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BART는 인코더와 디코더가 동일한 임데딩 계층을 공유(shard)하는 구조를 가지고 있다. 공유된 임베딩 계층은 인코더와 디코더 간의 연결을 강화한다. 이 공유된 임베딩 계층은 모델이 입력 데이터의 의미를 더 잘 학습할 수 있도록 해주며, 모델의 성능 향상에도 기여한다.\n",
    "인코더(encoder)는 양방향 트랜스포머 구조로 입력 텍스트를 받아 멀티헤드 셀프 어텐션과 순방향 신경망을 거쳐 문맥 정보를 인코딩한다. gogamzak/kobart-base-v2 모델의 경우 총 6개의 인코더 계층으로 구성되어 있다.\n",
    "\n",
    "양방향 트랜스포머 구조를 사용하는 인코더는 입력 시퀀스의 모든 위치에 대한 표현(Representation)을 생성할 수 있다.이를 통해 전체 문맥을 효과적으로 캡처할 수 있다. 또한 멀티 헤드 셀프 어텐션 메커니즘을 입력의 다양한 위치 간 의존성을 모델링해 의미 있는 특징을 추출할 수 있다. 순방향 신경망 계층은 이렇게 추출된 특징을 더 높은 수준의 표현으로 매핑하는 역할을 한다.\n",
    "\n",
    "디코더(decoder)는 자기회귀 트랜스포머 구조를 가지고 있다. 디코더는 셀프 어텐션, 인코더-디코더 교차 어텐션, 그리고 순방향 신경망으로 이루어져 있다. 디코더는 이전 출려과 인코더의 출력을 참조해 차례대로 출력값을 생성한다.\n",
    "\n",
    "자귀회귀 구조를 사용하는 디코너는 현재 시점의 출력을 생성할 때 이전 시점의 출력들만 참조할 수 있다. 이를 통해 순차적으로 토큰을 생성할 수 있다. 셀프 어텐션 매커니즘의은 이전에 생성된 출력들 간의 의존성을 모델링한다. 인코더-디코더 교차 어텐션은 인코더의 출력과 디코더의 현재 출력 간의 관계를 캡처한다. 순방향 신경망 계층은 이렇게 통합된 정보를 바탕으로 최종 출력을 생성한다.\n",
    "\n",
    "**언어 모델 헤드(lm_head)**는 디코더의 최종 출력을 받아 단어 임베딩 벡터와 선형 변환을 거쳐 단어 사전의 크기에 해당하는 로짓 값들을 출력한다. 이 로짓 값들은 각 단어가 다음 토큰으로 예측될 확률의 로그값을 의미한다. 모델은 이 로짓 값들에 활성화 함수를 적용해 각 단어가 다음 토큰으로 예측될 확률의 로그 값을 의미한다. 모델은 이 로짓 값들에 활성화 함수를 적용해 각 단어의 확률 분포를 얻는다. 그리고 정답 토큰과 교차 엔트로피 손실을 계산해 모델을 학습시킨다. \n",
    "\n",
    "BART모델은 [BOS]토큰으로 시작해 묹아 생성을 진행한다. 각 시점에서 언어 모델 헤드가 출력한 확률 분포에서 가장 높은 확률을 가진 토큰을 선택해 출력한다. 이 과정을[EOS]토큰이 출력될 때까지 반복한다. [EOS] 토큰이 출력되면 문장 생성이 종료된다. 하지만 이렇게 매 시점 가장 높은 확률의 토큰을 참욕적(Greedy)으로 선택하는 방식은 부적절한 문장을 생성할 수 있다. 따라서 BART는 빔 서치(Beam Search)나 샘플링(Sampling)과 같은 디코딩 전략을 사용해 다양한 문장을 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "└ shared\n",
      "└ encoder\n",
      "│  └ embed_tokens\n",
      "│  └ embed_positions\n",
      "│  └ layers\n",
      "│  │  └ 0\n",
      "│  │  └ 1\n",
      "│  │  └ 2\n",
      "│  │  └ 3\n",
      "│  │  └ 4\n",
      "│  │  └ 5\n",
      "│  └ layernorm_embedding\n",
      "└ decoder\n",
      "│  └ embed_tokens\n",
      "│  └ embed_positions\n",
      "│  └ layers\n",
      "│  │  └ 0\n",
      "│  │  └ 1\n",
      "│  │  └ 2\n",
      "│  │  └ 3\n",
      "│  │  └ 4\n",
      "│  │  └ 5\n",
      "│  └ layernorm_embedding\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "# BartModel 구조\n",
    "\n",
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-base-v2')\n",
    "\n",
    "for main_name, main_module in model.named_children():\n",
    "    print(main_name)\n",
    "    for sub_name, sub_module in main_module.named_children():\n",
    "        print(\"└\", sub_name)\n",
    "        for ssub_name, ssub_module in sub_module.named_children():\n",
    "            print(\"│  └\", ssub_name)\n",
    "            for sssub_name, sssub_module in ssub_module.named_children():\n",
    "                print(\"│  │  └\", sssub_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4. 요약문 생성 모델 학습\n",
    "\n",
    "네이버 뉴스 요약 데이터세트를 불러와 모델을 학습한다. \n",
    "\n",
    "네이버 영화 뉴스 요약 데이터세트는 약 22,000개의 핛브데이터, 약 2,500개의 검증데이터, 약 2,700개의 테스트 데이터로 구성되어 있다. 각 데이터는 뉴스 기사가 작성된 날짜(date), 뉴스 분야(category), 언론사 이름(press), 기사 제목(title), 본문(document)기사 URL(link), 본문을 요약한 요약문(summary)으로 구성되어 있다.\n",
    "\n",
    "트랜스포머 라이브러리의 토크나이저는 text_target 인자를 사용해 정답 텍스트를 인코딩할 수 있으며, 이 인코딩된 정답 텍스트는 레이블(labels)로 반환되어 학습에 사용된다 gogamza/kobart-base-v2모델의 토크나이저는 기본적으로 최대 길이가 설정돼 있지 않으므로, 입력 최대 길이를 설정해 토큰화 과정에서 이 길이를 초과하지 않도록 한다.\n",
    "\n",
    "반환된 input id와 레이블은 길이가 서로 다르므로, 배치 처리를 위해 동일한 길이로 맞춰야한다. 레이블도 정수 인코딩되므로, 레이블도 패딩 처리해야 한다. 트랜스포머 라이브러리에서는 DataCollatorForSeq2Seq을 사용해 이러한 패딩 처리를 자동으로 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
      "        num_rows: 22194\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
      "        num_rows: 2466\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
      "        num_rows: 2740\n",
      "    })\n",
      "})\n",
      "[14516, 18115, 15736, 17856, 11372, 14336, 18156, 14099, 11697, 12037, 14239, 250, 11764, 16186, 14184, 14631, 19115, 22935, 15530, 28440, 14424, 14199, 22814, 14536, 13363, 10608, 12037, 15363, 23656, 14185, 14281, 14428, 17212, 12034, 14681, 26219, 22935, 14445, 15363, 27689, 21671, 9499, 14088, 15383, 14185, 17856, 15186, 23066, 15634, 12178, 14145, 14333, 26795, 21671, 11028, 14423, 9120, 17711, 11268, 10770, 16490, 14382, 14634, 15272, 19754]\n",
      "올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록한 가운데, 정부가 하반기에 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 결정한 가운데, 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했다.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import BartTokenizerFast, BartForConditionalGeneration\n",
    "\n",
    "def preprocess_data(example, tokenizer):\n",
    "    return tokenizer(\n",
    "        example[\"document\"],\n",
    "        text_target=example[\"summary\"],\n",
    "    )\n",
    "\n",
    "model_name = \"gogamza/kobart-base-v2\"\n",
    "\n",
    "tokenizer = BartTokenizerFast.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"daekeun-ml/naver-news-summarization-ko\")\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "tokenizer.model_max_length = model.config.max_position_embeddings\n",
    "processed_dataset = dataset.map(\n",
    "    lambda example: preprocess_data(example, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "sample = processed_dataset[\"train\"][\"labels\"][0]\n",
    "\n",
    "print(sample)\n",
    "print(tokenizer.decode(sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([4, 720])\n",
      "attention_mask torch.Size([4, 720])\n",
      "labels torch.Size([4, 105])\n"
     ]
    }
   ],
   "source": [
    "# DataCollatorForSeq2Seq를 이용한 배치 단위 패딩\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForSeq2Seq, BartTokenizerFast\n",
    "\n",
    "tokenizer = BartTokenizerFast.from_pretrained(\"gogamza/kobart-base-v2\")\n",
    "\n",
    "seq2seq_collator = DataCollatorForSeq2Seq(\n",
    "  tokenizer=tokenizer,\n",
    "  padding=\"longest\",\n",
    "  return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "seq2seq_dataloader = DataLoader(\n",
    "  processed_dataset[\"train\"],\n",
    "  collate_fn=seq2seq_collator,\n",
    "  batch_size=4,\n",
    "  shuffle=False,\n",
    ")\n",
    "\n",
    "seq2seq_interator = iter(seq2seq_dataloader)\n",
    "seq2seq_batch = next(seq2seq_interator)\n",
    "for key, value in seq2seq_batch.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "패딩의 결과를 보면 인코더에 입력되는 입력ID(input_ids)와 어텐션 마스크(attention_mask)가 동일한 최대 길이인 720으로 패딩되었다. 이는 인코더가 입력 시퀀스의 모든 위치에 대한 표현을 생성하기 위해 필요한 전처리 과정으로 패딩된 입력을 받아 셀프 어텐션 등의 메커니즘을 통해 문맥 정보를 인코딩한다.\n",
    "\n",
    "디코더가 예측해야하는 labels도 배치 내에서 가장 긴 길이인 105로 배딩되어 있다. 이는 자기회귀 디코더가 각 시점에서 다음 토큰을 예측할 때, 그 이전까지의 정답 토큰 시퀀스르 참조해야 하기 때문이다. 따라서 labels도 패딩 처리해야 한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
      "        num_rows: 22194\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
      "        num_rows: 2466\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
      "        num_rows: 2740\n",
      "    })\n",
      "})\n",
      "[14516, 18115, 15736, 17856, 11372, 14336, 18156, 14099, 11697, 12037, 14239, 250, 11764, 16186, 14184, 14631, 19115, 22935, 15530, 28440, 14424, 14199, 22814, 14536, 13363, 10608, 12037, 15363, 23656, 14185, 14281, 14428, 17212, 12034, 14681, 26219, 22935, 14445, 15363, 27689, 21671, 9499, 14088, 15383, 14185, 17856, 15186, 23066, 15634, 12178, 14145, 14333, 26795, 21671, 11028, 14423, 9120, 17711, 11268, 10770, 16490, 14382, 14634, 15272, 19754]\n",
      "올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록한 가운데, 정부가 하반기에 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 결정한 가운데, 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했다.\n",
      "input_ids torch.Size([4, 720])\n",
      "attention_mask torch.Size([4, 720])\n",
      "labels torch.Size([4, 105])\n"
     ]
    }
   ],
   "source": [
    "# 요약문 생성 모델 학습\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, BartTokenizerFast, BartForConditionalGeneration, DataCollatorForSeq2Seq\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Set CUDA_LAUNCH_BLOCKING for better error trace\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "\n",
    "def preprocess_data(example, tokenizer):\n",
    "    return tokenizer(\n",
    "        example[\"document\"],\n",
    "        text_target=example[\"summary\"],\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "model_name = \"gogamza/kobart-base-v2\"\n",
    "\n",
    "tokenizer = BartTokenizerFast.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "dataset = load_dataset(\"daekeun-ml/naver-news-summarization-ko\")\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "tokenizer.model_max_length = model.config.max_position_embeddings\n",
    "\n",
    "processed_dataset = dataset.map(\n",
    "    lambda example: preprocess_data(example, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "sample = processed_dataset[\"train\"][\"labels\"][0]\n",
    "print(sample)\n",
    "print(tokenizer.decode(sample))\n",
    "\n",
    "seq2seq_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "seq2seq_dataloader = DataLoader(\n",
    "    processed_dataset[\"train\"],\n",
    "    collate_fn=seq2seq_collator,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "seq2seq_interator = iter(seq2seq_dataloader)\n",
    "seq2seq_batch = next(seq2seq_interator)\n",
    "\n",
    "for key, value in seq2seq_batch.items():\n",
    "    print(key, value.shape)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./runs/text-summarization\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=1,\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=seq2seq_collator,\n",
    "    train_dataset=processed_dataset[\"train\"],\n",
    "    eval_dataset=processed_dataset[\"validation\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
