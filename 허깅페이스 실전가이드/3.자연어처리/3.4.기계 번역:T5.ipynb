{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. 기계 번역: T5\n",
    "\n",
    "기계 번역(Machine Translation)은 컴퓨터 프로그램을 활용해 한 언어의 텍스트를 다른 언어의 텍스트로 자동 변환하는 기술이다. 빠르고 저렴하지만 신조어, 관용구, 은유등 쫌 힘든 부분이 있다.\n",
    "\n",
    "기계 번역 기술은 통계적 기계 번역(Statistical Machine Translation), 신경망 기계 번역(Neural Machine Translation) 등이 있다.\n",
    "통계적 기계 번역은 원문과 번역문 쌍을 기반으로 단어 순서와 언어 패턴을 인식해 학습한다. 반면, 신경망 기계 번역은 심층 신경망 모델을 사용해 번역문과 단어 시퀀스 간의 관계를 학습한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1. T5\n",
    "\n",
    "T5(Text-to-Text Transfer Transformer)는 구글에서 개발한 언어 모델로 인코더와 디코더로 이루어진 시퀀스-투-시퀀스(Seq2Seq) 모델이다. 이 모델은 모든 자연어 처리 과제를 텍스트-투-텍스트 형태의 데이터로 변환하고, 이를 시퀀스-투-시퀀스 문제로 인식해 해결한다. 입력과 출력이 모두 텍스트이다.\n",
    "\n",
    "**기계 번역 과제**\n",
    "\n",
    "* **입력 텍스트**: \"오늘 날씨는 어때요?\"\n",
    "* **입력 시퀀스**: [\"오늘, 날씨는, 어때요?\"]\n",
    "* **출력 시퀀스**: [\"How's, the, weather, today?\"]\n",
    "* **출력 텍스트**: \"How's the weather today?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2. 기계 번역 모델 학습\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type longt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at KETI-AIR/long-ke-t5-small and are newly initialized: ['encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23c9b9180864af3831885a013dbeb49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25383f42b9074165a3f45792be9cf44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191f01bbdfa344e69387fd2ae184dd7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [20004, 20525, 20048, 20298, 20480, 20025, 20263, 20027, 20187, 20050, 43305, 20009, 21015, 20047, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [20004, 23477, 20048, 92, 14, 4256, 11, 1363, 71, 1133, 2951, 20371, 33, 16, 75, 242, 10, 513, 20047, 1]}\n",
      "변환된 출발 언어: en: What makes you think I want an intro to anyone?</s>\n",
      "변환된 도착 언어: ko: 내가 너를 누구에게 소개하고 싶어한다고 생각하니?</s>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "\n",
    "def preprocess_data(example, tokenizer):\n",
    "    translation = example['translation']\n",
    "    translation_source = [\"en: \" + instance[\"en\"] for instance in translation]\n",
    "    translation_target = [\"ko: \" + instance[\"ko\"] for instance in translation]\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        translation_source,\n",
    "        text_target=translation_target,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "model_name = \"KETI-AIR/long-ke-t5-small\"\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ko\")\n",
    "\n",
    "processed_dataset = dataset.map(\n",
    "    lambda example: preprocess_data(example, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "sample = processed_dataset[\"test\"][0]\n",
    "print(sample)\n",
    "print(\"변환된 출발 언어:\", tokenizer.decode(sample[\"input_ids\"]))\n",
    "print(\"변환된 도착 언어:\", tokenizer.decode(sample[\"labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPUS-100 데이터세트는 다양한 언어 간 기계 번역 작업을 위해 구축된 대규모 병렬 말붕치 (Parallel corpus)다. 이중 en-ko는 영어와 한국어 간의 번역 쌍을 나타닌다. 이 데이터세트의 영어-한국어 부분은 100만개의 학습데이터와 각각 2,000개의 검증 및 테스트 데이터로 구성되어 있다.\n",
    "\n",
    "데이터세트는`translation` 열로만 이루어져 있다. 이 열에는 영어 문장과 한국어 문장이 en과 ko 키를 갖는 딕셔너리 형태로 저장 되어 있다. \n",
    "\n",
    "`T5ForConditionalGeneration`을 사용할 때는 수행할 하위 작업을 프롬프트 형식으로 정의해야 한다. `en: 프롬프트`,를 이용해 모델에 전달한다. 모델은 이 프롬프트를 입력받아 처리한후 \"ko: 프롬프트\"를 출력한다.\n",
    "\n",
    "`T5ForConditionalGeneration` 은 프롬프트 기반 접근법을 사용해 다양한 자연어 처리 작업을 수행할 수 있다. 번역 작업뿐만 아니라 요약, 질의응답 등의 장업도 프롬프트 형식을 적절히 정의하면 동일한 방식으로 수행할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type longt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at KETI-AIR/long-ke-t5-small and are newly initialized: ['encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5fde7bba2448e19173f9194cc93411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # 첫 번째 GPU를 사용하도록 설정\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, T5TokenizerFast, T5ForConditionalGeneration\n",
    "\n",
    "def preprocess_data(example, tokenizer):\n",
    "  translation = example['translation']  # 번역된 문장 쌍을 가져옴\n",
    "  translation_source = [\"en: \" + instance[\"en\"] for instance in translation]  # 영어 문장에 \"en: \" 접두사 추가\n",
    "  translation_target = [\"ko: \" + instance[\"ko\"] for instance in translation]  # 한국어 문장에 \"ko: \" 접두사 추가\n",
    "\n",
    "  tokenized = tokenizer(\n",
    "    translation_source,\n",
    "    text_target=translation_target,\n",
    "    truncation=True,  # 길이가 너무 길면 잘라냄\n",
    "  )\n",
    "\n",
    "  return tokenized  # 토큰화된 결과 반환\n",
    "\n",
    "model_name = \"KETI-AIR/long-ke-t5-small\"  # 사용할 모델 이름\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)  # 모델에 맞는 토크나이저 로드\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)  # 모델 로드\n",
    "\n",
    "dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ko\")  # 데이터셋 로드\n",
    "\n",
    "processed_dataset = dataset.map(\n",
    "  lambda example: preprocess_data(example, tokenizer),  # 데이터셋 전처리\n",
    "  batched=True,  # 배치 단위로 처리\n",
    "  remove_columns=dataset[\"train\"].column_names,  # 원래 컬럼 제거\n",
    ")\n",
    "\n",
    "\n",
    "seq2seq2_collator = DataCollatorForSeq2Seq(\n",
    "  tokenizer=tokenizer,\n",
    "  padding=\"longest\",  # 가장 긴 시퀀스에 맞춰 패딩\n",
    "  return_tensors=\"pt\",  # PyTorch 텐서로 반환\n",
    ")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "  output_dir=\"./runs/t5-translation\",  # 출력 디렉토리\n",
    "  logging_dir='./runs/t5-translation/logs',  # 로그 디렉토리\n",
    "  per_device_train_batch_size=8,  # 훈련 배치 크기\n",
    "  per_device_eval_batch_size=16,  # 평가 배치 크기\n",
    "  learning_rate=2e-5,  # 학습률\n",
    "  num_train_epochs=100,  # 학습 에포크 수\n",
    "  eval_steps=200,  # 평가 스텝\n",
    "  logging_steps=50,  # 로그 스텝\n",
    "  save_steps=200,  # 저장 스텝\n",
    "  seed=42,  # 랜덤 시드\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "  model=model,  # 모델\n",
    "  data_collator=seq2seq2_collator,  # 데이터 콜레이터\n",
    "  args=training_args,  # 훈련 인자\n",
    "  train_dataset=processed_dataset[\"train\"].select(range(100000)),  # 훈련 데이터셋\n",
    "  eval_dataset=processed_dataset[\"test\"].select(range(1000)),  # 평가 데이터셋\n",
    ")\n",
    "\n",
    "# trainer.train()  # 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5ForConditionalGeneration 을 활용한 번역 모델 학습에서는 DataCollectorForSeq2Seq 클래스를 이용해 배치 단위로 입력 시퀀스와 출력 시퀀스에 대한 패딩 처리를 수행한다. T5와 BART등의 시퀀스-투-시퀀스 모델에서는 입력 데이터와 출력 데이터가 모두 텍스트 형태의 시퀀스로 구성되어 있다.\n",
    "따라서 입력 ID와 레이블을 동일한 길이로 맞추기 위해 DataCollatorForSeq2Seq 클래스로 패딩 작업을 수행한다.\n",
    "\n",
    "기계 번역은 출발어와 목적어 모두를 깊이 이해해야 하는 복잡한 자연어 처리 과제다. 그러므로 기존 엄청나게 오래 걸린다. tmux를 사용해야 한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"./runs/t5-translation/best_model\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "data = \"en: It's always great to acquire new knowledge.\"\n",
    "\n",
    "inputs = tokenizer(data, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_length=512,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기계 번역 모델 평가\n",
    "\n",
    "import evaluate\n",
    "import mlflow\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(\n",
    "  processed_dataset[\"test\"].select(range(100)),\n",
    "  collate_fn=seq2seq2_collator,\n",
    "  batch_size=4,\n",
    "  shuffle=False\n",
    ")\n",
    "\n",
    "generated_translated = []\n",
    "\n",
    "true_translated_ids = processed_dataset[\"test\"].select(range(100))[\"labels\"]\n",
    "\n",
    "true_translated = tokenizer.batch_decode(true_translated_ids, skip_special_tokens=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(device)\n",
    "        outputs = model.generate(\n",
    "            **batch,\n",
    "            max_length=1026,\n",
    "            num_beams=4,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        batch_translated = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        generated_translated.extend(batch_translated)\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "blue_score = bleu.compute(predictions=generated_translated, references=true_translated)\n",
    "\n",
    "# 번역 평가 지표 로깅\n",
    "mlflow.log_metrics(blue_score)\n",
    "\n",
    "print(blue_score)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
