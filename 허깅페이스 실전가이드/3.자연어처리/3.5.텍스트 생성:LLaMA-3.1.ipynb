{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. 텍스트 생성: LLaMA-3.1\n",
    "\n",
    "**텍스트 생성(Text Generation)**은 주어진 입력 텍스트를 기반으로 새로운 텍스트를 만들어내는 기술이다. 이 기술은 단순히 주어진 입력을 바탕으로 텍스트를 만들어내는 것이 아니다. 이 기술은 맥락을 이해하고 적절한 응답을 생성하는 복잡한 과정을 포함한다. 요약, 기계 번역, 이외에도 생성형 질의 응답, 챗봇과 같은 다양한 분야에서 텍스트 생성 기술이 사용되고 있다.\n",
    "\n",
    "허깅페이스 트랜스포머 라이브러리에서는 크게 시퀀스-투-시퀀스(Sequence-to-Sequence) 모델과 인과적 언어 모델(Causal Language Model)로 나뉜다. 시퀀스-투-시퀀스 모델은 입력 시퀀스를 출력 시퀀스로 매핑하는 모델이고, 인과적 언어 모델은 주어진 입력 시퀀스를 기반으로 새로운 텍스트를 생성하는 모델이다.\n",
    "\n",
    "**시퀀스-투-시퀀스 모델**\n",
    "\n",
    "* **대포적인 모델**:Transformer, MASS, BART, T5\n",
    "* **구조**: 인코더와 디코더로 구성\n",
    "* **특징**: 양방향 모델로, 입력 텍스트 전체를 고려해 출력을 생성\n",
    "* **주요 응용 분야**: 기계 번역, 텍스트 요약, 질의 응답\n",
    "\n",
    "**인과적 언어 모델**\n",
    "\n",
    "* **대포적인 모델**:GPT, LLaMA, ChatGPT, Gemma, PaLM등이 있다.\n",
    "* **구조**: 단일 디코더\n",
    "* **특징**: 단방향 모델로, 주어진 입력 시퀀스를 기반으로 새로운 텍스트를 생성\n",
    "* **주요 응용 분야**: 텍스트 생성, 챗봇\n",
    "\n",
    "최근에 주목받고 있는 대형 언어 모델들은 대부분 인과적 언어 모델 구조를 채택하고 있다. 이 모델들은 입력이 들어오는 대로 순차적으로 처리할 수 있어 실시간 대화 처리에 적합하다. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1. LLaMA-3 시리즈\n",
    "\n",
    "LLaMA(Large Language Model Meta AI)는 메타엣서 개발 및 공개한 대규모 언어 모델 시리즈다. LLaMA-3 시리즈는 2024년 4월에 공개된 버전이다. LLaMA-3는 이전 버전들에 비해 큰폭으로 성능이 좋아졌다.\n",
    "현재는 8B(80억) 매개변수를 갖는 모델이 출시 되었다. LLaMA-3는 15조 개 이상의 토큰으로 학습되었다. LLaMA-2에 비해 7배 이상 많은 토큰을 사용하여 학습되었다. 30개 이상의 언어로 된 고품질 데이터가 포함되어 있다.\n",
    "LLaMA-3sms 8,192토큰 길이의 시퀀스로 학습되었으며, 128K 토큰의 더 효율적인 토크나이저를 사용해 언어 인코딩 효율성을 크게 높였다. 이전 모델보다 토큰 사용이 최대 15%감소 되었다. 또한 **그룹 쿼리 어텐션(Grouped Query Attention, GQA)**을 사용하여 효율적인 어텐션 계산을 가능하게 했다. \n",
    "\n",
    "2024년 7월에는 기능이 더욱 강화한 LLaMA-3.1이 공개 되었다. 기존 8B, 70B 모델 이외에 40B(4,050억) 매개변수 모델이 추가로 공개 되었다. LLaMA-3는 다국어를 지원하도록 사전 학습 되었지만 주로 영어로 되어 있다.\n",
    "\n",
    "LLaMA-3시리즈의 학습 데이터에는 일반 지식에 해당하는 토큰이 약 50% 수학 및 추론 토큰이 25% 코드 토큰이 17% 다국어 토큰이 8% 포함된다. \n",
    "\n",
    "**LLaMA-3의 특징**\n",
    "\n",
    "* **토크나이저**: 128K 토큰의 확장된 어휘 사전을 가진 토크나이저를 사용한다. 이는 언어를 더욱 효율적으로 인코딩해 모델 성능을 크게 향상시킨다. 실제로 이 새로운 토크나이저는 LLaMA-2에 비해 토큰 사용을 최대 15%까지 줄였다.\n",
    "* **그룹 쿼리 어텐션**: 그룹 쿼리 어텐션은 어텐션 계산을 효율적으로 수행할 수 있도록 도와준다. 이를 통해 모델은 더 많은 토큰을 처리할 수 있으며, 더 높은 성능을 달성할 수 있다.\n",
    "* **위치 정보 인코딩**: 최전 위치 임페딩(Rotary Position Embedding)을 사용하여 위치 정보를 인코딩한다. 이 방식은 코사인과 사인 함수를 이용한 기존의 위치 임코딩보다 더 효과적으로 상대적 위치 정보를 모델에 제공한다.\n",
    "* **활성화 함수**: SwiGLU(Switch-Gated Linear Unit)를 사용한다. 이는 GLU(Gated Linear Unit)의 변형으로 ReLU나 GeLU보다 더 복잡한 패턴을 학습할 수 있다.\n",
    "* **정규화 기법**: 배치 정규화 대신 RMS(Root Mean Square) 정규화를 사용한다. 이 방식은 학습을 더 안정적으로 만들고 계산 효율성도 향상시킨다.\n",
    "* **시퀀스 길이 및 마스킹**: 8,192 토큰 길이의 시퀀스로 학습되었으며, 문서 경계를 넘지 않도록 하는 마스크를 사용했다. 이는 모델이 긴 컨텍스트를 더 잘 이해하고 처리할 수 있다.\n",
    "* **KV 캐시 기법**: 추론 시에는 어텐션 연산에 사용되는 키와 값을 캐시에 저장하는 KV 캐시 기법을 사용해 연산 효율을 크게 개선했다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2. 텍스트 생성 모델 실습\n",
    "\n",
    "모델의 학습 및 추론을 최적화 하기 위해 사용하는 `bitsAndBytesConfig` 객체를 사용해 모델의 메모리 사용량을 줄이고 계산 속도를 높인다. 이 객체는 모델의 크기와 성능을 조정할 수 있는 다양한 하이퍼파라미터를 제공한다. 이를 통해 모델의 성능을 최적화하고 메모리 사용량을 줄일 수 있다.\n",
    "\n",
    "큰 모델을 상대적으로 작은 메모리를 가진 하드웨어에서 실행해야 할때 사용하며 8비트나 4비트 양자화(Quantization)를 사용해 모델의 메모리 사용량을 줄일 수 있다. 이를 통해 모델의 성능을 최적화하고 메모리 사용량을 줄일 수 있다.\n",
    "\n",
    "**BitsAndBytesConfig 객체의 주요 하이퍼파라미터**\n",
    "\n",
    "* **load_in_8bit**: 8비트 양자화를 활성화 한다. 4비트 양자화 설정과는 베타적이다. 메모리 사용량을 줄이고 추론 속도를 높일 수 있다.\n",
    "* **load_in_4bit**: 4비트 양자화를 활성화 한다. 8비트 양자화 설정과는 베타적이다. 메모리 사용량을 줄이고 추론 속도를 높일 수 있다.\n",
    "* **llm_int8_threshold**: 8비트 양자화에서 이상치 참지를 위한 임계값이다. 기본값은 6.0이다.\n",
    "* **bnb_4bit_compute_dtype**: 4비트 양자화에서 사용될 계산 데이터 형식을 지정한다. 이 값은 입력 형식과 다를 수 있으며, 연산 속도 향상을 위해 사용된다. \n",
    "* **bnb_4bit_quant_type**: 4비트 양자화에서 사용될 양자화 유형을 지정한다. \"pf4\"는 4비트 부동 소수점을 의미한다. nf4(4-bit normalized float)\"는 4비트 정규화된 부동 소수점을 의미한다.\n",
    "* **bnb_4bit_use_double_quant**: 이중 양자화를 활성화하는 매개변수로 추가적인 메모리 절약이 가능하지만 정확도에 영향을 줄 수 있다.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7afdb1c8e2942a79d3eb94ad9f26624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "위키북스의 대표 저자를 알려주세요.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "윤대희, 김동화, 송종민, 진현두<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "dataset = load_dataset(\"s076923/llama3-wikibook-ko\")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "model_name = \"meta-llama/Meta-llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][\"text\"][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모model.save_pretrained(\"./models/\"+model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "위키북스의 대표 저자를 알려주세요.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "윤대희, 김동화, 송종민, 진현두<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.use_cache = False\n",
    "\n",
    "dataset = load_dataset(\"s076923/llama3-wikibook-ko\")\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][\"text\"][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=128,\n",
    "    lora_alpha=4,\n",
    "    lora_dropout=0.1,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef23a3a4ee14ddeb7a0312f3339d047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "위키북스의 대표 저자를 알려주세요.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "윤대희, 김동화, 송종민, 진현두<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import LoraConfig\n",
    "from datasets import load_dataset\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # GPU 디바이스 설정\n",
    "\n",
    "dataset = load_dataset(\"s076923/llama3-wikibook-ko\")  # 위키북 한국어 데이터셋 로드\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(  # 4-bit 양자화 설정\n",
    "  load_in_4bit=True,\n",
    "  bnb_4bit_quant_type=\"nf4\",\n",
    "  bnb_4bit_compute_dtype=torch.float16,\n",
    "  bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "model_name = \"meta-llama/Meta-llama-3.1-8B-Instruct\"  # 사용할 모델 이름\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(  # 토크나이저 로드\n",
    "  model_name,\n",
    "  trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(  # 모델 로드\n",
    "  model_name,\n",
    "  quantization_config=quantization_config,\n",
    "  device_map={\"\": 0},\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  # 패딩 토큰 설정\n",
    "model.config.use_cache = False  # 캐시 사용 안함\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 디바이스 설정\n",
    "model.to(device)  # 모델을 디바이스로 이동\n",
    "\n",
    "print(dataset)  # 데이터셋 정보 출력\n",
    "print(dataset[\"train\"][\"text\"][7])  # 훈련 데이터셋의 7번째 샘플 텍스트 출력\n",
    "\n",
    "# SFTTrainer 초기화\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # 모델\n",
    "    args=SFTConfig(  # SFT 설정\n",
    "      output_dir=\"./runs/\"+model_name,  # 출력 디렉토리\n",
    "      max_seq_length=64,  # 최대 시퀀스 길이\n",
    "      dataset_text_field=\"text\",  # 데이터셋의 텍스트 필드 이름\n",
    "      per_device_train_batch_size=4,  # 디바이스 당 배치 크기\n",
    "      gradient_accumulation_steps=8,  # 그래디언트 누적 스텝 수\n",
    "      max_steps=1000,  # 최대 스텝 수\n",
    "      learning_rate=2e-5,  # 학습률\n",
    "      warmup_steps=100,  # 워밍업 스텝 수\n",
    "      logging_steps=100,  # 로깅 스텝 수\n",
    "      fp16=True,  # FP16 사용\n",
    "      optim=\"paged_adamw_8bit\",  # 옵티마이저\n",
    "      seed=42,  # 랜덤 시드\n",
    "    ),\n",
    "    peft_config=LoraConfig(  # LoRA 설정\n",
    "      r=128,  # LoRA 랭크\n",
    "      lora_alpha=4,  # LoRA 알파\n",
    "      lora_dropout=0.1,  # LoRA 드롭아웃\n",
    "      task_type=\"CAUSAL_LM\"  # 작업 유형\n",
    "    ),\n",
    "    processing_class=tokenizer,  # 토크나이저\n",
    "    train_dataset=dataset[\"train\"],  # 훈련 데이터셋\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860e18f543da4a7d9850fef98f88faa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "위를키 북스의 주요 저자를 알려드릴 수 있습니다: \n",
      "\n",
      "- 김도현: 주로 네트워크 및 보안 관련 책을 출판했습니다.\n",
      "- 신대현 : 주으로 데이터 사이언스 및 AI 관련책을 出판하셨습니다. \n",
      "- 이재호: 다양한 주제의 책, 특히 데이터 관련 주題을 다룬 책의 저자입니다. \n",
      "\n",
      "위의 정보는 특정한 데이터로限되어 있으므로 더 최신\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./runs/meta-llama/Meta-llama-3.1-8B-Instruct\")  # 훈련된 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./runs/meta-llama/Meta-llama-3.1-8B-Instruct\")  # 훈련된 토크나이저 로드\n",
    "model.eval()\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"user\", \"content\":\"위키북스의 대표 저자는 누구인가요?\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "  messages,\n",
    "  add_generation_prompt=True,\n",
    "  return_tensors=\"pt\",\n",
    "  padding=True,\n",
    "  truncation=True,\n",
    ").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=100,\n",
    "    attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    top_p=0.9,\n",
    "    no_repeat_ngram_size=2,\n",
    "  )\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "The model 'LlamaForCausalLM' is not supported for question-answering. Supported models are ['AlbertForQuestionAnswering', 'BartForQuestionAnswering', 'BertForQuestionAnswering', 'BigBirdForQuestionAnswering', 'BigBirdPegasusForQuestionAnswering', 'BloomForQuestionAnswering', 'CamembertForQuestionAnswering', 'CanineForQuestionAnswering', 'ConvBertForQuestionAnswering', 'Data2VecTextForQuestionAnswering', 'DebertaForQuestionAnswering', 'DebertaV2ForQuestionAnswering', 'DistilBertForQuestionAnswering', 'ElectraForQuestionAnswering', 'ErnieForQuestionAnswering', 'ErnieMForQuestionAnswering', 'FalconForQuestionAnswering', 'FlaubertForQuestionAnsweringSimple', 'FNetForQuestionAnswering', 'FunnelForQuestionAnswering', 'GPT2ForQuestionAnswering', 'GPTNeoForQuestionAnswering', 'GPTNeoXForQuestionAnswering', 'GPTJForQuestionAnswering', 'IBertForQuestionAnswering', 'LayoutLMv2ForQuestionAnswering', 'LayoutLMv3ForQuestionAnswering', 'LEDForQuestionAnswering', 'LiltForQuestionAnswering', 'LlamaForQuestionAnswering', 'LongformerForQuestionAnswering', 'LukeForQuestionAnswering', 'LxmertForQuestionAnswering', 'MarkupLMForQuestionAnswering', 'MBartForQuestionAnswering', 'MegaForQuestionAnswering', 'MegatronBertForQuestionAnswering', 'MistralForQuestionAnswering', 'MixtralForQuestionAnswering', 'MobileBertForQuestionAnswering', 'MPNetForQuestionAnswering', 'MptForQuestionAnswering', 'MraForQuestionAnswering', 'MT5ForQuestionAnswering', 'MvpForQuestionAnswering', 'NemotronForQuestionAnswering', 'NezhaForQuestionAnswering', 'NystromformerForQuestionAnswering', 'OPTForQuestionAnswering', 'QDQBertForQuestionAnswering', 'Qwen2ForQuestionAnswering', 'Qwen2MoeForQuestionAnswering', 'ReformerForQuestionAnswering', 'RemBertForQuestionAnswering', 'RobertaForQuestionAnswering', 'RobertaPreLayerNormForQuestionAnswering', 'RoCBertForQuestionAnswering', 'RoFormerForQuestionAnswering', 'SplinterForQuestionAnswering', 'SqueezeBertForQuestionAnswering', 'T5ForQuestionAnswering', 'UMT5ForQuestionAnswering', 'XLMForQuestionAnsweringSimple', 'XLMRobertaForQuestionAnswering', 'XLMRobertaXLForQuestionAnswering', 'XLNetForQuestionAnsweringSimple', 'XmodForQuestionAnswering', 'YosoForQuestionAnswering'].\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'start_logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m위키북스의 대표 저자는 누구인가요?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m대표 저자는 누구인가요?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mqa_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ai/lib/python3.11/site-packages/transformers/pipelines/question_answering.py:398\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args_parser(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(examples) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(examples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/ai/lib/python3.11/site-packages/transformers/pipelines/base.py:1294\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[0;32m-> 1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/miniforge3/envs/ai/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ai/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/miniforge3/envs/ai/lib/python3.11/site-packages/transformers/pipelines/base.py:1209\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1208\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1209\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/ai/lib/python3.11/site-packages/transformers/pipelines/question_answering.py:527\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline._forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    525\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstart_logits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m: output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample\u001b[39m\u001b[38;5;124m\"\u001b[39m: example, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs}\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     start, end \u001b[38;5;241m=\u001b[39m output[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/ai/lib/python3.11/site-packages/transformers/utils/generic.py:431\u001b[0m, in \u001b[0;36mModelOutput.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    430\u001b[0m     inner_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_tuple()[k]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'start_logits'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
