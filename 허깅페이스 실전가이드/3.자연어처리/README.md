# 자연어 처리

인공적으로 만들어진 언어와 달리 인간이 사용하는 언어를 자연어라고 한다. 자연어 처리(Natural Language Processing, NLP)란 이러한 자연어를 처리하는 기술을 의미한다. 
자연어 처리는 컴퓨터와 인간이 의사소통을 할 수 있도록 도와주는 기술로, 음성 인식, 기계 번역, 질의 응답 시스템, 챗봇, 감성 분석, 텍스트 분류, 요약, 정보 검색, 정보 추출, 문서 분류, 문서 군집화, 텍스트 유사도 측정, 텍스트 생성 등 다양한 분야에서 활용된다.

자연어 처리 모델을 개발하기 위해서는 모호성, 가변성, 구조, 맥락, 다국어의 과제를 해결해야한다.

* **모호성(Ambiguity)**: 단어나 문장이 맥락에 따라 여러 의미를 가질 수 있는 문제
* **가변성(Variability)**: 방언, 억양, 신조어, 문제 등으로 인한 언어 표현의 다양성 문제
* **구조(Structure)**: 문장의 구조, 문법, 어순 등을 이해하는 문제
* **맥락(Context)**: 언어 사용의 사회문화적, 상화엊ㄱ 맥락을 이해하는 문제
* **다국어(Multilingual)**: 다양한 언어를 처리하는 문제

이런 문제를 해결하는데 순환 신경망(Recurrent Neural Network, RNN), 장단기 메모리(Long Short-Term Memory, LSTM), 어텐션(Attention), 트랜스포머(Transformer) 등 다양한 딥러닝 모델이 사용되었다.

**RNN, LSTM의 한계**

RNN과 LSTM은 시퀀스 데이터를 처리하는데 강점을 가지고 있지만, 시퀀스가 길어질수록 학습 능력이 저하되는 문제가 있다. 이를 장기 의존성 문제라고 한다. 이 문제를 해결하기 위해 어텐션 메커니즘이 제안되었고, 트랜스포머 모델이 이를 구현하였다.
또한 기울기 소실(Vanishing Gradient) 문제도 존재하는데, 이를 해결하기 위해 게이트 순환 유닛(Gated Recurrent Unit, GRU)이나 셀프 어텐션(Self-Attention) 등이 사용된다.

허깅페이스의 트랜스포머 라이브러리는 다양한 자연어 처리 과제에 적합한 여러 모델 구조를 제공한다.

* **텍스트분류**: BERT모델을 사용하여 텍스트를 여러 카테고리로 분류하는 방법을 학습한다. BERT는 양방향 인코딩을 통해 문맥을 효과적으로 파악할 수 있다.
* **요약문 생성**: BERT모델을 활용하여 긴 문서의 핵심 내용을 추출하고 간결한 요약문을 생성하는 기법을 배운다.
* **질의응답**: RoBERTa모델을 이용해 주어진 문서에서의 질문에 대한 정확한 답변을 추출하는 시스템을 구축한다. RoBERTa는 BERT의 개선된 버전이다.
* **기계 번역**: T5모델을 사용하여 한 언어에서 다른 언어로 텍스트를 번역하는 방법을 학습한다. T5는 "Text-to-Text Transfer Transformer"의 약자로, 다양한 자연어 처리 과제를 통일된 형식으로 표현할 수 있다.
* **텍스트 생성**: LLaMA-3.1을 활용해 텍스트 생성 방법을 학습한다. LLaMA는 "Language Model for Many Applications"의 약자로, 다양한 자연어 처리 과제에 적용할 수 있는 모델이다.

